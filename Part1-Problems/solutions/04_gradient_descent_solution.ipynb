{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Gradient Descent - Rolling the Ball Downhill\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this problem, you will:\n",
    "- Understand gradient descent as the fundamental optimization algorithm\n",
    "- Calculate gradients by hand and see how they guide weight updates\n",
    "- Explore how learning rate affects convergence\n",
    "- Watch gradient descent automatically find optimal weights for \"Go Dolphins!\"\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "1. **Manual Gradient Calculation** - Compute gradients by hand to understand the math\n",
    "2. **Gradient Descent Implementation** - Build the algorithm from scratch\n",
    "3. **Learning Rate Exploration** - See how step size affects optimization\n",
    "4. **Full Training Loop** - Watch the model learn optimal weights automatically\n",
    "\n",
    "---\n",
    "\n",
    "## The Story Continues\n",
    "\n",
    "In Problem 3, you discovered the loss landscape - a mathematical terrain where:\n",
    "- **Valleys** represent good weights (low loss)\n",
    "- **Hills** represent bad weights (high loss)\n",
    "- **The goal** is to find the deepest valley (optimal weights)\n",
    "\n",
    "But here's the crucial question: **How do we automatically navigate this landscape to find the optimal weights?**\n",
    "\n",
    "The answer is **gradient descent** - an algorithm that follows the steepest downhill direction to roll a ball to the bottom of the valley. It's the optimization engine that powers all machine learning.\n",
    "\n",
    "## What Is Gradient Descent?\n",
    "\n",
    "**Physical intuition**: Imagine you're in thick fog on a hillside and want to reach the bottom. You can't see the valley, but you can feel which direction slopes downward most steeply. Take a step in that direction, feel the slope again, repeat.\n",
    "\n",
    "**Mathematical definition**: \n",
    "- **Gradient** = direction of steepest increase in loss\n",
    "- **Negative gradient** = direction of steepest decrease (downhill)\n",
    "- **Step** = adjust weights slightly in the downhill direction\n",
    "\n",
    "**The algorithm**:\n",
    "```\n",
    "1. Calculate gradient of loss with respect to weights\n",
    "2. Move weights in the opposite direction (downhill)\n",
    "3. Repeat until you reach the bottom\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "# Import our utilities\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from data_generators import load_sports_dataset\n",
    "from gradient_helpers import numerical_gradient, analytical_gradient_mse, gradient_descent_with_history\n",
    "from visualization import plot_gradient_descent_path\n",
    "\n",
    "# Load our data\n",
    "features, labels, feature_names, texts = load_sports_dataset()\n",
    "\n",
    "print(\"Ready to optimize 'Go Dolphins!' sentiment classification!\")\n",
    "print(f\"Goal: Find optimal weights for {len(texts)} tweets\")\n",
    "print(f\"Starting point: Random weights need to learn from data\")\n",
    "print(f\"Method: Gradient descent optimization\")\n",
    "\n",
    "# Define our loss and activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def binary_cross_entropy(prediction, actual):\n",
    "    \"\"\"Binary cross-entropy loss\"\"\"\n",
    "    prediction = np.clip(prediction, 1e-15, 1 - 1e-15)\n",
    "    if actual == 1:\n",
    "        return -np.log(prediction)\n",
    "    else:\n",
    "        return -np.log(1 - prediction)\n",
    "\n",
    "def compute_loss_and_prediction(features_single, label_single, weights):\n",
    "    \"\"\"Compute prediction and loss for a single example\"\"\"\n",
    "    raw_prediction = np.dot(features_single, weights)\n",
    "    probability = sigmoid(raw_prediction)\n",
    "    loss = binary_cross_entropy(probability, label_single)\n",
    "    return probability, loss, raw_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Manual Gradient Calculation\n",
    "\n",
    "Before we automate gradient descent, let's understand the math by calculating gradients by hand for our \"Go Dolphins!\" example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step gradient calculation for \"Go Dolphins!\"\n",
    "go_dolphins_features = features[0]  # [2, 1, 1]\n",
    "go_dolphins_label = labels[0]       # 1 (positive)\n",
    "\n",
    "# Start with some initial weights\n",
    "initial_weights = np.array([0.1, 0.2, 0.1])\n",
    "\n",
    "print(\"MANUAL GRADIENT CALCULATION FOR 'GO DOLPHINS!'\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Input features: {go_dolphins_features} (word_count, has_team, has_exclamation)\")\n",
    "print(f\"True label: {go_dolphins_label} (positive sentiment)\")\n",
    "print(f\"Initial weights: {initial_weights}\")\n",
    "print()\n",
    "\n",
    "# Step 1: Forward pass\n",
    "raw_pred = np.dot(go_dolphins_features, initial_weights)\n",
    "probability = sigmoid(raw_pred)\n",
    "loss = binary_cross_entropy(probability, go_dolphins_label)\n",
    "\n",
    "print(\"FORWARD PASS:\")\n",
    "print(f\"1. Raw prediction (z): {go_dolphins_features} · {initial_weights} = {raw_pred:.4f}\")\n",
    "print(f\"2. Probability (σ(z)): sigmoid({raw_pred:.4f}) = {probability:.4f}\")\n",
    "print(f\"3. Loss: -log({probability:.4f}) = {loss:.4f}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Manual gradient calculation using chain rule\n",
    "print(\"GRADIENT CALCULATION (Chain Rule):\")\n",
    "print(\"Goal: Find ∂Loss/∂w for each weight\")\n",
    "print()\n",
    "\n",
    "# Chain rule: ∂Loss/∂w = (∂Loss/∂prob) × (∂prob/∂z) × (∂z/∂w)\n",
    "\n",
    "# ∂Loss/∂prob (derivative of BCE loss)\n",
    "dloss_dprob = -1/probability  # For positive examples\n",
    "print(f\"∂Loss/∂probability = -1/{probability:.4f} = {dloss_dprob:.4f}\")\n",
    "\n",
    "# ∂prob/∂z (derivative of sigmoid)\n",
    "dprob_dz = probability * (1 - probability)\n",
    "print(f\"∂probability/∂z = {probability:.4f} × (1 - {probability:.4f}) = {dprob_dz:.4f}\")\n",
    "\n",
    "# ∂z/∂w (derivative of dot product)\n",
    "dz_dw = go_dolphins_features  # This is just the input features!\n",
    "print(f\"∂z/∂w = {dz_dw} (input features)\")\n",
    "\n",
    "# Combine using chain rule\n",
    "dloss_dz = dloss_dprob * dprob_dz\n",
    "gradient = dloss_dz * dz_dw\n",
    "\n",
    "print(f\"\\nCombined:\")\n",
    "print(f\"∂Loss/∂z = {dloss_dprob:.4f} × {dprob_dz:.4f} = {dloss_dz:.4f}\")\n",
    "print(f\"∂Loss/∂w = {dloss_dz:.4f} × {dz_dw} = {gradient}\")\n",
    "print()\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "print(f\"Gradient: {gradient}\")\n",
    "print(\"This tells us:\")\n",
    "for i, (feature_name, grad_val) in enumerate(zip(feature_names, gradient)):\n",
    "    direction = \"increase\" if grad_val > 0 else \"decrease\"\n",
    "    print(f\"  - {direction} {feature_name} weight (gradient: {grad_val:+.4f})\")\n",
    "\n",
    "print(f\"\\nNote: All gradients are positive because our prediction ({probability:.4f}) is too low\")\n",
    "print(f\"compared to the true label ({go_dolphins_label}). We need to increase all weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify manual calculation with numerical gradient\n",
    "def loss_function_single_example(weights):\n",
    "    \"\"\"Loss function for gradient verification\"\"\"\n",
    "    _, loss, _ = compute_loss_and_prediction(go_dolphins_features, go_dolphins_label, weights)\n",
    "    return loss\n",
    "\n",
    "# Calculate numerical gradient (finite differences)\n",
    "numerical_grad = numerical_gradient(loss_function_single_example, initial_weights)\n",
    "\n",
    "print(\"GRADIENT VERIFICATION:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Manual calculation:    {gradient}\")\n",
    "print(f\"Numerical calculation: {numerical_grad}\")\n",
    "print(f\"Difference:            {np.abs(gradient - numerical_grad)}\")\n",
    "print(f\"Max difference:        {np.max(np.abs(gradient - numerical_grad)):.2e}\")\n",
    "\n",
    "if np.max(np.abs(gradient - numerical_grad)) < 1e-6:\n",
    "    print(\"✅ Manual calculation is correct!\")\n",
    "else:\n",
    "    print(\"❌ Check manual calculation - there might be an error\")\n",
    "\n",
    "# Show what a gradient descent step would look like\n",
    "learning_rate = 0.1\n",
    "new_weights = initial_weights - learning_rate * gradient\n",
    "\n",
    "print(f\"\\nGRADIENT DESCENT STEP:\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Weight update: w_new = w_old - α × gradient\")\n",
    "print(f\"New weights: {initial_weights} - {learning_rate} × {gradient}\")\n",
    "print(f\"           = {new_weights}\")\n",
    "\n",
    "# Check if this improved our prediction\n",
    "new_prob, new_loss, _ = compute_loss_and_prediction(go_dolphins_features, go_dolphins_label, new_weights)\n",
    "print(f\"\\nIMPROVEMENT CHECK:\")\n",
    "print(f\"Old prediction: {probability:.4f}, loss: {loss:.4f}\")\n",
    "print(f\"New prediction: {new_prob:.4f}, loss: {new_loss:.4f}\")\n",
    "print(f\"Loss change: {new_loss - loss:+.4f} ({'✅ improved!' if new_loss < loss else '❌ got worse'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Gradient Descent Implementation\n",
    "\n",
    "Now let's implement the full gradient descent algorithm and watch it automatically optimize our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent from scratch\n",
    "\n",
    "def compute_batch_gradient(features_batch, labels_batch, weights):\n",
    "    \"\"\"\n",
    "    Compute gradient across all training examples.\n",
    "    \"\"\"\n",
    "    total_gradient = np.zeros_like(weights)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i in range(len(features_batch)):\n",
    "        # Forward pass\n",
    "        raw_pred = np.dot(features_batch[i], weights)\n",
    "        probability = sigmoid(raw_pred)\n",
    "        loss = binary_cross_entropy(probability, labels_batch[i])\n",
    "        \n",
    "        # Backward pass (gradient calculation)\n",
    "        # Chain rule: ∂Loss/∂w = (∂Loss/∂prob) × (∂prob/∂z) × (∂z/∂w)\n",
    "        \n",
    "        if labels_batch[i] == 1:\n",
    "            dloss_dprob = -1/probability\n",
    "        else:\n",
    "            dloss_dprob = 1/(1-probability)\n",
    "        \n",
    "        dprob_dz = probability * (1 - probability)\n",
    "        dz_dw = features_batch[i]\n",
    "        \n",
    "        gradient = dloss_dprob * dprob_dz * dz_dw\n",
    "        \n",
    "        total_gradient += gradient\n",
    "        total_loss += loss\n",
    "    \n",
    "    # Return average gradient and loss\n",
    "    avg_gradient = total_gradient / len(features_batch)\n",
    "    avg_loss = total_loss / len(features_batch)\n",
    "    \n",
    "    return avg_gradient, avg_loss\n",
    "\n",
    "def gradient_descent(features_batch, labels_batch, initial_weights, \n",
    "                    learning_rate=0.1, num_iterations=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Run gradient descent optimization.\n",
    "    \"\"\"\n",
    "    weights = initial_weights.copy()\n",
    "    weight_history = [weights.copy()]\n",
    "    loss_history = []\n",
    "    gradient_norms = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Starting gradient descent with learning rate {learning_rate}\")\n",
    "        print(f\"Initial weights: {weights}\")\n",
    "        print()\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Compute gradient and loss\n",
    "        gradient, avg_loss = compute_batch_gradient(features_batch, labels_batch, weights)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        \n",
    "        # Store history\n",
    "        loss_history.append(avg_loss)\n",
    "        gradient_norms.append(gradient_norm)\n",
    "        \n",
    "        # Update weights\n",
    "        weights = weights - learning_rate * gradient\n",
    "        weight_history.append(weights.copy())\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (iteration % 20 == 0 or iteration < 10):\n",
    "            print(f\"Iteration {iteration:3d}: Loss = {avg_loss:.4f}, |Gradient| = {gradient_norm:.4f}, Weights = {weights}\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        if gradient_norm < 1e-6:\n",
    "            if verbose:\n",
    "                print(f\"\\nConverged after {iteration} iterations!\")\n",
    "            break\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFinal weights: {weights}\")\n",
    "        print(f\"Final loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return weights, weight_history, loss_history, gradient_norms\n",
    "\n",
    "# Run gradient descent on our sports tweets\n",
    "print(\"TRAINING 'GO DOLPHINS!' SENTIMENT CLASSIFIER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start with random weights\n",
    "np.random.seed(42)  # For reproducibility\n",
    "initial_weights = np.random.normal(0, 0.1, 3)\n",
    "print(f\"Random initial weights: {initial_weights}\")\n",
    "print()\n",
    "\n",
    "# Run optimization\n",
    "optimal_weights, weight_history, loss_history, gradient_norms = gradient_descent(\n",
    "    features, labels, initial_weights, learning_rate=0.5, num_iterations=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimization process\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "iterations = range(len(loss_history))\n",
    "\n",
    "# Plot 1: Loss over time\n",
    "axes[0, 0].plot(iterations, loss_history, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Average Loss')\n",
    "axes[0, 0].set_title('Loss Reduction During Training')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')  # Log scale to see improvement better\n",
    "\n",
    "# Plot 2: Gradient magnitude over time\n",
    "axes[0, 1].plot(iterations, gradient_norms, 'r-', linewidth=2, marker='s', markersize=4)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Gradient Magnitude')\n",
    "axes[0, 1].set_title('Gradient Magnitude (Convergence Indicator)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# Plot 3: Weight evolution\n",
    "weight_history_array = np.array(weight_history)\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    axes[1, 0].plot(range(len(weight_history)), weight_history_array[:, i], \n",
    "                   linewidth=2, marker='o', markersize=3, label=f'{feature_name}')\n",
    "\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Weight Value')\n",
    "axes[1, 0].set_title('Weight Evolution During Training')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 4: Final predictions\n",
    "final_predictions = []\n",
    "for i in range(len(features)):\n",
    "    prob, _, _ = compute_loss_and_prediction(features[i], labels[i], optimal_weights)\n",
    "    final_predictions.append(prob)\n",
    "\n",
    "final_predictions = np.array(final_predictions)\n",
    "pos_mask = labels == 1\n",
    "neg_mask = labels == 0\n",
    "\n",
    "axes[1, 1].scatter(range(np.sum(pos_mask)), final_predictions[pos_mask], \n",
    "                  c='green', s=100, alpha=0.7, label='Positive tweets')\n",
    "axes[1, 1].scatter(range(np.sum(neg_mask)), final_predictions[neg_mask], \n",
    "                  c='red', s=100, alpha=0.7, label='Negative tweets')\n",
    "axes[1, 1].axhline(y=0.5, color='black', linestyle='--', alpha=0.7, label='Decision threshold')\n",
    "axes[1, 1].set_xlabel('Tweet Index')\n",
    "axes[1, 1].set_ylabel('Predicted Probability')\n",
    "axes[1, 1].set_title('Final Predictions After Training')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate final accuracy\n",
    "final_binary_predictions = (final_predictions > 0.5).astype(int)\n",
    "accuracy = np.mean(final_binary_predictions == labels)\n",
    "\n",
    "print(f\"\\nTRAINING RESULTS:\")\n",
    "print(f\"Final accuracy: {accuracy:.1%}\")\n",
    "print(f\"Initial loss: {loss_history[0]:.4f}\")\n",
    "print(f\"Final loss: {loss_history[-1]:.4f}\")\n",
    "print(f\"Loss reduction: {loss_history[0] - loss_history[-1]:.4f}\")\n",
    "print(f\"Training iterations: {len(loss_history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Learning Rate Exploration\n",
    "\n",
    "The learning rate is crucial for gradient descent. Let's explore how different step sizes affect the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "lr_results = {}\n",
    "\n",
    "print(\"LEARNING RATE COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        weights, weight_hist, loss_hist, grad_norms = gradient_descent(\n",
    "            features, labels, initial_weights.copy(), \n",
    "            learning_rate=lr, num_iterations=100, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Calculate final accuracy\n",
    "        final_preds = []\n",
    "        for i in range(len(features)):\n",
    "            prob, _, _ = compute_loss_and_prediction(features[i], labels[i], weights)\n",
    "            final_preds.append(prob)\n",
    "        \n",
    "        final_accuracy = np.mean((np.array(final_preds) > 0.5) == labels)\n",
    "        \n",
    "        lr_results[lr] = {\n",
    "            'final_weights': weights,\n",
    "            'loss_history': loss_hist,\n",
    "            'final_loss': loss_hist[-1],\n",
    "            'final_accuracy': final_accuracy,\n",
    "            'iterations': len(loss_hist),\n",
    "            'converged': grad_norms[-1] < 1e-3,\n",
    "            'stable': not (np.any(np.isnan(weights)) or np.any(np.isinf(weights)))\n",
    "        }\n",
    "        \n",
    "        status = \"✅ Converged\" if lr_results[lr]['converged'] else \"⚠️  Did not converge\"\n",
    "        if not lr_results[lr]['stable']:\n",
    "            status = \"❌ Unstable (NaN/Inf)\"\n",
    "        \n",
    "        print(f\"Final loss: {lr_results[lr]['final_loss']:.4f}\")\n",
    "        print(f\"Final accuracy: {lr_results[lr]['final_accuracy']:.1%}\")\n",
    "        print(f\"Iterations: {lr_results[lr]['iterations']}\")\n",
    "        print(f\"Status: {status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed: {str(e)}\")\n",
    "        lr_results[lr] = None\n",
    "\n",
    "# Visualize learning rate effects\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Loss curves for different learning rates\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    if lr_results[lr] is not None and lr_results[lr]['stable']:\n",
    "        loss_hist = lr_results[lr]['loss_history']\n",
    "        axes[0].plot(range(len(loss_hist)), loss_hist, \n",
    "                    color=colors[i], linewidth=2, label=f'α = {lr}')\n",
    "\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curves for Different Learning Rates')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Final performance summary\n",
    "valid_lrs = [lr for lr in learning_rates if lr_results[lr] is not None and lr_results[lr]['stable']]\n",
    "final_losses = [lr_results[lr]['final_loss'] for lr in valid_lrs]\n",
    "final_accuracies = [lr_results[lr]['final_accuracy'] for lr in valid_lrs]\n",
    "\n",
    "axes[1].scatter(valid_lrs, final_losses, s=100, alpha=0.7, color='red', label='Final Loss')\n",
    "ax2 = axes[1].twinx()\n",
    "ax2.scatter(valid_lrs, final_accuracies, s=100, alpha=0.7, color='blue', label='Final Accuracy')\n",
    "\n",
    "axes[1].set_xlabel('Learning Rate')\n",
    "axes[1].set_ylabel('Final Loss', color='red')\n",
    "ax2.set_ylabel('Final Accuracy', color='blue')\n",
    "axes[1].set_title('Learning Rate vs Final Performance')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add legends\n",
    "axes[1].legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze learning rate effects in detail\n",
    "print(\"\\nLEARNING RATE ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Find the best learning rate\n",
    "valid_results = [(lr, result) for lr, result in lr_results.items() \n",
    "                if result is not None and result['stable']]\n",
    "\n",
    "if valid_results:\n",
    "    # Sort by final loss (lower is better)\n",
    "    valid_results.sort(key=lambda x: x[1]['final_loss'])\n",
    "    \n",
    "    print(\"Learning rates ranked by final loss (best to worst):\")\n",
    "    for i, (lr, result) in enumerate(valid_results, 1):\n",
    "        converged = \"✓\" if result['converged'] else \"✗\"\n",
    "        print(f\"{i}. α = {lr:<4} | Loss: {result['final_loss']:.4f} | Accuracy: {result['final_accuracy']:.1%} | Converged: {converged}\")\n",
    "    \n",
    "    best_lr, best_result = valid_results[0]\n",
    "    print(f\"\\n🏆 Best learning rate: {best_lr}\")\n",
    "    print(f\"   Final loss: {best_result['final_loss']:.4f}\")\n",
    "    print(f\"   Final accuracy: {best_result['final_accuracy']:.1%}\")\n",
    "    print(f\"   Optimal weights: {best_result['final_weights']}\")\n",
    "\n",
    "print(\"\\nLEARNING RATE INSIGHTS:\")\n",
    "print(\"1. Too small (0.01): Slow convergence, many iterations needed\")\n",
    "print(\"2. Just right (0.1-0.5): Fast, stable convergence\")\n",
    "print(\"3. Too large (1.0+): May overshoot, unstable, or diverge\")\n",
    "print(\"4. Way too large (2.0+): Likely to explode or oscillate\")\n",
    "\n",
    "# Show what happens with extreme learning rates\n",
    "print(\"\\nWHY LEARNING RATE MATTERS:\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    if lr_results[lr] is None:\n",
    "        print(f\"α = {lr}: ❌ Training failed completely\")\n",
    "    elif not lr_results[lr]['stable']:\n",
    "        print(f\"α = {lr}: ❌ Weights became infinite (exploded)\")\n",
    "    elif lr_results[lr]['final_loss'] > 1.0:\n",
    "        print(f\"α = {lr}: ⚠️  Poor final performance\")\n",
    "    elif not lr_results[lr]['converged']:\n",
    "        print(f\"α = {lr}: ⚠️  Slow convergence\")\n",
    "    else:\n",
    "        print(f\"α = {lr}: ✅ Good performance and convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Full Training Loop\n",
    "\n",
    "Let's put it all together and watch gradient descent automatically learn to classify \"Go Dolphins!\" and all our other tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete training demonstration with detailed analysis\n",
    "\n",
    "def detailed_training_analysis(features, labels, texts, learning_rate=0.3, num_iterations=150):\n",
    "    \"\"\"\n",
    "    Run complete training with detailed tracking and analysis.\n",
    "    \"\"\"\n",
    "    # Initialize with small random weights\n",
    "    np.random.seed(42)\n",
    "    weights = np.random.normal(0, 0.1, 3)\n",
    "    \n",
    "    # Track everything\n",
    "    weight_history = [weights.copy()]\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    prediction_history = []\n",
    "    \n",
    "    print(f\"COMPLETE TRAINING DEMONSTRATION\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"Dataset: {len(texts)} sports tweets\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Initial weights: {weights}\")\n",
    "    print()\n",
    "    \n",
    "    # Training loop with detailed tracking\n",
    "    for iteration in range(num_iterations):\n",
    "        # Compute predictions and gradients for all examples\n",
    "        total_gradient = np.zeros_like(weights)\n",
    "        total_loss = 0.0\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(len(features)):\n",
    "            # Forward pass\n",
    "            prob, loss, _ = compute_loss_and_prediction(features[i], labels[i], weights)\n",
    "            predictions.append(prob)\n",
    "            \n",
    "            # Backward pass\n",
    "            if labels[i] == 1:\n",
    "                dloss_dprob = -1/prob\n",
    "            else:\n",
    "                dloss_dprob = 1/(1-prob)\n",
    "            \n",
    "            dprob_dz = prob * (1 - prob)\n",
    "            dz_dw = features[i]\n",
    "            \n",
    "            gradient = dloss_dprob * dprob_dz * dz_dw\n",
    "            total_gradient += gradient\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Average across all examples\n",
    "        avg_gradient = total_gradient / len(features)\n",
    "        avg_loss = total_loss / len(features)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        binary_predictions = (np.array(predictions) > 0.5).astype(int)\n",
    "        accuracy = np.mean(binary_predictions == labels)\n",
    "        \n",
    "        # Store history\n",
    "        loss_history.append(avg_loss)\n",
    "        accuracy_history.append(accuracy)\n",
    "        prediction_history.append(predictions.copy())\n",
    "        \n",
    "        # Update weights\n",
    "        weights = weights - learning_rate * avg_gradient\n",
    "        weight_history.append(weights.copy())\n",
    "        \n",
    "        # Print periodic updates\n",
    "        if iteration % 30 == 0 or iteration < 5:\n",
    "            print(f\"Iteration {iteration:3d}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.1%}, Weights = {weights}\")\n",
    "    \n",
    "    return weights, weight_history, loss_history, accuracy_history, prediction_history\n",
    "\n",
    "# Run the complete training\n",
    "final_weights, weight_hist, loss_hist, acc_hist, pred_hist = detailed_training_analysis(\n",
    "    features, labels, texts, learning_rate=0.3, num_iterations=100\n",
    ")\n",
    "\n",
    "print(f\"\\nTRAINING COMPLETE!\")\n",
    "print(f\"Final weights: {final_weights}\")\n",
    "print(f\"Final accuracy: {acc_hist[-1]:.1%}\")\n",
    "print(f\"Final loss: {loss_hist[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what the model learned\n",
    "print(\"\\nWHAT DID THE MODEL LEARN?\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Analyze final weights\n",
    "print(f\"Learned weights: {final_weights}\")\n",
    "print(\"\\nWeight interpretation:\")\n",
    "for i, (feature_name, weight) in enumerate(zip(feature_names, final_weights)):\n",
    "    strength = \"strongly\" if abs(weight) > 0.5 else \"moderately\" if abs(weight) > 0.2 else \"weakly\"\n",
    "    direction = \"positive\" if weight > 0 else \"negative\"\n",
    "    print(f\"  {feature_name:<15}: {weight:+.3f} → {strength} {direction} sentiment signal\")\n",
    "\n",
    "# Test on our key examples\n",
    "print(\"\\nFINAL PREDICTIONS ON KEY EXAMPLES:\")\n",
    "key_examples = [\n",
    "    (0, \"Go Dolphins!\"),\n",
    "    (1, \"Terrible game\"),\n",
    "    (2, \"Love the fins!\"),\n",
    "    (4, \"Great win!!\"),\n",
    "    (7, \"Worst season ever\")\n",
    "]\n",
    "\n",
    "for idx, text in key_examples:\n",
    "    prob, loss, raw_pred = compute_loss_and_prediction(features[idx], labels[idx], final_weights)\n",
    "    true_sentiment = \"Positive\" if labels[idx] == 1 else \"Negative\"\n",
    "    pred_sentiment = \"Positive\" if prob > 0.5 else \"Negative\"\n",
    "    confidence = prob if prob > 0.5 else (1 - prob)\n",
    "    correct = \"✅\" if (prob > 0.5) == (labels[idx] == 1) else \"❌\"\n",
    "    \n",
    "    print(f\"{correct} '{text:<20}' | True: {true_sentiment:<8} | Pred: {pred_sentiment:<8} ({confidence:.1%} confidence)\")\n",
    "\n",
    "# Show the learning trajectory\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training curves\n",
    "iterations = range(len(loss_hist))\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(iterations, loss_hist, 'b-', linewidth=2, label='Loss')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(iterations, acc_hist, 'r-', linewidth=2, label='Accuracy')\n",
    "ax1_twin.set_ylabel('Accuracy', color='red')\n",
    "ax1_twin.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "ax1.set_title('Training Progress: Loss and Accuracy')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Weight evolution\n",
    "weight_history_array = np.array(weight_hist)\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    axes[0, 1].plot(range(len(weight_hist)), weight_history_array[:, i], \n",
    "                   linewidth=2, marker='o', markersize=2, label=f'{feature_name}')\n",
    "\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Weight Value')\n",
    "axes[0, 1].set_title('How Weights Changed During Learning')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Prediction evolution for key examples\n",
    "pred_history_array = np.array(pred_hist)\n",
    "for idx, text in [(0, \"Go Dolphins!\"), (1, \"Terrible game\")]:\n",
    "    axes[1, 0].plot(range(len(pred_hist)), pred_history_array[:, idx], \n",
    "                   linewidth=2, label=f'\"{text}\" (true: {\"pos\" if labels[idx]==1 else \"neg\"})')\n",
    "\n",
    "axes[1, 0].axhline(y=0.5, color='black', linestyle='--', alpha=0.7, label='Decision threshold')\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Predicted Probability')\n",
    "axes[1, 0].set_title('How Predictions Improved During Training')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Final weight visualization\n",
    "axes[1, 1].bar(feature_names, final_weights, color=['blue', 'green', 'orange'], alpha=0.7)\n",
    "axes[1, 1].set_ylabel('Weight Value')\n",
    "axes[1, 1].set_title('Final Learned Weights')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 1].axhline(y=0, color='black', linewidth=1)\n",
    "\n",
    "# Add weight values on bars\n",
    "for i, (name, weight) in enumerate(zip(feature_names, final_weights)):\n",
    "    axes[1, 1].text(i, weight + 0.02 if weight > 0 else weight - 0.05, \n",
    "                    f'{weight:.3f}', ha='center', va='bottom' if weight > 0 else 'top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎉 GRADIENT DESCENT SUCCESS!\")\n",
    "print(\"The algorithm automatically discovered that:\")\n",
    "print(f\"- Team mentions (weight: {final_weights[1]:+.3f}) are important for positive sentiment\")\n",
    "print(f\"- Exclamation marks (weight: {final_weights[2]:+.3f}) indicate excitement/positivity\")\n",
    "print(f\"- Word count (weight: {final_weights[0]:+.3f}) has some influence\")\n",
    "print(\"\\nThis matches our intuition about sports sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You've now witnessed the magic of gradient descent - automatic optimization that finds optimal weights! Here's what we discovered:\n",
    "\n",
    "**🔑 Key Insights:**\n",
    "1. **Gradients show the steepest uphill direction** - so we go downhill (negative gradient)\n",
    "2. **The chain rule connects all the pieces** - from final loss back to every weight\n",
    "3. **Learning rate controls the step size** - too small is slow, too large is unstable\n",
    "4. **The algorithm discovers patterns automatically** - no human programming of decision rules!\n",
    "\n",
    "**🔗 The Connection:**\n",
    "- **Problem 1**: Text → Features (`[2, 1, 1]`)\n",
    "- **Problem 2**: Features + Weights → Predictions (via dot products)\n",
    "- **Problem 3**: Predictions + Truth → Loss (measuring quality)\n",
    "- **Problem 4**: Loss + Gradients → Automatic weight optimization\n",
    "- **Problem 5**: Coming up - How do we scale this to millions of examples?\n",
    "\n",
    "**The Big Picture:**\n",
    "Gradient descent is the engine that powers all machine learning. From simple classifiers to ChatGPT, this same algorithm (with variations) automatically discovers optimal parameters from data. You've now seen the mathematical heart of artificial intelligence!\n",
    "\n",
    "**Coming up in Problem 5: Matrix Operations**\n",
    "- How do we process thousands of tweets simultaneously?\n",
    "- What are the computational tricks that make large-scale ML possible?\n",
    "- How do GPUs accelerate the math we've been doing?\n",
    "\n",
    "The journey from \"Go Dolphins!\" to scalable machine learning is almost complete! 🐬➡️📊➡️🎯➡️⚡➡️🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}