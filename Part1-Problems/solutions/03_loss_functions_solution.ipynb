{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Loss Functions - How Models Measure Their Mistakes\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this problem, you will:\n",
    "- Understand what loss functions are and why they're crucial\n",
    "- Compare different loss functions (MSE vs Binary Cross-Entropy)\n",
    "- See how loss guides the learning process\n",
    "- Connect prediction quality to mathematical optimization\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "1. **Loss Function Basics** - Understand how models measure prediction quality\n",
    "2. **Compare MSE vs Binary Cross-Entropy** - Explore different loss function behaviors\n",
    "3. **Loss Landscapes** - Visualize how loss changes with different weights\n",
    "4. **Connecting Loss to Learning** - See how loss guides weight optimization\n",
    "\n",
    "---\n",
    "\n",
    "## The Story Continues\n",
    "\n",
    "In Problem 2, you learned how dot products transform features `[2, 1, 1]` into predictions. You saw different weight strategies produce different results:\n",
    "\n",
    "- Some weights correctly predicted \"Go Dolphins!\" as positive\n",
    "- Others failed miserably\n",
    "\n",
    "But here's the key question: **How do we know which weights are better?**\n",
    "\n",
    "The answer is **loss functions** - mathematical measures that tell us exactly how wrong our predictions are. Loss functions are the model's \"report card\" that guides the entire learning process.\n",
    "\n",
    "## What Is a Loss Function?\n",
    "\n",
    "**Simple definition**: A loss function measures the difference between what the model predicted and what actually happened.\n",
    "\n",
    "**Mathematical form**: `Loss = f(prediction, actual)`\n",
    "\n",
    "**Key properties**:\n",
    "- **Loss = 0** when prediction is perfect\n",
    "- **Loss > 0** when prediction is wrong\n",
    "- **Higher loss** = worse prediction\n",
    "\n",
    "**Why it matters**: Without loss functions, models have no way to improve. Loss provides the feedback signal that drives learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "# Import our utilities\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from data_generators import load_sports_dataset\n",
    "from visualization import plot_loss_landscape_3d\n",
    "\n",
    "# Load our data\n",
    "features, labels, feature_names, texts = load_sports_dataset()\n",
    "\n",
    "print(\"Continuing our 'Go Dolphins!' journey...\")\n",
    "print(f\"We have {len(texts)} tweets to evaluate\")\n",
    "print(f\"Our key example: '{texts[0]}' ‚Üí Features: {features[0]} ‚Üí True label: {labels[0]}\")\n",
    "\n",
    "# Let's see what some predictions look like\n",
    "example_weights = np.array([0.3, 0.5, 0.4])\n",
    "prediction = np.dot(features[0], example_weights)\n",
    "print(f\"\\nWith weights {example_weights}:\")\n",
    "print(f\"Raw prediction: {prediction:.3f}\")\n",
    "print(f\"Actual label: {labels[0]}\")\n",
    "print(f\"How wrong are we? That's what loss functions tell us!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Loss Function Basics\n",
    "\n",
    "Let's start with the fundamentals - understanding what loss functions measure and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic loss functions\n",
    "\n",
    "def mean_squared_error(prediction: float, actual: float) -> float:\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) - squares the difference\n",
    "    \"\"\"\n",
    "    return (prediction - actual) ** 2\n",
    "\n",
    "def binary_cross_entropy(prediction: float, actual: float) -> float:\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy - logarithmic loss for binary classification\n",
    "    \"\"\"\n",
    "    # Clip prediction to avoid log(0)\n",
    "    prediction = np.clip(prediction, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    if actual == 1:\n",
    "        return -np.log(prediction)\n",
    "    else:\n",
    "        return -np.log(1 - prediction)\n",
    "\n",
    "def sigmoid(x: float) -> float:\n",
    "    \"\"\"Convert raw prediction to probability\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "# Calculate loss for different prediction scenarios\n",
    "print(\"LOSS FUNCTION COMPARISON FOR 'GO DOLPHINS!'\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"True label: {labels[0]} (positive sentiment)\")\n",
    "print()\n",
    "\n",
    "# Test different prediction scenarios\n",
    "scenarios = [\n",
    "    (\"Perfect prediction\", 1.0),\n",
    "    (\"Good prediction\", 0.8),\n",
    "    (\"Uncertain prediction\", 0.6),\n",
    "    (\"Slightly wrong\", 0.4),\n",
    "    (\"Very wrong\", 0.1),\n",
    "    (\"Completely wrong\", 0.0),\n",
    "]\n",
    "\n",
    "for scenario_name, pred_prob in scenarios:\n",
    "    mse_loss = mean_squared_error(pred_prob, labels[0])\n",
    "    bce_loss = binary_cross_entropy(pred_prob, labels[0])\n",
    "    \n",
    "    print(f\"{scenario_name:<20} | Prediction: {pred_prob:.1f} | MSE: {mse_loss:.3f} | BCE: {bce_loss:.3f}\")\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Both losses are 0 when prediction is perfect\")\n",
    "print(\"- Both increase as predictions get worse\")\n",
    "print(\"- BCE grows much faster for confident wrong predictions\")\n",
    "print(\"- MSE treats all errors more 'equally'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement full loss calculation for the complete pipeline\n",
    "def calculate_full_loss(features: np.ndarray, labels: np.ndarray, weights: np.ndarray, \n",
    "                       loss_function: Callable) -> float:\n",
    "    \"\"\"\n",
    "    Calculate total loss across all tweets for given weights.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        # Step 1: Calculate raw prediction (dot product)\n",
    "        raw_prediction = np.dot(features[i], weights)\n",
    "        \n",
    "        # Step 2: Convert to probability (sigmoid)\n",
    "        probability = sigmoid(raw_prediction)\n",
    "        \n",
    "        # Step 3: Calculate loss\n",
    "        loss = loss_function(probability, labels[i])\n",
    "        total_loss += loss\n",
    "    \n",
    "    # Return average loss\n",
    "    return total_loss / len(features)\n",
    "\n",
    "# Test different weight strategies from Problem 2\n",
    "weight_strategies = {\n",
    "    \"Random weights\": np.array([0.1, 0.2, 0.1]),\n",
    "    \"Team-focused\": np.array([0.1, 0.8, 0.1]),\n",
    "    \"Excitement-focused\": np.array([0.1, 0.1, 0.8]),\n",
    "    \"Balanced weights\": np.array([0.3, 0.3, 0.4]),\n",
    "    \"Optimized-guess\": np.array([0.2, 0.5, 0.6]),\n",
    "}\n",
    "\n",
    "print(\"\\nLOSS EVALUATION FOR DIFFERENT WEIGHT STRATEGIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "strategy_losses = []\n",
    "\n",
    "for name, weights in weight_strategies.items():\n",
    "    mse_loss = calculate_full_loss(features, labels, weights, mean_squared_error)\n",
    "    bce_loss = calculate_full_loss(features, labels, weights, binary_cross_entropy)\n",
    "    \n",
    "    strategy_losses.append((name, weights, mse_loss, bce_loss))\n",
    "    print(f\"{name:<18} | MSE: {mse_loss:.4f} | BCE: {bce_loss:.4f} | Weights: {weights}\")\n",
    "\n",
    "# Rank by BCE loss (better for classification)\n",
    "strategy_losses.sort(key=lambda x: x[3])  # Sort by BCE loss\n",
    "\n",
    "print(\"\\nRANKING BY BCE LOSS (best to worst):\")\n",
    "for i, (name, weights, mse_loss, bce_loss) in enumerate(strategy_losses, 1):\n",
    "    print(f\"{i}. {name:<18} | BCE: {bce_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Compare MSE vs Binary Cross-Entropy\n",
    "\n",
    "Let's dive deeper into how different loss functions behave and why the choice matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed comparison of loss function behaviors\n",
    "\n",
    "# Generate range of predictions for visualization\n",
    "predictions = np.linspace(0.001, 0.999, 1000)  # Avoid exact 0 and 1 for BCE\n",
    "\n",
    "# Calculate losses for both positive and negative true labels\n",
    "mse_losses_pos = [mean_squared_error(p, 1.0) for p in predictions]\n",
    "mse_losses_neg = [mean_squared_error(p, 0.0) for p in predictions]\n",
    "\n",
    "bce_losses_pos = [binary_cross_entropy(p, 1.0) for p in predictions]\n",
    "bce_losses_neg = [binary_cross_entropy(p, 0.0) for p in predictions]\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: MSE for positive true label\n",
    "axes[0, 0].plot(predictions, mse_losses_pos, 'b-', linewidth=3, label='MSE Loss')\n",
    "axes[0, 0].set_xlabel('Prediction')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('MSE Loss (True Label = 1)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axvline(x=0.5, color='r', linestyle='--', alpha=0.7, label='Decision threshold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: BCE for positive true label\n",
    "axes[0, 1].plot(predictions, bce_losses_pos, 'r-', linewidth=3, label='BCE Loss')\n",
    "axes[0, 1].set_xlabel('Prediction')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('Binary Cross-Entropy Loss (True Label = 1)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axvline(x=0.5, color='r', linestyle='--', alpha=0.7, label='Decision threshold')\n",
    "axes[0, 1].set_ylim(0, 5)  # Limit y-axis for better visualization\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Direct comparison for positive label\n",
    "axes[1, 0].plot(predictions, mse_losses_pos, 'b-', linewidth=2, label='MSE Loss')\n",
    "axes[1, 0].plot(predictions, bce_losses_pos, 'r-', linewidth=2, label='BCE Loss')\n",
    "axes[1, 0].set_xlabel('Prediction')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('MSE vs BCE Comparison (True Label = 1)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axvline(x=0.5, color='k', linestyle='--', alpha=0.7, label='Decision threshold')\n",
    "axes[1, 0].set_ylim(0, 2)  # Focus on lower loss values\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 4: Key differences analysis\n",
    "# Calculate ratio of BCE to MSE loss\n",
    "loss_ratio = np.array(bce_losses_pos) / (np.array(mse_losses_pos) + 1e-10)\n",
    "axes[1, 1].plot(predictions, loss_ratio, 'g-', linewidth=3, label='BCE/MSE Ratio')\n",
    "axes[1, 1].set_xlabel('Prediction')\n",
    "axes[1, 1].set_ylabel('BCE/MSE Ratio')\n",
    "axes[1, 1].set_title('How Much More BCE Penalizes Wrong Predictions')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axvline(x=0.5, color='r', linestyle='--', alpha=0.7, label='Decision threshold')\n",
    "axes[1, 1].axhline(y=1, color='k', linestyle='-', alpha=0.5, label='Equal penalty')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"KEY DIFFERENCES BETWEEN MSE AND BCE:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Shape:\")\n",
    "print(\"   - MSE: Quadratic (parabola) - symmetric penalty\")\n",
    "print(\"   - BCE: Logarithmic - asymmetric, steep near extremes\")\n",
    "print(\"\\n2. Penalty for confident wrong predictions:\")\n",
    "print(\"   - MSE: Moderate penalty (max = 1.0)\")\n",
    "print(\"   - BCE: Severe penalty (approaches infinity)\")\n",
    "print(\"\\n3. Optimization behavior:\")\n",
    "print(\"   - MSE: Smoother gradients, easier optimization\")\n",
    "print(\"   - BCE: Stronger signal for bad predictions, faster learning\")\n",
    "print(\"\\n4. Use cases:\")\n",
    "print(\"   - MSE: Regression, when predictions are continuous\")\n",
    "print(\"   - BCE: Classification, when predictions are probabilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific examples to understand the differences\n",
    "print(\"DETAILED ANALYSIS: WHY BCE IS BETTER FOR CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze specific prediction scenarios\n",
    "test_cases = [\n",
    "    (\"Confident and correct\", 0.9, 1),\n",
    "    (\"Confident but wrong\", 0.9, 0),\n",
    "    (\"Uncertain\", 0.5, 1),\n",
    "    (\"Slightly wrong\", 0.4, 1),\n",
    "    (\"Very wrong\", 0.1, 1),\n",
    "]\n",
    "\n",
    "for description, prediction, true_label in test_cases:\n",
    "    mse = mean_squared_error(prediction, true_label)\n",
    "    bce = binary_cross_entropy(prediction, true_label)\n",
    "    \n",
    "    print(f\"\\n{description}:\")\n",
    "    print(f\"  Prediction: {prediction:.1f}, True: {true_label}\")\n",
    "    print(f\"  MSE loss: {mse:.3f}\")\n",
    "    print(f\"  BCE loss: {bce:.3f}\")\n",
    "    print(f\"  BCE/MSE ratio: {bce/mse:.1f}x\")\n",
    "    \n",
    "    if prediction > 0.8 and true_label == 0:\n",
    "        print(f\"  ‚Üí BCE heavily penalizes this confident wrong prediction!\")\n",
    "    elif prediction < 0.2 and true_label == 1:\n",
    "        print(f\"  ‚Üí BCE heavily penalizes this confident wrong prediction!\")\n",
    "    elif abs(prediction - true_label) < 0.2:\n",
    "        print(f\"  ‚Üí Both losses are reasonable for this good prediction\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSION: Why BCE is preferred for classification:\")\n",
    "print(\"1. Punishes confident wrong predictions much more severely\")\n",
    "print(\"2. Encourages the model to be more careful about confidence\")\n",
    "print(\"3. Provides stronger learning signals when the model is wrong\")\n",
    "print(\"4. Better mathematical properties for probability optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Loss Landscapes\n",
    "\n",
    "Now let's visualize how loss changes as we adjust weights - this creates the \"landscape\" that optimization algorithms navigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss landscape visualization\n",
    "# We'll fix one weight and vary two others to create 2D/3D visualizations\n",
    "\n",
    "def create_loss_landscape(weight1_range: tuple, weight2_range: tuple, fixed_weight3: float = 0.3,\n",
    "                         resolution: int = 50):\n",
    "    \"\"\"\n",
    "    Create a 2D loss landscape by varying two weights.\n",
    "    \"\"\"\n",
    "    w1_vals = np.linspace(weight1_range[0], weight1_range[1], resolution)\n",
    "    w2_vals = np.linspace(weight2_range[0], weight2_range[1], resolution)\n",
    "    \n",
    "    W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "    \n",
    "    # Initialize loss surfaces\n",
    "    MSE_loss = np.zeros_like(W1)\n",
    "    BCE_loss = np.zeros_like(W1)\n",
    "    \n",
    "    # Calculate loss at each point\n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            weights = np.array([W1[i, j], W2[i, j], fixed_weight3])\n",
    "            \n",
    "            MSE_loss[i, j] = calculate_full_loss(features, labels, weights, mean_squared_error)\n",
    "            BCE_loss[i, j] = calculate_full_loss(features, labels, weights, binary_cross_entropy)\n",
    "    \n",
    "    return W1, W2, MSE_loss, BCE_loss\n",
    "\n",
    "# Generate loss landscape\n",
    "print(\"Generating loss landscape...\")\n",
    "W1, W2, MSE_surface, BCE_surface = create_loss_landscape(\n",
    "    weight1_range=(-0.5, 1.0),  # word_count weight\n",
    "    weight2_range=(-0.5, 1.0),  # has_team weight\n",
    "    fixed_weight3=0.4,          # has_exclamation weight (fixed)\n",
    "    resolution=30\n",
    ")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: MSE Loss Surface\n",
    "contour1 = axes[0].contourf(W1, W2, MSE_surface, levels=20, cmap='viridis')\n",
    "axes[0].contour(W1, W2, MSE_surface, levels=20, colors='white', alpha=0.5, linewidths=0.5)\n",
    "fig.colorbar(contour1, ax=axes[0], label='MSE Loss')\n",
    "axes[0].set_xlabel('Weight 1 (word_count)')\n",
    "axes[0].set_ylabel('Weight 2 (has_team)')\n",
    "axes[0].set_title('MSE Loss Landscape')\n",
    "\n",
    "# Find and mark minimum\n",
    "min_idx = np.unravel_index(np.argmin(MSE_surface), MSE_surface.shape)\n",
    "axes[0].plot(W1[min_idx], W2[min_idx], 'r*', markersize=15, label=f'MSE Min: ({W1[min_idx]:.2f}, {W2[min_idx]:.2f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: BCE Loss Surface\n",
    "contour2 = axes[1].contourf(W1, W2, BCE_surface, levels=20, cmap='plasma')\n",
    "axes[1].contour(W1, W2, BCE_surface, levels=20, colors='white', alpha=0.5, linewidths=0.5)\n",
    "fig.colorbar(contour2, ax=axes[1], label='BCE Loss')\n",
    "axes[1].set_xlabel('Weight 1 (word_count)')\n",
    "axes[1].set_ylabel('Weight 2 (has_team)')\n",
    "axes[1].set_title('Binary Cross-Entropy Loss Landscape')\n",
    "\n",
    "# Find and mark minimum\n",
    "min_idx_bce = np.unravel_index(np.argmin(BCE_surface), BCE_surface.shape)\n",
    "axes[1].plot(W1[min_idx_bce], W2[min_idx_bce], 'r*', markersize=15, \n",
    "            label=f'BCE Min: ({W1[min_idx_bce]:.2f}, {W2[min_idx_bce]:.2f})')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: Weight strategies from earlier\n",
    "axes[2].contourf(W1, W2, BCE_surface, levels=20, cmap='plasma', alpha=0.7)\n",
    "axes[2].contour(W1, W2, BCE_surface, levels=10, colors='white', alpha=0.5, linewidths=0.5)\n",
    "\n",
    "# Plot our weight strategies\n",
    "strategy_colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for i, (name, weights) in enumerate(weight_strategies.items()):\n",
    "    if weights[0] >= W1.min() and weights[0] <= W1.max() and weights[1] >= W2.min() and weights[1] <= W2.max():\n",
    "        axes[2].plot(weights[0], weights[1], 'o', color=strategy_colors[i], \n",
    "                    markersize=10, label=name, markeredgecolor='black', markeredgewidth=2)\n",
    "\n",
    "axes[2].set_xlabel('Weight 1 (word_count)')\n",
    "axes[2].set_ylabel('Weight 2 (has_team)')\n",
    "axes[2].set_title('Weight Strategies on BCE Landscape')\n",
    "axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal weights found:\")\n",
    "print(f\"MSE minimum at: w1={W1[min_idx]:.3f}, w2={W2[min_idx]:.3f}, loss={MSE_surface[min_idx]:.4f}\")\n",
    "print(f\"BCE minimum at: w1={W1[min_idx_bce]:.3f}, w2={W2[min_idx_bce]:.3f}, loss={BCE_surface[min_idx_bce]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the loss landscape properties\n",
    "print(\"LOSS LANDSCAPE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate landscape statistics\n",
    "mse_min, mse_max = MSE_surface.min(), MSE_surface.max()\n",
    "bce_min, bce_max = BCE_surface.min(), BCE_surface.max()\n",
    "\n",
    "print(f\"MSE Loss Range: {mse_min:.4f} to {mse_max:.4f} (range: {mse_max-mse_min:.4f})\")\n",
    "print(f\"BCE Loss Range: {bce_min:.4f} to {bce_max:.4f} (range: {bce_max-bce_min:.4f})\")\n",
    "\n",
    "# Analyze smoothness (gradient variation)\n",
    "mse_gradients = np.gradient(MSE_surface)\n",
    "bce_gradients = np.gradient(BCE_surface)\n",
    "\n",
    "mse_gradient_variation = np.std(mse_gradients[0]) + np.std(mse_gradients[1])\n",
    "bce_gradient_variation = np.std(bce_gradients[0]) + np.std(bce_gradients[1])\n",
    "\n",
    "print(f\"\\nLandscape smoothness (lower = smoother):\")\n",
    "print(f\"MSE gradient variation: {mse_gradient_variation:.4f}\")\n",
    "print(f\"BCE gradient variation: {bce_gradient_variation:.4f}\")\n",
    "\n",
    "# Find areas of steepest descent\n",
    "mse_gradient_magnitude = np.sqrt(mse_gradients[0]**2 + mse_gradients[1]**2)\n",
    "bce_gradient_magnitude = np.sqrt(bce_gradients[0]**2 + bce_gradients[1]**2)\n",
    "\n",
    "print(f\"\\nSteepest gradients (higher = steeper):\")\n",
    "print(f\"MSE max gradient: {mse_gradient_magnitude.max():.4f}\")\n",
    "print(f\"BCE max gradient: {bce_gradient_magnitude.max():.4f}\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Both landscapes have clear minima (good for optimization)\")\n",
    "print(\"2. BCE typically has steeper gradients (faster learning)\")\n",
    "print(\"3. The optimal weights differ between MSE and BCE\")\n",
    "print(\"4. This landscape guides gradient descent to find optimal weights\")\n",
    "\n",
    "# Show how our weight strategies perform\n",
    "print(\"\\nHOW OUR WEIGHT STRATEGIES RANK ON THE LANDSCAPE:\")\n",
    "for i, (name, weights, mse_loss, bce_loss) in enumerate(strategy_losses, 1):\n",
    "    percentile_mse = (1 - (mse_loss - mse_min) / (mse_max - mse_min)) * 100\n",
    "    percentile_bce = (1 - (bce_loss - bce_min) / (bce_max - bce_min)) * 100\n",
    "    \n",
    "    print(f\"{i}. {name:<18} | BCE percentile: {percentile_bce:.1f}% | MSE percentile: {percentile_mse:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Connecting Loss to Learning\n",
    "\n",
    "Finally, let's see how loss functions guide the learning process and connect to the optimization we'll explore in Problem 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how loss provides learning signals\n",
    "\n",
    "def calculate_prediction_breakdown(features: np.ndarray, labels: np.ndarray, \n",
    "                                 weights: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze predictions and losses for each example.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'texts': [],\n",
    "        'true_labels': [],\n",
    "        'raw_predictions': [],\n",
    "        'probabilities': [],\n",
    "        'binary_predictions': [],\n",
    "        'mse_losses': [],\n",
    "        'bce_losses': [],\n",
    "        'correct': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        # Forward pass\n",
    "        raw_pred = np.dot(features[i], weights)\n",
    "        prob = sigmoid(raw_pred)\n",
    "        binary_pred = 1 if prob > 0.5 else 0\n",
    "        \n",
    "        # Calculate losses\n",
    "        mse_loss = mean_squared_error(prob, labels[i])\n",
    "        bce_loss = binary_cross_entropy(prob, labels[i])\n",
    "        \n",
    "        # Store results\n",
    "        results['texts'].append(texts[i])\n",
    "        results['true_labels'].append(labels[i])\n",
    "        results['raw_predictions'].append(raw_pred)\n",
    "        results['probabilities'].append(prob)\n",
    "        results['binary_predictions'].append(binary_pred)\n",
    "        results['mse_losses'].append(mse_loss)\n",
    "        results['bce_losses'].append(bce_loss)\n",
    "        results['correct'].append(binary_pred == labels[i])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze our best performing weights\n",
    "best_weights = strategy_losses[0][1]  # Best BCE performing weights\n",
    "worst_weights = strategy_losses[-1][1]  # Worst BCE performing weights\n",
    "\n",
    "print(\"LEARNING SIGNAL ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nBest weights: {best_weights} (BCE loss: {strategy_losses[0][3]:.4f})\")\n",
    "print(f\"Worst weights: {worst_weights} (BCE loss: {strategy_losses[-1][3]:.4f})\")\n",
    "\n",
    "# Analyze predictions with both weight sets\n",
    "best_results = calculate_prediction_breakdown(features, labels, best_weights)\n",
    "worst_results = calculate_prediction_breakdown(features, labels, worst_weights)\n",
    "\n",
    "print(\"\\nDETAILED PREDICTION ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Text':<25} | {'True':<4} | {'Best Prob':<9} | {'Best Loss':<9} | {'Worst Prob':<10} | {'Worst Loss':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    text_short = texts[i][:23] + '..' if len(texts[i]) > 25 else texts[i]\n",
    "    true_label = 'Pos' if labels[i] == 1 else 'Neg'\n",
    "    \n",
    "    best_prob = best_results['probabilities'][i]\n",
    "    best_loss = best_results['bce_losses'][i]\n",
    "    worst_prob = worst_results['probabilities'][i]\n",
    "    worst_loss = worst_results['bce_losses'][i]\n",
    "    \n",
    "    print(f\"{text_short:<25} | {true_label:<4} | {best_prob:<9.3f} | {best_loss:<9.3f} | {worst_prob:<10.3f} | {worst_loss:<10.3f}\")\n",
    "\n",
    "# Calculate learning signals\n",
    "best_accuracy = np.mean(best_results['correct'])\n",
    "worst_accuracy = np.mean(worst_results['correct'])\n",
    "best_avg_loss = np.mean(best_results['bce_losses'])\n",
    "worst_avg_loss = np.mean(worst_results['bce_losses'])\n",
    "\n",
    "print(f\"\\nPERFORMANCE SUMMARY:\")\n",
    "print(f\"Best weights  - Accuracy: {best_accuracy:.1%}, Avg Loss: {best_avg_loss:.4f}\")\n",
    "print(f\"Worst weights - Accuracy: {worst_accuracy:.1%}, Avg Loss: {worst_avg_loss:.4f}\")\n",
    "print(f\"Improvement   - Accuracy: {best_accuracy-worst_accuracy:+.1%}, Loss reduction: {worst_avg_loss-best_avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how loss guides optimization direction\n",
    "print(\"\\nHOW LOSS GUIDES LEARNING:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Analyze which examples provide strongest learning signals\n",
    "learning_signals = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    best_loss = best_results['bce_losses'][i]\n",
    "    worst_loss = worst_results['bce_losses'][i]\n",
    "    loss_improvement = worst_loss - best_loss\n",
    "    \n",
    "    learning_signals.append({\n",
    "        'text': texts[i],\n",
    "        'true_label': labels[i],\n",
    "        'best_loss': best_loss,\n",
    "        'worst_loss': worst_loss,\n",
    "        'improvement': loss_improvement,\n",
    "        'best_correct': best_results['correct'][i],\n",
    "        'worst_correct': worst_results['correct'][i]\n",
    "    })\n",
    "\n",
    "# Sort by learning signal strength (loss improvement)\n",
    "learning_signals.sort(key=lambda x: x['improvement'], reverse=True)\n",
    "\n",
    "print(\"Examples ranked by learning signal strength:\")\n",
    "print(\"(How much the loss improved from worst to best weights)\")\n",
    "print()\n",
    "\n",
    "for i, signal in enumerate(learning_signals[:8], 1):  # Show top 8\n",
    "    text_short = signal['text'][:30] + '..' if len(signal['text']) > 32 else signal['text']\n",
    "    true_sentiment = 'Positive' if signal['true_label'] == 1 else 'Negative'\n",
    "    \n",
    "    print(f\"{i}. '{text_short:<32}' ({true_sentiment})\")\n",
    "    print(f\"   Loss improvement: {signal['improvement']:.4f}\")\n",
    "    print(f\"   Best weights: {'‚úì' if signal['best_correct'] else '‚úó'} | Worst weights: {'‚úì' if signal['worst_correct'] else '‚úó'}\")\n",
    "    \n",
    "    if signal['improvement'] > 1.0:\n",
    "        print(f\"   ‚Üí Strong learning signal! This example teaches the model a lot.\")\n",
    "    elif signal['improvement'] > 0.5:\n",
    "        print(f\"   ‚Üí Moderate learning signal.\")\n",
    "    else:\n",
    "        print(f\"   ‚Üí Weak learning signal.\")\n",
    "    print()\n",
    "\n",
    "print(\"KEY INSIGHTS ABOUT LOSS-DRIVEN LEARNING:\")\n",
    "print(\"1. Examples with biggest loss improvements provide strongest learning signals\")\n",
    "print(\"2. Loss tells us exactly which direction to adjust weights\")\n",
    "print(\"3. Better weights produce lower loss across all examples\")\n",
    "print(\"4. Loss reduction correlates with accuracy improvement\")\n",
    "print(\"\\nNext up: Problem 4 will show HOW to automatically find these optimal weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You've now discovered how loss functions measure prediction quality and guide learning! Here's what we learned:\n",
    "\n",
    "**üîë Key Insights:**\n",
    "1. **Loss functions are the model's report card** - they measure how wrong predictions are\n",
    "2. **Different loss functions have different behaviors** - BCE punishes confident wrong predictions more severely\n",
    "3. **Loss landscapes show the optimization challenge** - finding the minimum in weight space\n",
    "4. **Loss provides learning signals** - telling us which direction to adjust weights\n",
    "\n",
    "**üîó The Connection:**\n",
    "- **Problem 1**: Text ‚Üí Features (`[2, 1, 1]`)\n",
    "- **Problem 2**: Features + Weights ‚Üí Predictions (via dot products)\n",
    "- **Problem 3**: Predictions + Truth ‚Üí Loss (measuring quality)\n",
    "- **Problem 4**: Coming up - How do we automatically find weights that minimize loss?\n",
    "\n",
    "**The Big Picture:**\n",
    "Loss functions transform machine learning from guessing to systematic optimization. They provide the mathematical foundation that allows models to learn from their mistakes and improve automatically.\n",
    "\n",
    "**Coming up in Problem 4: Gradient Descent**\n",
    "- How do we automatically find optimal weights?\n",
    "- What is gradient descent and why does it work?\n",
    "- How do we \"roll the ball downhill\" in the loss landscape?\n",
    "\n",
    "The journey from \"Go Dolphins!\" to automatic learning continues! üê¨‚û°Ô∏èüìä‚û°Ô∏èüéØ‚û°Ô∏è‚ö°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}