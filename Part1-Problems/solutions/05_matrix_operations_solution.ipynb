{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Matrix Operations - Scaling to Reality\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this problem, you will:\n",
    "- Understand how matrices enable batch processing of multiple examples\n",
    "- See how matrix operations scale machine learning to millions of samples\n",
    "- Explore the computational advantages of vectorized operations\n",
    "- Connect individual calculations to the linear algebra that powers modern AI\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "1. **From Loops to Matrices** - Transform individual calculations into batch operations\n",
    "2. **Matrix Multiplication Deep Dive** - Understand the fundamental operation of ML\n",
    "3. **Computational Efficiency** - Compare vectorized vs loop-based approaches\n",
    "4. **Scaling to Real Datasets** - See how this enables modern deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## The Story Continues\n",
    "\n",
    "In Problems 1-4, you've built a complete sentiment classifier:\n",
    "- **Problem 1**: \"Go Dolphins!\" â†’ Features `[2, 1, 1]`\n",
    "- **Problem 2**: Features + Weights â†’ Predictions (dot products)\n",
    "- **Problem 3**: Predictions + Truth â†’ Loss (quality measurement)\n",
    "- **Problem 4**: Loss + Gradients â†’ Automatic optimization\n",
    "\n",
    "But there's a crucial challenge: **How do we scale this to real datasets?**\n",
    "\n",
    "Our current approach processes one tweet at a time in Python loops. This works fine for 16 tweets, but what about:\n",
    "- **1,000 tweets** per batch?\n",
    "- **1,000,000 tweets** in a dataset?\n",
    "- **1,000,000,000 parameters** in a model like ChatGPT?\n",
    "\n",
    "The answer is **matrix operations** - mathematical operations that process thousands of examples simultaneously using highly optimized linear algebra libraries.\n",
    "\n",
    "## What Are Matrix Operations?\n",
    "\n",
    "**Key insight**: Instead of processing examples one-by-one in loops, we organize data into matrices and use single operations to process everything at once.\n",
    "\n",
    "**The transformation**:\n",
    "```python\n",
    "# Slow: Loop over examples\n",
    "for i in range(1000):\n",
    "    prediction[i] = dot_product(features[i], weights)\n",
    "\n",
    "# Fast: Single matrix operation\n",
    "predictions = features @ weights  # All 1000 predictions at once!\n",
    "```\n",
    "\n",
    "**Why this matters**:\n",
    "- **GPUs excel at parallel matrix operations**\n",
    "- **Optimized libraries** (BLAS, cuBLAS) are incredibly fast\n",
    "- **Modern deep learning** requires processing millions of parameters\n",
    "- **ChatGPT** uses this exact approach at massive scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "# Import our utilities\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from data_generators import load_sports_dataset, generate_synthetic_sports_data\n",
    "\n",
    "# Load our original data\n",
    "features, labels, feature_names, texts = load_sports_dataset()\n",
    "\n",
    "print(\"TRANSITIONING FROM INDIVIDUAL TO BATCH PROCESSING\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Original dataset: {len(texts)} tweets\")\n",
    "print(f\"Feature matrix shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print()\n",
    "print(\"Matrix view of our data:\")\n",
    "print(f\"Features (first 5 rows):\")\n",
    "print(features[:5])\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Our learned weights from Problem 4\n",
    "learned_weights = np.array([0.3, 0.5, 0.4])  # Approximate from gradient descent\n",
    "print(f\"\\nLearned weights: {learned_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: From Loops to Matrices\n",
    "\n",
    "Let's transform our individual tweet processing into batch matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loop-based vs matrix-based approaches\n",
    "\n",
    "def predict_with_loops(features_matrix, weights):\n",
    "    \"\"\"\n",
    "    Old way: Process each example individually with loops\n",
    "    \"\"\"\n",
    "    predictions = np.zeros(len(features_matrix))\n",
    "    \n",
    "    for i in range(len(features_matrix)):\n",
    "        # Individual dot product for each tweet\n",
    "        raw_prediction = 0.0\n",
    "        for j in range(len(weights)):\n",
    "            raw_prediction += features_matrix[i, j] * weights[j]\n",
    "        \n",
    "        # Apply sigmoid\n",
    "        predictions[i] = 1 / (1 + np.exp(-raw_prediction))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_with_matrices(features_matrix, weights):\n",
    "    \"\"\"\n",
    "    New way: Process all examples at once with matrix operations\n",
    "    \"\"\"\n",
    "    # Single matrix-vector multiplication for all tweets\n",
    "    raw_predictions = features_matrix @ weights  # Matrix multiplication!\n",
    "    \n",
    "    # Apply sigmoid to all predictions at once\n",
    "    predictions = 1 / (1 + np.exp(-raw_predictions))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test both approaches\n",
    "print(\"COMPARING LOOP-BASED VS MATRIX-BASED PREDICTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Time the operations\n",
    "start_time = time.time()\n",
    "loop_predictions = predict_with_loops(features, learned_weights)\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "matrix_predictions = predict_with_matrices(features, learned_weights)\n",
    "matrix_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop-based approach time: {loop_time:.6f} seconds\")\n",
    "print(f\"Matrix-based approach time: {matrix_time:.6f} seconds\")\n",
    "print(f\"Speed improvement: {loop_time/matrix_time:.1f}x faster\")\n",
    "print()\n",
    "\n",
    "# Verify they give the same results\n",
    "max_difference = np.max(np.abs(loop_predictions - matrix_predictions))\n",
    "print(f\"Maximum difference between approaches: {max_difference:.2e}\")\n",
    "print(f\"Results match: {'âœ…' if max_difference < 1e-10 else 'âŒ'}\")\n",
    "print()\n",
    "\n",
    "# Show detailed comparison\n",
    "print(\"DETAILED PREDICTION COMPARISON:\")\n",
    "print(f\"{'Tweet':<25} | {'Loop':<8} | {'Matrix':<8} | {'Match':<5}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    text_short = texts[i][:23] + '..' if len(texts[i]) > 25 else texts[i]\n",
    "    loop_pred = loop_predictions[i]\n",
    "    matrix_pred = matrix_predictions[i]\n",
    "    match = \"âœ…\" if abs(loop_pred - matrix_pred) < 1e-10 else \"âŒ\"\n",
    "    \n",
    "    print(f\"{text_short:<25} | {loop_pred:<8.4f} | {matrix_pred:<8.4f} | {match:<5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement batch gradient computation using matrices\n",
    "def compute_gradients_with_loops(features_matrix, labels_array, weights):\n",
    "    \"\"\"\n",
    "    Compute gradients using loops (the old way)\n",
    "    \"\"\"\n",
    "    total_gradient = np.zeros_like(weights)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i in range(len(features_matrix)):\n",
    "        # Forward pass for one example\n",
    "        raw_pred = np.dot(features_matrix[i], weights)\n",
    "        prob = 1 / (1 + np.exp(-raw_pred))\n",
    "        \n",
    "        # Loss for one example\n",
    "        if labels_array[i] == 1:\n",
    "            loss = -np.log(prob)\n",
    "            dloss_dprob = -1/prob\n",
    "        else:\n",
    "            loss = -np.log(1 - prob)\n",
    "            dloss_dprob = 1/(1-prob)\n",
    "        \n",
    "        # Gradient for one example\n",
    "        dprob_dz = prob * (1 - prob)\n",
    "        gradient = dloss_dprob * dprob_dz * features_matrix[i]\n",
    "        \n",
    "        total_gradient += gradient\n",
    "        total_loss += loss\n",
    "    \n",
    "    return total_gradient / len(features_matrix), total_loss / len(features_matrix)\n",
    "\n",
    "def compute_gradients_with_matrices(features_matrix, labels_array, weights):\n",
    "    \"\"\"\n",
    "    Compute gradients using matrix operations (the new way)\n",
    "    \"\"\"\n",
    "    # Forward pass for all examples at once\n",
    "    raw_predictions = features_matrix @ weights  # Matrix-vector multiplication\n",
    "    probabilities = 1 / (1 + np.exp(-raw_predictions))  # Vectorized sigmoid\n",
    "    \n",
    "    # Loss for all examples at once\n",
    "    # BCE: -y*log(p) - (1-y)*log(1-p)\n",
    "    losses = -labels_array * np.log(probabilities) - (1 - labels_array) * np.log(1 - probabilities)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    # Gradient computation (vectorized)\n",
    "    # For BCE with sigmoid: gradient = (predictions - targets) / n\n",
    "    errors = probabilities - labels_array  # Prediction errors\n",
    "    gradient = features_matrix.T @ errors / len(features_matrix)  # Matrix-vector multiplication\n",
    "    \n",
    "    return gradient, avg_loss\n",
    "\n",
    "# Compare gradient computation approaches\n",
    "print(\"\\nCOMPARING GRADIENT COMPUTATION METHODS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Time both approaches\n",
    "start_time = time.time()\n",
    "loop_gradient, loop_loss = compute_gradients_with_loops(features, labels, learned_weights)\n",
    "loop_grad_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "matrix_gradient, matrix_loss = compute_gradients_with_matrices(features, labels, learned_weights)\n",
    "matrix_grad_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop gradient computation: {loop_grad_time:.6f} seconds\")\n",
    "print(f\"Matrix gradient computation: {matrix_grad_time:.6f} seconds\")\n",
    "print(f\"Speed improvement: {loop_grad_time/matrix_grad_time:.1f}x faster\")\n",
    "print()\n",
    "\n",
    "print(f\"Gradient comparison:\")\n",
    "print(f\"Loop method:   {loop_gradient}\")\n",
    "print(f\"Matrix method: {matrix_gradient}\")\n",
    "print(f\"Max difference: {np.max(np.abs(loop_gradient - matrix_gradient)):.2e}\")\n",
    "print(f\"Gradients match: {'âœ…' if np.max(np.abs(loop_gradient - matrix_gradient)) < 1e-10 else 'âŒ'}\")\n",
    "print()\n",
    "\n",
    "print(f\"Loss comparison:\")\n",
    "print(f\"Loop method:   {loop_loss:.6f}\")\n",
    "print(f\"Matrix method: {matrix_loss:.6f}\")\n",
    "print(f\"Loss difference: {abs(loop_loss - matrix_loss):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Matrix Multiplication Deep Dive\n",
    "\n",
    "Let's understand exactly what happens in matrix multiplication and why it's so powerful for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into matrix multiplication mechanics\n",
    "\n",
    "def visualize_matrix_multiplication(features_matrix, weights_vector):\n",
    "    \"\"\"\n",
    "    Break down matrix multiplication step by step\n",
    "    \"\"\"\n",
    "    print(\"MATRIX MULTIPLICATION BREAKDOWN\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Features matrix shape: {features_matrix.shape}\")\n",
    "    print(f\"Weights vector shape: {weights_vector.shape}\")\n",
    "    print(f\"Result shape: ({features_matrix.shape[0]},)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Features matrix (first 5 rows):\")\n",
    "    print(features_matrix[:5])\n",
    "    print(f\"\\nWeights vector: {weights_vector}\")\n",
    "    print()\n",
    "    \n",
    "    # Show the operation visually\n",
    "    print(\"Matrix multiplication: Features @ Weights\")\n",
    "    print(\"Each row of features gets dot product with weights:\")\n",
    "    print()\n",
    "    \n",
    "    result = np.zeros(features_matrix.shape[0])\n",
    "    for i in range(min(5, features_matrix.shape[0])):  # Show first 5\n",
    "        row = features_matrix[i]\n",
    "        dot_product = np.dot(row, weights_vector)\n",
    "        result[i] = dot_product\n",
    "        \n",
    "        print(f\"Row {i}: {row} Â· {weights_vector} = {dot_product:.4f}\")\n",
    "        print(f\"       = {row[0]}Ã—{weights_vector[0]} + {row[1]}Ã—{weights_vector[1]} + {row[2]}Ã—{weights_vector[2]}\")\n",
    "        print(f\"       = {row[0]*weights_vector[0]:.3f} + {row[1]*weights_vector[1]:.3f} + {row[2]*weights_vector[2]:.3f} = {dot_product:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Compare with numpy result\n",
    "    numpy_result = features_matrix @ weights_vector\n",
    "    print(f\"NumPy result (all at once): {numpy_result[:5]}\")\n",
    "    print(f\"Manual calculation:          {result[:5]}\")\n",
    "    print(f\"Results match: {'âœ…' if np.allclose(numpy_result[:5], result[:5]) else 'âŒ'}\")\n",
    "    \n",
    "    return numpy_result\n",
    "\n",
    "# Demonstrate with our data\n",
    "predictions = visualize_matrix_multiplication(features, learned_weights)\n",
    "\n",
    "print(f\"\\nAll {len(predictions)} predictions computed simultaneously!\")\n",
    "print(f\"Full prediction vector: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore different matrix shapes and operations\n",
    "print(\"\\nEXPLORING MATRIX DIMENSIONS IN NEURAL NETWORKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate different network architectures\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"Our Current Setup\",\n",
    "        \"batch_size\": 16,\n",
    "        \"input_features\": 3,\n",
    "        \"output_size\": 1,\n",
    "        \"description\": \"16 tweets, 3 features each, 1 prediction each\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Larger Batch\",\n",
    "        \"batch_size\": 1000,\n",
    "        \"input_features\": 3,\n",
    "        \"output_size\": 1,\n",
    "        \"description\": \"1000 tweets, 3 features each, 1 prediction each\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"More Features\",\n",
    "        \"batch_size\": 100,\n",
    "        \"input_features\": 100,\n",
    "        \"output_size\": 1,\n",
    "        \"description\": \"100 tweets, 100 features each, 1 prediction each\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Hidden Layer\",\n",
    "        \"batch_size\": 100,\n",
    "        \"input_features\": 50,\n",
    "        \"output_size\": 10,\n",
    "        \"description\": \"100 examples, 50 input features, 10 hidden neurons\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Deep Learning Scale\",\n",
    "        \"batch_size\": 1024,\n",
    "        \"input_features\": 512,\n",
    "        \"output_size\": 256,\n",
    "        \"description\": \"Typical deep learning layer sizes\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    batch_size = scenario[\"batch_size\"]\n",
    "    input_dim = scenario[\"input_features\"]\n",
    "    output_dim = scenario[\"output_size\"]\n",
    "    \n",
    "    # Create random matrices of appropriate sizes\n",
    "    input_matrix = np.random.randn(batch_size, input_dim)\n",
    "    weight_matrix = np.random.randn(input_dim, output_dim)\n",
    "    \n",
    "    # Time the matrix multiplication\n",
    "    start_time = time.time()\n",
    "    result = input_matrix @ weight_matrix\n",
    "    computation_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate computational complexity\n",
    "    operations = batch_size * input_dim * output_dim\n",
    "    \n",
    "    print(f\"\\n{scenario['name']}:\")\n",
    "    print(f\"  {scenario['description']}\")\n",
    "    print(f\"  Input matrix: {input_matrix.shape}\")\n",
    "    print(f\"  Weight matrix: {weight_matrix.shape}\")\n",
    "    print(f\"  Output matrix: {result.shape}\")\n",
    "    print(f\"  Multiplication operations: {operations:,}\")\n",
    "    print(f\"  Computation time: {computation_time:.6f} seconds\")\n",
    "    print(f\"  Operations per second: {operations/computation_time:,.0f}\")\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. Matrix multiplication scales efficiently with problem size\")\n",
    "print(\"2. Modern hardware (GPUs) can perform billions of operations per second\")\n",
    "print(\"3. Batch processing enables massive parallelization\")\n",
    "print(\"4. This is how ChatGPT processes thousands of tokens simultaneously\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Computational Efficiency\n",
    "\n",
    "Let's measure the dramatic performance improvements that matrix operations provide, especially as data size grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison across different dataset sizes\n",
    "\n",
    "def benchmark_approaches(max_samples=10000):\n",
    "    \"\"\"\n",
    "    Benchmark loop vs matrix approaches across different dataset sizes.\n",
    "    \"\"\"\n",
    "    sample_sizes = [10, 50, 100, 500, 1000, 5000]\n",
    "    if max_samples >= 10000:\n",
    "        sample_sizes.append(10000)\n",
    "    \n",
    "    results = {\n",
    "        'sizes': [],\n",
    "        'loop_times': [],\n",
    "        'matrix_times': [],\n",
    "        'speedups': []\n",
    "    }\n",
    "    \n",
    "    print(\"PERFORMANCE BENCHMARK ACROSS DATASET SIZES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Size':<8} | {'Loop Time':<12} | {'Matrix Time':<12} | {'Speedup':<8}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in sample_sizes:\n",
    "        if size > max_samples:\n",
    "            continue\n",
    "            \n",
    "        # Generate synthetic data of this size\n",
    "        synthetic_features, synthetic_labels = generate_synthetic_sports_data(size)\n",
    "        \n",
    "        # Ensure we have the right number of features (3)\n",
    "        if synthetic_features.shape[1] != 3:\n",
    "            # Add or remove features to match our weight vector\n",
    "            if synthetic_features.shape[1] > 3:\n",
    "                synthetic_features = synthetic_features[:, :3]\n",
    "            else:\n",
    "                # Add random features\n",
    "                extra_features = np.random.rand(size, 3 - synthetic_features.shape[1])\n",
    "                synthetic_features = np.hstack([synthetic_features, extra_features])\n",
    "        \n",
    "        # Time loop-based approach\n",
    "        start_time = time.time()\n",
    "        loop_predictions = predict_with_loops(synthetic_features, learned_weights)\n",
    "        loop_time = time.time() - start_time\n",
    "        \n",
    "        # Time matrix-based approach\n",
    "        start_time = time.time()\n",
    "        matrix_predictions = predict_with_matrices(synthetic_features, learned_weights)\n",
    "        matrix_time = time.time() - start_time\n",
    "        \n",
    "        speedup = loop_time / matrix_time if matrix_time > 0 else float('inf')\n",
    "        \n",
    "        # Store results\n",
    "        results['sizes'].append(size)\n",
    "        results['loop_times'].append(loop_time)\n",
    "        results['matrix_times'].append(matrix_time)\n",
    "        results['speedups'].append(speedup)\n",
    "        \n",
    "        print(f\"{size:<8} | {loop_time:<12.6f} | {matrix_time:<12.6f} | {speedup:<8.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark_results = benchmark_approaches(max_samples=5000)  # Limit for reasonable runtime\n",
    "\n",
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sizes = benchmark_results['sizes']\n",
    "loop_times = benchmark_results['loop_times']\n",
    "matrix_times = benchmark_results['matrix_times']\n",
    "speedups = benchmark_results['speedups']\n",
    "\n",
    "# Plot 1: Execution times\n",
    "axes[0].plot(sizes, loop_times, 'r-o', linewidth=2, markersize=8, label='Loop-based')\n",
    "axes[0].plot(sizes, matrix_times, 'b-o', linewidth=2, markersize=8, label='Matrix-based')\n",
    "axes[0].set_xlabel('Dataset Size')\n",
    "axes[0].set_ylabel('Execution Time (seconds)')\n",
    "axes[0].set_title('Execution Time vs Dataset Size')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Speedup factor\n",
    "axes[1].plot(sizes, speedups, 'g-o', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Dataset Size')\n",
    "axes[1].set_ylabel('Speedup Factor (x faster)')\n",
    "axes[1].set_title('Matrix Operations Speedup')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPERFORMANCE SUMMARY:\")\n",
    "print(f\"Maximum speedup achieved: {max(speedups):.1f}x\")\n",
    "print(f\"Average speedup: {np.mean(speedups):.1f}x\")\n",
    "print(f\"Speedup tends to increase with dataset size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate memory efficiency of batch processing\n",
    "import sys\n",
    "\n",
    "def analyze_memory_usage():\n",
    "    \"\"\"\n",
    "    Compare memory usage patterns of different approaches.\n",
    "    \"\"\"\n",
    "    print(\"\\nMEMORY EFFICIENCY ANALYSIS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create test data\n",
    "    large_features, large_labels = generate_synthetic_sports_data(1000)\n",
    "    if large_features.shape[1] != 3:\n",
    "        large_features = large_features[:, :2]\n",
    "        large_features = np.hstack([large_features, np.random.rand(1000, 1)])\n",
    "    \n",
    "    # Memory usage of different data structures\n",
    "    feature_memory = large_features.nbytes\n",
    "    weight_memory = learned_weights.nbytes\n",
    "    prediction_memory = (1000 * 8)  # 1000 float64 predictions\n",
    "    \n",
    "    print(f\"Feature matrix memory (1000Ã—3): {feature_memory:,} bytes ({feature_memory/1024:.1f} KB)\")\n",
    "    print(f\"Weight vector memory (3Ã—1): {weight_memory:,} bytes\")\n",
    "    print(f\"Prediction vector memory (1000Ã—1): {prediction_memory:,} bytes ({prediction_memory/1024:.1f} KB)\")\n",
    "    print(f\"Total memory for batch: {(feature_memory + weight_memory + prediction_memory)/1024:.1f} KB\")\n",
    "    \n",
    "    # Compare with individual processing\n",
    "    individual_memory = (3 + 1 + 1) * 8  # features + weight access + prediction per iteration\n",
    "    print(f\"\\nMemory per individual calculation: {individual_memory} bytes\")\n",
    "    print(f\"Total if processing sequentially: {individual_memory * 1000:,} bytes ({individual_memory * 1000/1024:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nKey Insights:\")\n",
    "    print(f\"1. Batch processing uses memory more efficiently\")\n",
    "    print(f\"2. Data stays in contiguous memory blocks (cache-friendly)\")\n",
    "    print(f\"3. Reduces overhead of function calls and Python loops\")\n",
    "    print(f\"4. Enables GPU processing (GPU memory is optimized for large contiguous arrays)\")\n",
    "\n",
    "analyze_memory_usage()\n",
    "\n",
    "# Demonstrate the power of vectorization with a larger example\n",
    "print(\"\\nVECTORIZATION POWER DEMONSTRATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a larger synthetic dataset\n",
    "large_features, large_labels = generate_synthetic_sports_data(10000)\n",
    "if large_features.shape[1] != 3:\n",
    "    large_features = large_features[:, :2]\n",
    "    large_features = np.hstack([large_features, np.random.rand(10000, 1)])\n",
    "\n",
    "print(f\"Processing {len(large_features):,} examples...\")\n",
    "\n",
    "# Time matrix approach on large dataset\n",
    "start_time = time.time()\n",
    "large_predictions = predict_with_matrices(large_features, learned_weights)\n",
    "matrix_time = time.time() - start_time\n",
    "\n",
    "print(f\"Matrix approach: {matrix_time:.4f} seconds\")\n",
    "print(f\"Predictions per second: {len(large_features)/matrix_time:,.0f}\")\n",
    "print(f\"\\nThis is how modern ML processes millions of examples efficiently!\")\n",
    "print(f\"ChatGPT uses similar matrix operations to process thousands of tokens simultaneously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Scaling to Real Datasets\n",
    "\n",
    "Let's see how matrix operations enable the massive scale of modern machine learning and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate real-world ML scenarios\n",
    "\n",
    "def simulate_ml_scenarios():\n",
    "    \"\"\"\n",
    "    Simulate the computational requirements of real ML systems.\n",
    "    \"\"\"\n",
    "    scenarios = [\n",
    "        {\n",
    "            \"name\": \"Our Sports Classifier\",\n",
    "            \"samples\": 16,\n",
    "            \"features\": 3,\n",
    "            \"description\": \"Small dataset, basic features\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Production Text Classifier\",\n",
    "            \"samples\": 100000,\n",
    "            \"features\": 1000,\n",
    "            \"description\": \"Real text classification with TF-IDF features\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Image Classification (CIFAR-10)\",\n",
    "            \"samples\": 50000,\n",
    "            \"features\": 3072,  # 32x32x3 pixels\n",
    "            \"description\": \"Image pixels as features\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Language Model (Small)\",\n",
    "            \"samples\": 1000,\n",
    "            \"features\": 50000,  # Vocabulary size\n",
    "            \"description\": \"Token embeddings for language modeling\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Deep Learning Layer (Typical)\",\n",
    "            \"samples\": 1024,  # Batch size\n",
    "            \"features\": 2048,  # Hidden layer size\n",
    "            \"description\": \"Single layer in a deep network\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"REAL-WORLD ML COMPUTATIONAL REQUIREMENTS\")\n",
    "    print(\"=\" * 55)\n",
    "    print(f\"{'Scenario':<25} | {'Matrix Size':<15} | {'Operations':<12} | {'Est. Time':<10}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        samples = scenario[\"samples\"]\n",
    "        features = scenario[\"features\"]\n",
    "        \n",
    "        # Calculate computational requirements\n",
    "        matrix_size = f\"{samples}Ã—{features}\"\n",
    "        operations = samples * features\n",
    "        \n",
    "        # Estimate time based on our benchmarks (very rough)\n",
    "        # Assume ~100M operations per second (conservative)\n",
    "        estimated_time = operations / 100_000_000\n",
    "        \n",
    "        if estimated_time < 0.001:\n",
    "            time_str = \"<1ms\"\n",
    "        elif estimated_time < 1:\n",
    "            time_str = f\"{estimated_time*1000:.0f}ms\"\n",
    "        else:\n",
    "            time_str = f\"{estimated_time:.2f}s\"\n",
    "        \n",
    "        print(f\"{scenario['name']:<25} | {matrix_size:<15} | {operations:<12,} | {time_str:<10}\")\n",
    "        print(f\"{'':25} | {scenario['description']}\")\n",
    "        print()\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "scenarios = simulate_ml_scenarios()\n",
    "\n",
    "print(\"SCALING INSIGHTS:\")\n",
    "print(\"1. Matrix operations make large-scale ML computationally feasible\")\n",
    "print(\"2. Modern GPUs can perform trillions of operations per second\")\n",
    "print(\"3. Batch processing is essential for training efficiency\")\n",
    "print(\"4. This is why GPUs revolutionized deep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how this connects to modern deep learning\n",
    "def demonstrate_deep_learning_connection():\n",
    "    \"\"\"\n",
    "    Show how our simple matrix operations scale to deep learning.\n",
    "    \"\"\"\n",
    "    print(\"\\nCONNECTION TO MODERN DEEP LEARNING\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Simulate a simple neural network with multiple layers\n",
    "    batch_size = 32\n",
    "    input_size = 100\n",
    "    hidden_sizes = [256, 128, 64]\n",
    "    output_size = 10\n",
    "    \n",
    "    print(f\"Simulating a neural network:\")\n",
    "    print(f\"  Input: {batch_size} samples Ã— {input_size} features\")\n",
    "    for i, hidden_size in enumerate(hidden_sizes, 1):\n",
    "        print(f\"  Hidden Layer {i}: {hidden_size} neurons\")\n",
    "    print(f\"  Output: {output_size} classes\")\n",
    "    print()\n",
    "    \n",
    "    # Simulate forward pass through the network\n",
    "    current_input = np.random.randn(batch_size, input_size)\n",
    "    total_operations = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    print(\"Forward pass through network:\")\n",
    "    print(f\"{'Layer':<15} | {'Input Shape':<12} | {'Weight Shape':<12} | {'Output Shape':<12} | {'Operations':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Input to first hidden layer\n",
    "    prev_size = input_size\n",
    "    for i, hidden_size in enumerate(hidden_sizes):\n",
    "        weights = np.random.randn(prev_size, hidden_size)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        output = current_input @ weights\n",
    "        # Apply activation (ReLU)\n",
    "        output = np.maximum(0, output)\n",
    "        layer_time = time.time() - start_time\n",
    "        \n",
    "        operations = batch_size * prev_size * hidden_size\n",
    "        total_operations += operations\n",
    "        total_time += layer_time\n",
    "        \n",
    "        print(f\"Hidden {i+1:<8} | {current_input.shape} | {weights.shape} | {output.shape} | {operations:<12,}\")\n",
    "        \n",
    "        current_input = output\n",
    "        prev_size = hidden_size\n",
    "    \n",
    "    # Final output layer\n",
    "    final_weights = np.random.randn(prev_size, output_size)\n",
    "    start_time = time.time()\n",
    "    final_output = current_input @ final_weights\n",
    "    final_time = time.time() - start_time\n",
    "    \n",
    "    final_operations = batch_size * prev_size * output_size\n",
    "    total_operations += final_operations\n",
    "    total_time += final_time\n",
    "    \n",
    "    print(f\"Output{'':9} | {current_input.shape} | {final_weights.shape} | {final_output.shape} | {final_operations:<12,}\")\n",
    "    \n",
    "    print(f\"\\nNetwork Summary:\")\n",
    "    print(f\"  Total operations: {total_operations:,}\")\n",
    "    print(f\"  Total time: {total_time:.6f} seconds\")\n",
    "    print(f\"  Operations per second: {total_operations/total_time:,.0f}\")\n",
    "    \n",
    "    # Compare to ChatGPT scale\n",
    "    print(f\"\\nCOMPARISON TO CHATGPT SCALE:\")\n",
    "    print(f\"Our network: ~{total_operations/1e6:.1f} million operations per forward pass\")\n",
    "    print(f\"ChatGPT-3.5: ~175 billion parameters\")\n",
    "    print(f\"ChatGPT forward pass: ~100+ billion operations\")\n",
    "    print(f\"Scale difference: ~{100e9/total_operations:,.0f}x larger!\")\n",
    "    \n",
    "    print(f\"\\nHow this scales to ChatGPT:\")\n",
    "    print(f\"1. Same matrix operations, but 1000x larger matrices\")\n",
    "    print(f\"2. Specialized hardware (GPUs/TPUs) with massive parallelism\")\n",
    "    print(f\"3. Optimized libraries (cuBLAS, cuDNN) for maximum performance\")\n",
    "    print(f\"4. Distributed computing across multiple devices\")\n",
    "\n",
    "demonstrate_deep_learning_connection()\n",
    "\n",
    "# Final summary of the entire journey\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ CONGRATULATIONS! YOU'VE COMPLETED PART 1! ðŸŽ‰\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYour journey from 'Go Dolphins!' to scalable machine learning:\")\n",
    "print(\"\\n1. ðŸ“ Problem 1 - Feature Engineering:\")\n",
    "print(\"   'Go Dolphins!' â†’ [2, 1, 1] (numbers machines can use)\")\n",
    "print(\"\\n2. ðŸ”¢ Problem 2 - Dot Products:\")\n",
    "print(\"   [2, 1, 1] Â· [0.3, 0.5, 0.4] = 1.5 (prediction score)\")\n",
    "print(\"\\n3. ðŸ“Š Problem 3 - Loss Functions:\")\n",
    "print(\"   1.5 vs 1.0 â†’ Loss = 0.25 (how wrong we are)\")\n",
    "print(\"\\n4. âš¡ Problem 4 - Gradient Descent:\")\n",
    "print(\"   Automatically adjust weights to minimize loss\")\n",
    "print(\"\\n5. ðŸš€ Problem 5 - Matrix Operations:\")\n",
    "print(\"   Process millions of examples simultaneously\")\n",
    "print(\"\\nYou now understand the mathematical foundation of ALL machine learning!\")\n",
    "print(\"Every AI system, from simple classifiers to ChatGPT, uses these same principles.\")\n",
    "print(\"\\nReady for Part 2? We'll dive deeper into the mathematical theory! ðŸ¤“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've completed Part 1 and built a complete understanding of machine learning fundamentals!\n",
    "\n",
    "**ðŸ”‘ Your Journey Summary:**\n",
    "1. **Features** - Converted \"Go Dolphins!\" into numbers `[2, 1, 1]`\n",
    "2. **Dot Products** - Transformed features into predictions via alignment\n",
    "3. **Loss Functions** - Measured prediction quality and guided learning\n",
    "4. **Gradient Descent** - Automatically optimized weights through calculus\n",
    "5. **Matrix Operations** - Scaled everything to process millions of examples\n",
    "\n",
    "**ðŸŒŸ What You Now Understand:**\n",
    "- How text becomes predictions through systematic mathematical operations\n",
    "- Why gradient descent can automatically learn from data\n",
    "- How matrix operations enable the massive scale of modern AI\n",
    "- The mathematical foundation underlying ALL machine learning systems\n",
    "\n",
    "**ðŸš€ The Big Picture:**\n",
    "ChatGPT, image classifiers, recommendation systems - they ALL use these same fundamental operations:\n",
    "- **Features â†’ Dot Products â†’ Loss â†’ Gradients â†’ Matrix Operations**\n",
    "- The only difference is scale: billions of parameters instead of 3!\n",
    "\n",
    "**ðŸŽ¯ Coming in Part 2:**\n",
    "Now that you understand WHAT happens, Part 2 will show you WHY it works through advanced mathematical analysis:\n",
    "- **Vector calculus** for optimization landscapes\n",
    "- **Jacobian matrices** for multi-layer networks  \n",
    "- **Chain rule** for complex function compositions\n",
    "- **Advanced optimization** theory and practice\n",
    "\n",
    "You're ready to go from practitioner to theorist! ðŸ¬âž¡ï¸ðŸ“Šâž¡ï¸ðŸŽ¯âž¡ï¸âš¡âž¡ï¸ðŸš€âž¡ï¸ðŸ§®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}