{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Multi-Layer Chain Rule - How Deep Networks Learn\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this problem, you will:\n",
    "- Understand backpropagation as systematic application of the chain rule\n",
    "- Analyze gradient flow through multiple computational layers\n",
    "- Identify and diagnose vanishing/exploding gradient problems\n",
    "- Connect mathematical chain rule to practical deep learning implementation\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "1. **Chain Rule Fundamentals** - From single to multi-variable chain rule\n",
    "2. **Multi-Layer Network Analysis** - Extend \"Go Dolphins!\" to deep architecture\n",
    "3. **Gradient Flow Dynamics** - Track gradients through network layers\n",
    "4. **Backpropagation Implementation** - Build the algorithm from mathematical principles\n",
    "\n",
    "---\n",
    "\n",
    "## From Single Layer to Deep Networks\n",
    "\n",
    "In Problem 1, you analyzed the loss landscape for our single-layer \"Go Dolphins!\" classifier:\n",
    "```\n",
    "Input [2,1,1] → Weights [w₁,w₂,w₃] → Prediction → Loss\n",
    "```\n",
    "\n",
    "But modern AI systems use **deep networks** with many layers:\n",
    "```\n",
    "Input → Hidden Layer 1 → Hidden Layer 2 → ... → Output → Loss\n",
    "```\n",
    "\n",
    "**The Challenge**: How do we compute gradients when the loss depends on weights through a complex chain of transformations?\n",
    "\n",
    "**The Solution**: The **chain rule** - one of the most important theorems in calculus and the mathematical foundation of deep learning.\n",
    "\n",
    "## Mathematical Foundation: The Chain Rule\n",
    "\n",
    "**Single Variable Chain Rule**:\n",
    "If $y = f(g(x))$, then $\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$\n",
    "\n",
    "**Multi-Variable Chain Rule**:\n",
    "If $z = f(x, y)$ where $x = g(t)$ and $y = h(t)$, then:\n",
    "$$\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial z}{\\partial y} \\frac{dy}{dt}$$\n",
    "\n",
    "**Deep Network Chain Rule**:\n",
    "For loss $L$ depending on weights $W^{(1)}$ through layers $z^{(1)}, z^{(2)}, ..., z^{(n)}$:\n",
    "$$\\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial z^{(n)}} \\frac{\\partial z^{(n)}}{\\partial z^{(n-1)}} \\cdots \\frac{\\partial z^{(2)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial W^{(1)}}$$\n",
    "\n",
    "This is **backpropagation** - working backwards through the network, applying the chain rule at each step!\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Understanding the mathematical chain rule reveals:\n",
    "- **Why backpropagation works** (systematic application of calculus)\n",
    "- **Where gradients vanish** (product of many small derivatives)\n",
    "- **How to design stable architectures** (controlling gradient magnitudes)\n",
    "- **The computational efficiency** (reusing intermediate calculations)\n",
    "\n",
    "Let's extend our \"Go Dolphins!\" example to see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for multi-layer chain rule analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Import our utilities\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from data_generators import load_sports_dataset\n",
    "\n",
    "# Load our \"Go Dolphins!\" dataset\n",
    "features, labels, feature_names, texts = load_sports_dataset()\n",
    "\n",
    "print(\"DEEP NETWORK ANALYSIS: EXTENDING 'GO DOLPHINS!' TO MULTIPLE LAYERS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset: {len(texts)} sports tweets\")\n",
    "print(f\"Base features: {feature_names}\")\n",
    "print()\n",
    "print(\"Network architectures we'll analyze:\")\n",
    "print(\"• Single layer: Input(3) → Output(1)\")\n",
    "print(\"• Two layer: Input(3) → Hidden(4) → Output(1)\")\n",
    "print(\"• Three layer: Input(3) → Hidden(4) → Hidden(2) → Output(1)\")\n",
    "print(\"• Deep network: Input(3) → Hidden(6) → Hidden(4) → Hidden(2) → Output(1)\")\n",
    "print()\n",
    "print(\"Mathematical focus: Chain rule application through each architecture\")\n",
    "\n",
    "# Define activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid: σ'(x) = σ(x)(1 - σ(x))\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU: 1 if x > 0, 0 otherwise\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Hyperbolic tangent activation\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"Derivative of tanh: 1 - tanh²(x)\"\"\"\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# Binary cross-entropy loss and its derivative\n",
    "def bce_loss(predictions, targets):\n",
    "    \"\"\"Binary cross-entropy loss\"\"\"\n",
    "    predictions = np.clip(predictions, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(targets * np.log(predictions) + (1 - targets) * np.log(1 - predictions))\n",
    "\n",
    "def bce_loss_derivative(predictions, targets):\n",
    "    \"\"\"Derivative of BCE loss with respect to predictions\"\"\"\n",
    "    predictions = np.clip(predictions, 1e-15, 1 - 1e-15)\n",
    "    return (predictions - targets) / (predictions * (1 - predictions)) / len(targets)\n",
    "\n",
    "print(\"\\n✅ Activation functions and derivatives ready for chain rule analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Chain Rule Fundamentals\n",
    "\n",
    "Let's build intuition by starting with simple composite functions and working up to neural network complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Demonstrate chain rule with simple examples\n",
    "def demonstrate_chain_rule_basics():\n",
    "    \"\"\"\n",
    "    Build intuition with simple chain rule examples.\n",
    "    \"\"\"\n",
    "    print(\"CHAIN RULE FUNDAMENTALS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Example 1: Single variable chain rule\n",
    "    print(\"Example 1: Single Variable Chain Rule\")\n",
    "    print(\"Function: f(g(x)) where g(x) = x² + 1, f(u) = sin(u)\")\n",
    "    print(\"So h(x) = sin(x² + 1)\")\n",
    "    print()\n",
    "    \n",
    "    x = 2.0\n",
    "    \n",
    "    # Forward pass\n",
    "    g_x = x**2 + 1  # g(x) = x² + 1\n",
    "    f_g = np.sin(g_x)  # f(g(x)) = sin(g(x))\n",
    "    \n",
    "    # Analytical derivatives\n",
    "    dg_dx = 2 * x  # g'(x) = 2x\n",
    "    df_dg = np.cos(g_x)  # f'(u) = cos(u)\n",
    "    \n",
    "    # Chain rule: dh/dx = df/dg * dg/dx\n",
    "    dh_dx_chain = df_dg * dg_dx\n",
    "    \n",
    "    # Verify with numerical differentiation\n",
    "    h = 1e-8\n",
    "    dh_dx_numerical = (np.sin((x+h)**2 + 1) - np.sin((x-h)**2 + 1)) / (2*h)\n",
    "    \n",
    "    print(f\"At x = {x}:\")\n",
    "    print(f\"  g(x) = {g_x}\")\n",
    "    print(f\"  f(g(x)) = {f_g:.6f}\")\n",
    "    print(f\"  dg/dx = {dg_dx}\")\n",
    "    print(f\"  df/dg = {df_dg:.6f}\")\n",
    "    print(f\"  Chain rule: dh/dx = {dh_dx_chain:.6f}\")\n",
    "    print(f\"  Numerical: dh/dx = {dh_dx_numerical:.6f}\")\n",
    "    print(f\"  Difference: {abs(dh_dx_chain - dh_dx_numerical):.2e}\")\n",
    "    print()\n",
    "    \n",
    "    # Example 2: Neural network style composition\n",
    "    print(\"Example 2: Neural Network Style Composition\")\n",
    "    print(\"Function: sigmoid(w·x + b) where x = [2, 1, 1], w = [0.3, 0.5, 0.4], b = 0.1\")\n",
    "    print()\n",
    "    \n",
    "    x = np.array([2, 1, 1])\n",
    "    w = np.array([0.3, 0.5, 0.4])\n",
    "    b = 0.1\n",
    "    \n",
    "    # Forward pass\n",
    "    z = np.dot(w, x) + b  # Linear transformation\n",
    "    a = sigmoid(z)  # Activation\n",
    "    \n",
    "    # Chain rule for ∂a/∂w\n",
    "    da_dz = sigmoid_derivative(z)  # ∂σ/∂z\n",
    "    dz_dw = x  # ∂z/∂w = x (since z = w·x + b)\n",
    "    da_dw = da_dz * dz_dw  # Chain rule\n",
    "    \n",
    "    print(f\"Input x: {x}\")\n",
    "    print(f\"Weights w: {w}\")\n",
    "    print(f\"Bias b: {b}\")\n",
    "    print(f\"Linear output z: {z:.6f}\")\n",
    "    print(f\"Activation output a: {a:.6f}\")\n",
    "    print(f\"∂a/∂z: {da_dz:.6f}\")\n",
    "    print(f\"∂z/∂w: {dz_dw}\")\n",
    "    print(f\"∂a/∂w (chain rule): {da_dw}\")\n",
    "    \n",
    "    # Verify numerically for first weight\n",
    "    h = 1e-8\n",
    "    w_plus = w.copy()\n",
    "    w_minus = w.copy()\n",
    "    w_plus[0] += h\n",
    "    w_minus[0] -= h\n",
    "    \n",
    "    a_plus = sigmoid(np.dot(w_plus, x) + b)\n",
    "    a_minus = sigmoid(np.dot(w_minus, x) + b)\n",
    "    da_dw0_numerical = (a_plus - a_minus) / (2*h)\n",
    "    \n",
    "    print(f\"\\nVerification for ∂a/∂w₀:\")\n",
    "    print(f\"  Chain rule: {da_dw[0]:.8f}\")\n",
    "    print(f\"  Numerical:  {da_dw0_numerical:.8f}\")\n",
    "    print(f\"  Difference: {abs(da_dw[0] - da_dw0_numerical):.2e}\")\n",
    "\n",
    "demonstrate_chain_rule_basics()\n",
    "print(\"\\n✅ Chain rule fundamentals demonstrated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze gradient magnitude through compositions\n",
    "def analyze_gradient_magnitudes():\n",
    "    \"\"\"\n",
    "    Explore how gradient magnitudes change through function compositions.\n",
    "    \"\"\"\n",
    "    print(\"\\nGRADIENT MAGNITUDE ANALYSIS THROUGH COMPOSITIONS\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Test different activation functions in chain\n",
    "    x_vals = np.linspace(-3, 3, 100)\n",
    "    \n",
    "    activations = {\n",
    "        'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "        'tanh': (tanh, tanh_derivative),\n",
    "        'relu': (relu, relu_derivative)\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    for i, (name, (func, deriv)) in enumerate(activations.items()):\n",
    "        # Plot activation function\n",
    "        y_vals = func(x_vals)\n",
    "        axes[0, i].plot(x_vals, y_vals, 'b-', linewidth=2, label=f'{name}(x)')\n",
    "        axes[0, i].set_xlabel('x')\n",
    "        axes[0, i].set_ylabel('f(x)')\n",
    "        axes[0, i].set_title(f'{name.capitalize()} Activation')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        axes[0, i].legend()\n",
    "        \n",
    "        # Plot derivative\n",
    "        dy_vals = deriv(x_vals)\n",
    "        axes[1, i].plot(x_vals, dy_vals, 'r-', linewidth=2, label=f\"{name}'(x)\")\n",
    "        axes[1, i].set_xlabel('x')\n",
    "        axes[1, i].set_ylabel(\"f'(x)\")\n",
    "        axes[1, i].set_title(f'{name.capitalize()} Derivative')\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "        axes[1, i].legend()\n",
    "        \n",
    "        # Analyze derivative properties\n",
    "        max_deriv = np.max(dy_vals)\n",
    "        min_deriv = np.min(dy_vals)\n",
    "        print(f\"\\n{name.capitalize()} activation:\")\n",
    "        print(f\"  Max derivative: {max_deriv:.4f}\")\n",
    "        print(f\"  Min derivative: {min_deriv:.4f}\")\n",
    "        print(f\"  Range: [{min_deriv:.4f}, {max_deriv:.4f}]\")\n",
    "        \n",
    "        if name == 'sigmoid':\n",
    "            print(f\"  ⚠️  Max derivative < 1: potential vanishing gradients\")\n",
    "        elif name == 'relu':\n",
    "            print(f\"  ✅ Derivative is 0 or 1: no vanishing (but can have dying ReLU)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Demonstrate gradient flow through multiple layers\n",
    "    print(\"\\nGRADIENT FLOW THROUGH MULTIPLE LAYERS:\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Simulate gradient flowing backwards through layers\n",
    "    initial_gradient = 1.0  # Start with unit gradient\n",
    "    layer_inputs = [-2, -1, 0, 1, 2]  # Different activation regions\n",
    "    \n",
    "    for activation_name, (func, deriv) in activations.items():\n",
    "        print(f\"\\n{activation_name.upper()} through 3 layers:\")\n",
    "        print(f\"{'Layer':<8} | {'Input':<8} | {'Derivative':<12} | {'Gradient':<12}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        gradient = initial_gradient\n",
    "        for layer in range(3):\n",
    "            # Use different inputs for each layer\n",
    "            layer_input = layer_inputs[layer % len(layer_inputs)]\n",
    "            layer_deriv = deriv(layer_input)\n",
    "            gradient *= layer_deriv\n",
    "            \n",
    "            print(f\"Layer {layer+1:<3} | {layer_input:<8.1f} | {layer_deriv:<12.6f} | {gradient:<12.6f}\")\n",
    "        \n",
    "        print(f\"Final gradient magnitude: {abs(gradient):.6f}\")\n",
    "        if abs(gradient) < 0.1:\n",
    "            print(\"⚠️  Vanishing gradient detected!\")\n",
    "        elif abs(gradient) > 10:\n",
    "            print(\"⚠️  Exploding gradient detected!\")\n",
    "        else:\n",
    "            print(\"✅ Gradient magnitude reasonable\")\n",
    "\n",
    "analyze_gradient_magnitudes()\n",
    "print(\"\\n✅ Gradient magnitude analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Layer Network Analysis\n",
    "\n",
    "Now let's extend our \"Go Dolphins!\" classifier to multiple layers and analyze gradient flow through the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement multi-layer neural network\n",
    "class MultiLayerNetwork:\n",
    "    \"\"\"\n",
    "    Multi-layer neural network for mathematical analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        Initialize network with specified architecture.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of layer sizes [input, hidden1, hidden2, ..., output]\n",
    "            activation: Activation function ('sigmoid', 'tanh', 'relu')\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Xavier/Glorot initialization\n",
    "            scale = np.sqrt(2.0 / (layer_sizes[i] + layer_sizes[i+1]))\n",
    "            weight_matrix = np.random.normal(0, scale, (layer_sizes[i], layer_sizes[i+1]))\n",
    "            bias_vector = np.zeros(layer_sizes[i+1])\n",
    "            \n",
    "            self.weights.append(weight_matrix)\n",
    "            self.biases.append(bias_vector)\n",
    "        \n",
    "        # Store activation functions\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation_func = sigmoid\n",
    "            self.activation_deriv = sigmoid_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_func = tanh\n",
    "            self.activation_deriv = tanh_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activation_func = relu\n",
    "            self.activation_deriv = relu_derivative\n",
    "        \n",
    "        print(f\"Initialized {self.num_layers}-layer network: {layer_sizes}\")\n",
    "        print(f\"Activation: {activation}\")\n",
    "        print(f\"Total parameters: {self.count_parameters()}\")\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total number of parameters.\"\"\"\n",
    "        total = 0\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            total += w.size + b.size\n",
    "        return total\n",
    "    \n",
    "    def forward_pass(self, x, store_activations=True):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Returns:\n",
    "            output: Final network output\n",
    "            activations: List of activations at each layer (if store_activations=True)\n",
    "            z_values: List of pre-activation values (if store_activations=True)\n",
    "        \"\"\"\n",
    "        if store_activations:\n",
    "            activations = [x]  # Input is first activation\n",
    "            z_values = []  # Pre-activation values\n",
    "        \n",
    "        current_input = x\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Linear transformation: z = W^T * a + b\n",
    "            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
    "            \n",
    "            if store_activations:\n",
    "                z_values.append(z)\n",
    "            \n",
    "            # Apply activation (except for output layer - we'll use sigmoid for final)\n",
    "            if i == self.num_layers - 1:\n",
    "                # Output layer always uses sigmoid for binary classification\n",
    "                a = sigmoid(z)\n",
    "            else:\n",
    "                # Hidden layers use specified activation\n",
    "                a = self.activation_func(z)\n",
    "            \n",
    "            if store_activations:\n",
    "                activations.append(a)\n",
    "            \n",
    "            current_input = a\n",
    "        \n",
    "        if store_activations:\n",
    "            return current_input, activations, z_values\n",
    "        else:\n",
    "            return current_input\n",
    "    \n",
    "    def backward_pass(self, x, y, activations, z_values):\n",
    "        \"\"\"\n",
    "        Backward pass using chain rule (backpropagation).\n",
    "        \n",
    "        Returns:\n",
    "            gradients_w: List of weight gradients\n",
    "            gradients_b: List of bias gradients\n",
    "            gradient_magnitudes: Gradient magnitudes at each layer\n",
    "        \"\"\"\n",
    "        gradients_w = [np.zeros_like(w) for w in self.weights]\n",
    "        gradients_b = [np.zeros_like(b) for b in self.biases]\n",
    "        gradient_magnitudes = []\n",
    "        \n",
    "        # Start with loss gradient (BCE with sigmoid output)\n",
    "        output = activations[-1]\n",
    "        delta = output - y  # For BCE + sigmoid, this is the gradient\n",
    "        \n",
    "        # Backward through each layer\n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            # Record gradient magnitude for analysis\n",
    "            gradient_magnitudes.append(np.linalg.norm(delta))\n",
    "            \n",
    "            # Gradients for current layer\n",
    "            gradients_w[i] = np.outer(activations[i], delta)\n",
    "            gradients_b[i] = delta.copy()\n",
    "            \n",
    "            # Propagate gradient to previous layer (if not input layer)\n",
    "            if i > 0:\n",
    "                # Chain rule: δ^(l-1) = (W^(l))^T δ^(l) ⊙ σ'(z^(l-1))\n",
    "                delta = np.dot(self.weights[i], delta)\n",
    "                \n",
    "                # Apply activation derivative\n",
    "                if i == 1:  # Previous layer is first hidden layer\n",
    "                    delta *= self.activation_deriv(z_values[i-1])\n",
    "                else:  # Previous layer is another hidden layer\n",
    "                    delta *= self.activation_deriv(z_values[i-1])\n",
    "        \n",
    "        # Reverse gradient magnitudes to match layer order\n",
    "        gradient_magnitudes.reverse()\n",
    "        \n",
    "        return gradients_w, gradients_b, gradient_magnitudes\n",
    "\n",
    "# Test different network architectures on \"Go Dolphins!\"\n",
    "print(\"MULTI-LAYER 'GO DOLPHINS!' CLASSIFIER ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define different architectures to test\n",
    "architectures = [\n",
    "    ([3, 1], \"Single Layer\"),\n",
    "    ([3, 4, 1], \"Two Layer\"),\n",
    "    ([3, 4, 2, 1], \"Three Layer\"),\n",
    "    ([3, 6, 4, 2, 1], \"Four Layer (Deep)\")\n",
    "]\n",
    "\n",
    "# Test each architecture\n",
    "networks = {}\n",
    "for arch, name in architectures:\n",
    "    print(f\"\\n{name} Network: {arch}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create network\n",
    "    net = MultiLayerNetwork(arch, activation='sigmoid')\n",
    "    networks[name] = net\n",
    "    \n",
    "    # Test forward pass on \"Go Dolphins!\" example\n",
    "    x = features[0]  # \"Go Dolphins!\" features [2, 1, 1]\n",
    "    y = labels[0]    # True label (positive)\n",
    "    \n",
    "    output, activations, z_values = net.forward_pass(x)\n",
    "    \n",
    "    print(f\"Input: {x}\")\n",
    "    print(f\"Output: {output:.6f}\")\n",
    "    print(f\"Target: {y}\")\n",
    "    print(f\"Loss: {bce_loss(np.array([output]), np.array([y])):.6f}\")\n",
    "    \n",
    "    # Show activations at each layer\n",
    "    for i, activation in enumerate(activations):\n",
    "        if i == 0:\n",
    "            print(f\"Layer {i} (input): {activation}\")\n",
    "        elif i == len(activations) - 1:\n",
    "            print(f\"Layer {i} (output): {activation:.6f}\")\n",
    "        else:\n",
    "            print(f\"Layer {i} (hidden): {activation}\")\n",
    "\n",
    "print(\"\\n✅ Multi-layer networks initialized and tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Gradient Flow Dynamics\n",
    "\n",
    "Let's analyze how gradients flow backwards through our multi-layer networks and identify potential problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze gradient flow through different architectures\n",
    "def analyze_gradient_flow():\n",
    "    \"\"\"\n",
    "    Analyze gradient flow through different network architectures.\n",
    "    \"\"\"\n",
    "    print(\"GRADIENT FLOW ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Test on a single example for detailed analysis\n",
    "    x = features[0]  # \"Go Dolphins!\" example\n",
    "    y = labels[0]    # True label\n",
    "    \n",
    "    gradient_data = {}\n",
    "    \n",
    "    for name, net in networks.items():\n",
    "        print(f\"\\n{name} Network Gradient Flow:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, activations, z_values = net.forward_pass(x)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_w, grad_b, grad_mags = net.backward_pass(x, y, activations, z_values)\n",
    "        \n",
    "        gradient_data[name] = {\n",
    "            'magnitudes': grad_mags,\n",
    "            'num_layers': net.num_layers,\n",
    "            'activations': activations,\n",
    "            'z_values': z_values\n",
    "        }\n",
    "        \n",
    "        print(f\"Forward pass output: {output:.6f}\")\n",
    "        print(f\"Target: {y}\")\n",
    "        print(f\"Number of layers: {net.num_layers}\")\n",
    "        print(\"\\nGradient magnitudes by layer (output → input):\")\n",
    "        \n",
    "        for i, mag in enumerate(grad_mags):\n",
    "            layer_name = f\"Layer {net.num_layers - i}\"\n",
    "            if i == 0:\n",
    "                layer_name += \" (output)\"\n",
    "            elif i == len(grad_mags) - 1:\n",
    "                layer_name += \" (first hidden)\"\n",
    "            else:\n",
    "                layer_name += \" (hidden)\"\n",
    "            \n",
    "            print(f\"  {layer_name}: {mag:.6f}\")\n",
    "        \n",
    "        # Calculate gradient decay ratio\n",
    "        if len(grad_mags) > 1:\n",
    "            decay_ratio = grad_mags[-1] / grad_mags[0]  # First layer / output layer\n",
    "            print(f\"\\nGradient decay ratio (first/output): {decay_ratio:.6f}\")\n",
    "            \n",
    "            if decay_ratio < 0.01:\n",
    "                print(\"⚠️  Severe vanishing gradient problem!\")\n",
    "            elif decay_ratio < 0.1:\n",
    "                print(\"⚠️  Moderate vanishing gradient problem\")\n",
    "            elif decay_ratio > 10:\n",
    "                print(\"⚠️  Exploding gradient problem!\")\n",
    "            else:\n",
    "                print(\"✅ Gradient flow looks healthy\")\n",
    "        \n",
    "        # Analyze activation saturation\n",
    "        print(\"\\nActivation analysis:\")\n",
    "        for i, (activation, z) in enumerate(zip(activations[1:], z_values)):\n",
    "            layer_num = i + 1\n",
    "            if isinstance(activation, np.ndarray):\n",
    "                avg_activation = np.mean(activation)\n",
    "                avg_z = np.mean(z)\n",
    "            else:\n",
    "                avg_activation = activation\n",
    "                avg_z = z\n",
    "            \n",
    "            print(f\"  Layer {layer_num}: avg_z = {avg_z:.4f}, avg_activation = {avg_activation:.4f}\")\n",
    "            \n",
    "            # Check for saturation in sigmoid\n",
    "            if net.activation == 'sigmoid' and layer_num < net.num_layers:\n",
    "                if abs(avg_z) > 3:\n",
    "                    print(f\"    ⚠️  Saturated sigmoid (|z| = {abs(avg_z):.2f} > 3)\")\n",
    "                else:\n",
    "                    print(f\"    ✅ Good sigmoid range (|z| = {abs(avg_z):.2f} < 3)\")\n",
    "    \n",
    "    return gradient_data\n",
    "\n",
    "gradient_analysis = analyze_gradient_flow()\n",
    "print(\"\\n✅ Gradient flow analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize gradient flow patterns\n",
    "def visualize_gradient_flow(gradient_data):\n",
    "    \"\"\"\n",
    "    Create visualizations of gradient flow through networks.\n",
    "    \"\"\"\n",
    "    print(\"\\nGRADIENT FLOW VISUALIZATION\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot 1: Gradient magnitude decay\n",
    "    ax = axes[0]\n",
    "    for name, data in gradient_data.items():\n",
    "        layers = list(range(data['num_layers'], 0, -1))  # Output to input\n",
    "        magnitudes = data['magnitudes']\n",
    "        ax.plot(layers, magnitudes, 'o-', linewidth=2, markersize=8, label=name)\n",
    "    \n",
    "    ax.set_xlabel('Layer Number (Output → Input)')\n",
    "    ax.set_ylabel('Gradient Magnitude')\n",
    "    ax.set_title('Gradient Magnitude by Layer')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Gradient decay ratio\n",
    "    ax = axes[1]\n",
    "    network_names = []\n",
    "    decay_ratios = []\n",
    "    \n",
    "    for name, data in gradient_data.items():\n",
    "        if len(data['magnitudes']) > 1:\n",
    "            ratio = data['magnitudes'][-1] / data['magnitudes'][0]\n",
    "            network_names.append(name)\n",
    "            decay_ratios.append(ratio)\n",
    "    \n",
    "    bars = ax.bar(network_names, decay_ratios, alpha=0.7)\n",
    "    ax.set_ylabel('Gradient Decay Ratio')\n",
    "    ax.set_title('Gradient Decay: First Layer / Output Layer')\n",
    "    ax.set_yscale('log')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color bars based on severity\n",
    "    for bar, ratio in zip(bars, decay_ratios):\n",
    "        if ratio < 0.01:\n",
    "            bar.set_color('red')  # Severe vanishing\n",
    "        elif ratio < 0.1:\n",
    "            bar.set_color('orange')  # Moderate vanishing\n",
    "        elif ratio > 10:\n",
    "            bar.set_color('purple')  # Exploding\n",
    "        else:\n",
    "            bar.set_color('green')  # Healthy\n",
    "    \n",
    "    # Plot 3: Activation distributions\n",
    "    ax = axes[2]\n",
    "    \n",
    "    # Compare different network depths for the deepest network\n",
    "    deepest_net_name = max(gradient_data.keys(), key=lambda k: gradient_data[k]['num_layers'])\n",
    "    deepest_data = gradient_data[deepest_net_name]\n",
    "    \n",
    "    for i, activation in enumerate(deepest_data['activations'][1:], 1):  # Skip input\n",
    "        if isinstance(activation, np.ndarray) and len(activation) > 1:\n",
    "            ax.hist(activation, bins=10, alpha=0.6, label=f'Layer {i}', density=True)\n",
    "        else:\n",
    "            # Single value - show as vertical line\n",
    "            ax.axvline(activation, alpha=0.8, linewidth=3, label=f'Layer {i}')\n",
    "    \n",
    "    ax.set_xlabel('Activation Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Activation Distributions ({deepest_net_name})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Chain rule multiplication effect\n",
    "    ax = axes[3]\n",
    "    \n",
    "    # Simulate cumulative gradient multiplication\n",
    "    x_vals = np.linspace(-3, 3, 100)\n",
    "    sigmoid_deriv_vals = sigmoid_derivative(x_vals)\n",
    "    \n",
    "    cumulative_products = []\n",
    "    for num_layers in [1, 2, 3, 4, 5]:\n",
    "        products = sigmoid_deriv_vals ** num_layers\n",
    "        cumulative_products.append(products)\n",
    "        ax.plot(x_vals, products, linewidth=2, label=f'{num_layers} layers')\n",
    "    \n",
    "    ax.set_xlabel('Pre-activation Value (z)')\n",
    "    ax.set_ylabel('Cumulative Derivative Product')\n",
    "    ax.set_title('Chain Rule Effect: σ\\'(z)^n')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis summary\n",
    "    print(\"\\nKEY INSIGHTS FROM VISUALIZATION:\")\n",
    "    print(\"1. Plot 1 shows how gradient magnitude decreases through layers\")\n",
    "    print(\"2. Plot 2 quantifies the vanishing gradient problem\")\n",
    "    print(\"3. Plot 3 reveals activation saturation patterns\")\n",
    "    print(\"4. Plot 4 demonstrates why deep networks struggle with sigmoid\")\n",
    "\n",
    "visualize_gradient_flow(gradient_analysis)\n",
    "print(\"\\n✅ Gradient flow visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Backpropagation Implementation\n",
    "\n",
    "Let's implement backpropagation from scratch, showing exactly how the chain rule is applied systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement detailed backpropagation with step-by-step chain rule\n",
    "class DetailedBackpropNetwork:\n",
    "    \"\"\"\n",
    "    Network that shows detailed backpropagation steps for educational purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        np.random.seed(42)  # For reproducible analysis\n",
    "        for i in range(self.num_layers):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.5\n",
    "            b = np.zeros(layer_sizes[i+1])\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def detailed_forward_pass(self, x, verbose=True):\n",
    "        \"\"\"\n",
    "        Forward pass with detailed intermediate steps.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"DETAILED FORWARD PASS\")\n",
    "            print(\"=\" * 25)\n",
    "        \n",
    "        activations = [x]\n",
    "        z_values = []\n",
    "        \n",
    "        current_input = x\n",
    "        if verbose:\n",
    "            print(f\"Input: {current_input}\")\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            if verbose:\n",
    "                print(f\"\\nLayer {i+1}:\")\n",
    "                print(f\"  Weights shape: {self.weights[i].shape}\")\n",
    "                print(f\"  Weights:\\n{self.weights[i]}\")\n",
    "                print(f\"  Biases: {self.biases[i]}\")\n",
    "            \n",
    "            # Linear transformation\n",
    "            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
    "            z_values.append(z)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Pre-activation (z): {z}\")\n",
    "            \n",
    "            # Activation\n",
    "            a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Post-activation (a): {a}\")\n",
    "            \n",
    "            current_input = a\n",
    "        \n",
    "        return current_input, activations, z_values\n",
    "    \n",
    "    def detailed_backward_pass(self, x, y, activations, z_values, verbose=True):\n",
    "        \"\"\"\n",
    "        Backward pass with detailed chain rule steps.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\nDETAILED BACKWARD PASS (CHAIN RULE APPLICATION)\")\n",
    "            print(\"=\" * 55)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        grad_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Start with loss gradient\n",
    "        output = activations[-1]\n",
    "        loss = bce_loss(np.array([output]), np.array([y]))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Final output: {output}\")\n",
    "            print(f\"Target: {y}\")\n",
    "            print(f\"Loss: {loss:.6f}\")\n",
    "            print(f\"\\nStarting backpropagation...\")\n",
    "        \n",
    "        # For BCE + sigmoid: ∂L/∂a = (a - y) / (a(1-a))\n",
    "        # But for ∂L/∂z (before sigmoid): ∂L/∂z = a - y\n",
    "        delta = output - y\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nOutput layer gradient:\")\n",
    "            print(f\"  ∂L/∂z_output = prediction - target = {output:.6f} - {y} = {delta:.6f}\")\n",
    "        \n",
    "        # Backward through each layer\n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            layer_num = i + 1\n",
    "            if verbose:\n",
    "                print(f\"\\nLayer {layer_num} gradients:\")\n",
    "                print(f\"  Current delta (∂L/∂z): {delta}\")\n",
    "            \n",
    "            # Gradients for weights and biases\n",
    "            # ∂L/∂W = a^(l-1) ⊗ δ^(l) (outer product)\n",
    "            # ∂L/∂b = δ^(l)\n",
    "            prev_activation = activations[i]\n",
    "            \n",
    "            if isinstance(delta, np.ndarray) and isinstance(prev_activation, np.ndarray):\n",
    "                if delta.ndim == 0 and prev_activation.ndim == 1:\n",
    "                    grad_weights[i] = np.outer(prev_activation, delta)\n",
    "                else:\n",
    "                    grad_weights[i] = np.outer(prev_activation, delta)\n",
    "            else:\n",
    "                grad_weights[i] = np.outer(prev_activation, delta)\n",
    "            \n",
    "            grad_biases[i] = delta.copy() if isinstance(delta, np.ndarray) else np.array([delta])\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Previous activation: {prev_activation}\")\n",
    "                print(f\"  ∂L/∂W (outer product): \\n{grad_weights[i]}\")\n",
    "                print(f\"  ∂L/∂b: {grad_biases[i]}\")\n",
    "            \n",
    "            # Propagate delta to previous layer (if not input layer)\n",
    "            if i > 0:\n",
    "                # ∂L/∂a^(l-1) = W^(l) × δ^(l)\n",
    "                delta_prev = np.dot(self.weights[i], delta)\n",
    "                \n",
    "                # ∂L/∂z^(l-1) = ∂L/∂a^(l-1) ⊙ σ'(z^(l-1))\n",
    "                sigmoid_deriv = sigmoid_derivative(z_values[i-1])\n",
    "                delta = delta_prev * sigmoid_deriv\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Propagating to layer {i}:\")\n",
    "                    print(f\"    ∂L/∂a_prev = W^T × δ = {self.weights[i].flatten()} × {delta} = {delta_prev}\")\n",
    "                    print(f\"    σ'(z_prev) = {sigmoid_deriv}\")\n",
    "                    print(f\"    ∂L/∂z_prev = ∂L/∂a_prev ⊙ σ'(z_prev) = {delta}\")\n",
    "        \n",
    "        return grad_weights, grad_biases, loss\n",
    "    \n",
    "    def numerical_gradients(self, x, y, h=1e-8):\n",
    "        \"\"\"\n",
    "        Compute numerical gradients for verification.\n",
    "        \"\"\"\n",
    "        numerical_grad_w = []\n",
    "        numerical_grad_b = []\n",
    "        \n",
    "        def compute_loss(weights, biases):\n",
    "            # Temporarily set weights and biases\n",
    "            old_weights = [w.copy() for w in self.weights]\n",
    "            old_biases = [b.copy() for b in self.biases]\n",
    "            \n",
    "            for i in range(len(weights)):\n",
    "                self.weights[i] = weights[i]\n",
    "                self.biases[i] = biases[i]\n",
    "            \n",
    "            output, _, _ = self.detailed_forward_pass(x, verbose=False)\n",
    "            loss = bce_loss(np.array([output]), np.array([y]))\n",
    "            \n",
    "            # Restore weights and biases\n",
    "            for i in range(len(old_weights)):\n",
    "                self.weights[i] = old_weights[i]\n",
    "                self.biases[i] = old_biases[i]\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        # Numerical gradients for weights\n",
    "        for i in range(self.num_layers):\n",
    "            grad_w = np.zeros_like(self.weights[i])\n",
    "            \n",
    "            for row in range(self.weights[i].shape[0]):\n",
    "                for col in range(self.weights[i].shape[1]):\n",
    "                    # Perturb weight\n",
    "                    weights_plus = [w.copy() for w in self.weights]\n",
    "                    weights_minus = [w.copy() for w in self.weights]\n",
    "                    weights_plus[i][row, col] += h\n",
    "                    weights_minus[i][row, col] -= h\n",
    "                    \n",
    "                    # Compute finite difference\n",
    "                    loss_plus = compute_loss(weights_plus, self.biases)\n",
    "                    loss_minus = compute_loss(weights_minus, self.biases)\n",
    "                    grad_w[row, col] = (loss_plus - loss_minus) / (2 * h)\n",
    "            \n",
    "            numerical_grad_w.append(grad_w)\n",
    "        \n",
    "        # Similar for biases (simplified for brevity)\n",
    "        for i in range(self.num_layers):\n",
    "            numerical_grad_b.append(np.zeros_like(self.biases[i]))\n",
    "        \n",
    "        return numerical_grad_w, numerical_grad_b\n",
    "\n",
    "# Test detailed backpropagation\n",
    "print(\"DETAILED BACKPROPAGATION IMPLEMENTATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create a simple 2-layer network for detailed analysis\n",
    "detailed_net = DetailedBackpropNetwork([3, 2, 1])\n",
    "\n",
    "# Test on \"Go Dolphins!\" example\n",
    "x = features[0]  # [2, 1, 1]\n",
    "y = labels[0]    # 1\n",
    "\n",
    "print(f\"Testing on: '{texts[0]}'\")\n",
    "print(f\"Features: {x}\")\n",
    "print(f\"True label: {y}\")\n",
    "\n",
    "# Forward pass\n",
    "output, activations, z_values = detailed_net.detailed_forward_pass(x)\n",
    "\n",
    "# Backward pass\n",
    "grad_w, grad_b, loss = detailed_net.detailed_backward_pass(x, y, activations, z_values)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Loss: {loss:.6f}\")\n",
    "print(f\"Output: {output:.6f}\")\n",
    "\n",
    "print(\"\\n✅ Detailed backpropagation implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify backpropagation with numerical gradients\n",
    "def verify_backpropagation():\n",
    "    \"\"\"\n",
    "    Verify analytical gradients against numerical gradients.\n",
    "    \"\"\"\n",
    "    print(\"\\nBACKPROPAGATION VERIFICATION\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Use a smaller network for verification\n",
    "    verify_net = DetailedBackpropNetwork([3, 2, 1])\n",
    "    \n",
    "    x = features[0]\n",
    "    y = labels[0]\n",
    "    \n",
    "    # Analytical gradients\n",
    "    output, activations, z_values = verify_net.detailed_forward_pass(x, verbose=False)\n",
    "    analytical_grad_w, analytical_grad_b, _ = verify_net.detailed_backward_pass(\n",
    "        x, y, activations, z_values, verbose=False\n",
    "    )\n",
    "    \n",
    "    # Numerical gradients (for first layer only due to computational cost)\n",
    "    print(\"Computing numerical gradients (first layer only)...\")\n",
    "    numerical_grad_w, numerical_grad_b = verify_net.numerical_gradients(x, y)\n",
    "    \n",
    "    # Compare gradients\n",
    "    print(\"\\nGRADIENT VERIFICATION RESULTS:\")\n",
    "    for i in range(min(2, len(analytical_grad_w))):  # Verify first 2 layers\n",
    "        print(f\"\\nLayer {i+1} Weight Gradients:\")\n",
    "        print(f\"Analytical:\\n{analytical_grad_w[i]}\")\n",
    "        if i < len(numerical_grad_w):\n",
    "            print(f\"Numerical:\\n{numerical_grad_w[i]}\")\n",
    "            \n",
    "            max_diff = np.max(np.abs(analytical_grad_w[i] - numerical_grad_w[i]))\n",
    "            print(f\"Max difference: {max_diff:.2e}\")\n",
    "            \n",
    "            if max_diff < 1e-6:\n",
    "                print(\"✅ Gradients match perfectly!\")\n",
    "            elif max_diff < 1e-4:\n",
    "                print(\"✅ Gradients match well\")\n",
    "            else:\n",
    "                print(\"⚠️  Gradient mismatch - check implementation\")\n",
    "    \n",
    "    # Summary of chain rule application\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CHAIN RULE SUMMARY FOR DEEP LEARNING\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\n1. FORWARD PASS: Compute activations layer by layer\")\n",
    "    print(\"   a⁽⁰⁾ = x (input)\")\n",
    "    print(\"   z⁽ˡ⁾ = W⁽ˡ⁾ᵀa⁽ˡ⁻¹⁾ + b⁽ˡ⁾\")\n",
    "    print(\"   a⁽ˡ⁾ = σ(z⁽ˡ⁾)\")\n",
    "    print(\"\\n2. BACKWARD PASS: Apply chain rule layer by layer\")\n",
    "    print(\"   δ⁽ᴸ⁾ = ∂L/∂z⁽ᴸ⁾ (output layer)\")\n",
    "    print(\"   δ⁽ˡ⁾ = (W⁽ˡ⁺¹⁾δ⁽ˡ⁺¹⁾) ⊙ σ'(z⁽ˡ⁾) (hidden layers)\")\n",
    "    print(\"\\n3. GRADIENTS: Compute parameter updates\")\n",
    "    print(\"   ∂L/∂W⁽ˡ⁾ = a⁽ˡ⁻¹⁾ ⊗ δ⁽ˡ⁾\")\n",
    "    print(\"   ∂L/∂b⁽ˡ⁾ = δ⁽ˡ⁾\")\n",
    "    print(\"\\n4. MATHEMATICAL INSIGHT:\")\n",
    "    print(\"   The chain rule enables us to decompose complex gradients\")\n",
    "    print(\"   into manageable, local computations at each layer.\")\n",
    "    print(\"   This is the mathematical foundation of ALL deep learning!\")\n",
    "\n",
    "verify_backpropagation()\n",
    "print(\"\\n✅ Backpropagation verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You've now mastered the mathematical foundation of deep learning - the chain rule and backpropagation! Here's what we discovered:\n",
    "\n",
    "**🔑 Key Mathematical Insights:**\n",
    "1. **Chain Rule Decomposition** - Complex gradients break down into local, manageable computations\n",
    "2. **Backpropagation Algorithm** - Systematic application of chain rule working backwards through layers\n",
    "3. **Gradient Flow Dynamics** - How gradients can vanish or explode through deep architectures\n",
    "4. **Computational Efficiency** - Reusing forward pass computations makes backprop efficient\n",
    "\n",
    "**🧮 Mathematical Tools Mastered:**\n",
    "- **Multi-variable chain rule** for composite functions\n",
    "- **Matrix calculus** for layer-wise gradient computation\n",
    "- **Numerical verification** of analytical gradients\n",
    "- **Gradient flow analysis** through deep architectures\n",
    "\n",
    "**🎯 Why This Matters:**\n",
    "This analysis reveals the mathematical elegance of deep learning:\n",
    "- Every deep network uses the same chain rule principle\n",
    "- Gradient problems (vanishing/exploding) have mathematical explanations\n",
    "- Activation function choice affects gradient flow properties\n",
    "- The backpropagation algorithm is just systematic calculus!\n",
    "\n",
    "**🚀 Coming in Problem 3: Jacobian Analysis**\n",
    "- How do we analyze sensitivity in multi-output systems?\n",
    "- What are Jacobian matrices and why do they matter?\n",
    "- How do we understand system-wide parameter sensitivity?\n",
    "- What role do Jacobians play in optimization landscapes?\n",
    "\n",
    "You're building towards a complete mathematical understanding of modern AI! 🐬➡️📊➡️🎯➡️⚡➡️🚀➡️🧮➡️🔗➡️📐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}