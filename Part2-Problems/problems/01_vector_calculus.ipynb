{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Vector Calculus - Complete Loss Landscape Analysis\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this problem, you will:\n",
    "- Apply vector calculus to analyze the complete loss landscape topology\n",
    "- Understand critical points, saddle points, and convergence basins\n",
    "- Use the Hessian matrix to characterize local curvature properties\n",
    "- Connect gradient magnitude and direction to optimization efficiency\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "1. **Gradient Analysis** - Compute and analyze the complete gradient field\n",
    "2. **Hessian Matrix Analysis** - Second derivatives and curvature characterization\n",
    "3. **Critical Point Classification** - Identify and classify all stationary points\n",
    "4. **Convergence Basin Analysis** - Map regions of attraction for gradient descent\n",
    "\n",
    "---\n",
    "\n",
    "## The Mathematical Deep Dive Begins\n",
    "\n",
    "Welcome to Part 2! You've mastered the fundamentals of machine learning. Now we dive into the advanced mathematical theory that explains WHY these methods work so effectively.\n",
    "\n",
    "In Part 1, you saw gradient descent \"rolling the ball downhill\" to find optimal weights for \"Go Dolphins!\" sentiment classification. But several deep questions remained:\n",
    "\n",
    "- **Why does gradient descent converge reliably?**\n",
    "- **What guarantees that we find good solutions, not just any local minimum?**\n",
    "- **How do we characterize the complete landscape topology?**\n",
    "- **What makes some optimization paths more efficient than others?**\n",
    "\n",
    "These questions require **vector calculus** - the mathematical framework for analyzing multivariable functions and their optimization landscapes.\n",
    "\n",
    "## Vector Calculus for Machine Learning\n",
    "\n",
    "**The Mathematical Setting**:\n",
    "- **Loss function**: $L(\\mathbf{w}) : \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
    "- **Weight space**: $\\mathbf{w} = [w_1, w_2, w_3]^T \\in \\mathbb{R}^3$\n",
    "- **Gradient field**: $\\nabla L(\\mathbf{w}) = [\\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, \\frac{\\partial L}{\\partial w_3}]^T$\n",
    "- **Hessian matrix**: $\\mathbf{H}(\\mathbf{w}) = \\frac{\\partial^2 L}{\\partial \\mathbf{w} \\partial \\mathbf{w}^T}$\n",
    "\n",
    "**Key Insights We'll Discover**:\n",
    "1. **Gradient = Steepest ascent direction** (so negative gradient = steepest descent)\n",
    "2. **Hessian eigenvalues determine local curvature** (convex vs. concave vs. saddle)\n",
    "3. **Critical points satisfy** $\\nabla L(\\mathbf{w}^*) = \\mathbf{0}$\n",
    "4. **Second derivative test** classifies critical points using Hessian properties\n",
    "\n",
    "This mathematical analysis will explain why your \"Go Dolphins!\" classifier learned so effectively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports for advanced mathematical analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy.linalg import eigvals, eig\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Import our utilities\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from data_generators import load_sports_dataset\n",
    "from gradient_helpers import analytical_gradient_bce, compute_hessian\n",
    "from visualization import plot_3d_loss_surface, plot_gradient_field\n",
    "\n",
    "# Load our \"Go Dolphins!\" dataset\n",
    "features, labels, feature_names, texts = load_sports_dataset()\n",
    "\n",
    "print(\"ADVANCED MATHEMATICAL ANALYSIS OF 'GO DOLPHINS!' CLASSIFIER\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"Dataset: {len(texts)} sports tweets\")\n",
    "print(f\"Parameter space: ℝ³ (3 weights for features {feature_names})\")\n",
    "print(f\"Loss function: Binary Cross-Entropy with sigmoid activation\")\n",
    "print()\n",
    "print(\"Mathematical objects we'll analyze:\")\n",
    "print(\"• Loss function L(w): ℝ³ → ℝ\")\n",
    "print(\"• Gradient field ∇L(w): ℝ³ → ℝ³\")\n",
    "print(\"• Hessian matrix H(w): ℝ³ → ℝ³ˣ³\")\n",
    "print(\"• Critical points: {w* : ∇L(w*) = 0}\")\n",
    "print(\"• Convergence basins: regions of attraction for gradient descent\")\n",
    "\n",
    "# Define our exact loss function for mathematical analysis\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def loss_function(weights):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss for our sports sentiment classifier.\n",
    "    L(w) = -(1/n) Σ [y_i log(σ(w·x_i)) + (1-y_i) log(1-σ(w·x_i))]\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    for i in range(len(features)):\n",
    "        z = np.dot(features[i], weights)\n",
    "        p = sigmoid(z)\n",
    "        # Clip to avoid log(0)\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        loss = -labels[i] * np.log(p) - (1 - labels[i]) * np.log(1 - p)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(features)\n",
    "\n",
    "# Test the loss function\n",
    "test_weights = np.array([0.3, 0.5, 0.4])\n",
    "test_loss = loss_function(test_weights)\n",
    "print(f\"\\nTest evaluation: L([0.3, 0.5, 0.4]) = {test_loss:.6f}\")\n",
    "print(\"Ready for deep mathematical analysis! 🧮\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Gradient Analysis\n",
    "\n",
    "Let's compute and analyze the complete gradient field to understand how the loss function varies throughout weight space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement analytical gradient computation\n",
    "def gradient_function(weights):\n",
    "    \"\"\"\n",
    "    Analytical gradient of binary cross-entropy loss.\n",
    "    ∇L(w) = (1/n) Σ [(σ(w·x_i) - y_i) x_i]\n",
    "    \"\"\"\n",
    "    total_gradient = np.zeros_like(weights)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        z = np.dot(features[i], weights)\n",
    "        p = sigmoid(z)\n",
    "        error = p - labels[i]\n",
    "        gradient_i = error * features[i]\n",
    "        total_gradient += gradient_i\n",
    "    \n",
    "    return total_gradient / len(features)\n",
    "\n",
    "# Verify gradient computation with numerical differentiation\n",
    "def numerical_gradient(f, weights, h=1e-8):\n",
    "    \"\"\"\n",
    "    Compute numerical gradient using finite differences.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(weights)\n",
    "    for i in range(len(weights)):\n",
    "        weights_plus = weights.copy()\n",
    "        weights_minus = weights.copy()\n",
    "        weights_plus[i] += h\n",
    "        weights_minus[i] -= h\n",
    "        grad[i] = (f(weights_plus) - f(weights_minus)) / (2 * h)\n",
    "    return grad\n",
    "\n",
    "print(\"GRADIENT ANALYSIS AND VERIFICATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test gradient at several points\n",
    "test_points = [\n",
    "    np.array([0.0, 0.0, 0.0]),\n",
    "    np.array([0.3, 0.5, 0.4]),\n",
    "    np.array([1.0, 1.0, 1.0]),\n",
    "    np.array([-0.5, 0.2, 0.8])\n",
    "]\n",
    "\n",
    "for i, point in enumerate(test_points):\n",
    "    analytical_grad = gradient_function(point)\n",
    "    numerical_grad = numerical_gradient(loss_function, point)\n",
    "    \n",
    "    print(f\"\\nTest point {i+1}: {point}\")\n",
    "    print(f\"Analytical gradient: {analytical_grad}\")\n",
    "    print(f\"Numerical gradient:  {numerical_grad}\")\n",
    "    print(f\"Max difference: {np.max(np.abs(analytical_grad - numerical_grad)):.2e}\")\n",
    "    print(f\"Gradient magnitude: {np.linalg.norm(analytical_grad):.6f}\")\n",
    "    \n",
    "    # Gradient direction analysis\n",
    "    if np.linalg.norm(analytical_grad) > 1e-10:\n",
    "        unit_grad = analytical_grad / np.linalg.norm(analytical_grad)\n",
    "        print(f\"Unit gradient (steepest ascent): {unit_grad}\")\n",
    "        print(f\"Steepest descent direction: {-unit_grad}\")\n",
    "    else:\n",
    "        print(\"Near critical point! (gradient ≈ 0)\")\n",
    "\n",
    "print(\"\\n✅ Gradient computation verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the gradient field\n",
    "def plot_gradient_field_slice(w1_range, w2_range, fixed_w3=0.4, resolution=15):\n",
    "    \"\"\"\n",
    "    Plot gradient field in a 2D slice of weight space.\n",
    "    \"\"\"\n",
    "    w1_vals = np.linspace(w1_range[0], w1_range[1], resolution)\n",
    "    w2_vals = np.linspace(w2_range[0], w2_range[1], resolution)\n",
    "    \n",
    "    W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "    \n",
    "    # Compute loss and gradients at each point\n",
    "    Loss = np.zeros_like(W1)\n",
    "    Grad1 = np.zeros_like(W1)\n",
    "    Grad2 = np.zeros_like(W1)\n",
    "    \n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            weights = np.array([W1[i, j], W2[i, j], fixed_w3])\n",
    "            Loss[i, j] = loss_function(weights)\n",
    "            grad = gradient_function(weights)\n",
    "            Grad1[i, j] = grad[0]  # ∂L/∂w1\n",
    "            Grad2[i, j] = grad[1]  # ∂L/∂w2\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot 1: Loss surface with contours\n",
    "    contour = axes[0].contourf(W1, W2, Loss, levels=20, cmap='viridis', alpha=0.8)\n",
    "    axes[0].contour(W1, W2, Loss, levels=20, colors='white', alpha=0.6, linewidths=0.5)\n",
    "    fig.colorbar(contour, ax=axes[0], label='Loss Value')\n",
    "    axes[0].set_xlabel('w₁ (word_count weight)')\n",
    "    axes[0].set_ylabel('w₂ (has_team weight)')\n",
    "    axes[0].set_title(f'Loss Surface (w₃ = {fixed_w3})')\n",
    "    \n",
    "    # Plot 2: Gradient field (vector field)\n",
    "    axes[1].contourf(W1, W2, Loss, levels=20, cmap='viridis', alpha=0.3)\n",
    "    # Subsample for cleaner arrows\n",
    "    skip = 2\n",
    "    axes[1].quiver(W1[::skip, ::skip], W2[::skip, ::skip], \n",
    "                   -Grad1[::skip, ::skip], -Grad2[::skip, ::skip],  # Negative = descent direction\n",
    "                   color='red', alpha=0.8, scale=50, width=0.003)\n",
    "    axes[1].set_xlabel('w₁ (word_count weight)')\n",
    "    axes[1].set_ylabel('w₂ (has_team weight)')\n",
    "    axes[1].set_title('Gradient Field (Red arrows = descent direction)')\n",
    "    \n",
    "    # Plot 3: Gradient magnitude\n",
    "    grad_magnitude = np.sqrt(Grad1**2 + Grad2**2)\n",
    "    magnitude_plot = axes[2].contourf(W1, W2, grad_magnitude, levels=20, cmap='plasma')\n",
    "    fig.colorbar(magnitude_plot, ax=axes[2], label='Gradient Magnitude')\n",
    "    axes[2].set_xlabel('w₁ (word_count weight)')\n",
    "    axes[2].set_ylabel('w₂ (has_team weight)')\n",
    "    axes[2].set_title('Gradient Magnitude |∇L|')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return W1, W2, Loss, Grad1, Grad2\n",
    "\n",
    "print(\"GRADIENT FIELD VISUALIZATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Visualize gradient field in weight space\n",
    "W1, W2, Loss, Grad1, Grad2 = plot_gradient_field_slice(\n",
    "    w1_range=(-1.0, 1.5), \n",
    "    w2_range=(-1.0, 1.5), \n",
    "    fixed_w3=0.4,\n",
    "    resolution=15\n",
    ")\n",
    "\n",
    "print(\"GRADIENT FIELD ANALYSIS:\")\n",
    "print(f\"• Minimum loss in slice: {np.min(Loss):.6f}\")\n",
    "print(f\"• Maximum loss in slice: {np.max(Loss):.6f}\")\n",
    "print(f\"• Maximum gradient magnitude: {np.max(np.sqrt(Grad1**2 + Grad2**2)):.6f}\")\n",
    "print(f\"• Minimum gradient magnitude: {np.min(np.sqrt(Grad1**2 + Grad2**2)):.6f}\")\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"1. Red arrows show gradient descent directions\")\n",
    "print(\"2. Arrow convergence points indicate potential minima\")\n",
    "print(\"3. Gradient magnitude shows steepness of landscape\")\n",
    "print(\"4. Contour lines reveal loss level sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Hessian Matrix Analysis\n",
    "\n",
    "The Hessian matrix contains all second-order partial derivatives, revealing the curvature properties of our loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Hessian matrix computation\n",
    "def hessian_function(weights):\n",
    "    \"\"\"\n",
    "    Analytical Hessian matrix of binary cross-entropy loss.\n",
    "    H_ij(w) = ∂²L/∂w_i∂w_j = (1/n) Σ [σ(w·x_k)(1-σ(w·x_k)) x_k[i] x_k[j]]\n",
    "    \"\"\"\n",
    "    n_features = len(weights)\n",
    "    hessian = np.zeros((n_features, n_features))\n",
    "    \n",
    "    for k in range(len(features)):\n",
    "        z = np.dot(features[k], weights)\n",
    "        p = sigmoid(z)\n",
    "        # Second derivative of sigmoid is p(1-p)\n",
    "        weight_factor = p * (1 - p)\n",
    "        \n",
    "        # Outer product of features weighted by sigmoid derivative\n",
    "        feature_outer = np.outer(features[k], features[k])\n",
    "        hessian += weight_factor * feature_outer\n",
    "    \n",
    "    return hessian / len(features)\n",
    "\n",
    "def numerical_hessian(f, weights, h=1e-6):\n",
    "    \"\"\"\n",
    "    Compute numerical Hessian using finite differences.\n",
    "    \"\"\"\n",
    "    n = len(weights)\n",
    "    hessian = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Compute ∂²f/∂w_i∂w_j using finite differences\n",
    "            weights_pp = weights.copy()\n",
    "            weights_pm = weights.copy()\n",
    "            weights_mp = weights.copy()\n",
    "            weights_mm = weights.copy()\n",
    "            \n",
    "            weights_pp[i] += h\n",
    "            weights_pp[j] += h\n",
    "            \n",
    "            weights_pm[i] += h\n",
    "            weights_pm[j] -= h\n",
    "            \n",
    "            weights_mp[i] -= h\n",
    "            weights_mp[j] += h\n",
    "            \n",
    "            weights_mm[i] -= h\n",
    "            weights_mm[j] -= h\n",
    "            \n",
    "            hessian[i, j] = (f(weights_pp) - f(weights_pm) - f(weights_mp) + f(weights_mm)) / (4 * h**2)\n",
    "    \n",
    "    return hessian\n",
    "\n",
    "print(\"HESSIAN MATRIX ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test Hessian computation at key points\n",
    "test_points = [\n",
    "    np.array([0.0, 0.0, 0.0]),\n",
    "    np.array([0.3, 0.5, 0.4]),\n",
    "    np.array([1.0, 1.0, 1.0])\n",
    "]\n",
    "\n",
    "for i, point in enumerate(test_points):\n",
    "    print(f\"\\nAnalysis at point {i+1}: {point}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Compute Hessian\n",
    "    analytical_hessian = hessian_function(point)\n",
    "    \n",
    "    print(\"Analytical Hessian matrix:\")\n",
    "    print(analytical_hessian)\n",
    "    \n",
    "    # Verify with numerical computation (smaller subset for speed)\n",
    "    if i == 1:  # Only verify for one point\n",
    "        numerical_hess = numerical_hessian(loss_function, point)\n",
    "        print(\"\\nNumerical Hessian matrix:\")\n",
    "        print(numerical_hess)\n",
    "        print(f\"\\nMax difference: {np.max(np.abs(analytical_hessian - numerical_hess)):.2e}\")\n",
    "    \n",
    "    # Eigenvalue analysis\n",
    "    eigenvalues, eigenvectors = eig(analytical_hessian)\n",
    "    eigenvalues = np.real(eigenvalues)  # Should be real for symmetric matrix\n",
    "    \n",
    "    print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "    print(f\"Determinant: {np.linalg.det(analytical_hessian):.6f}\")\n",
    "    print(f\"Trace: {np.trace(analytical_hessian):.6f}\")\n",
    "    \n",
    "    # Curvature classification\n",
    "    if np.all(eigenvalues > 0):\n",
    "        curvature = \"Positive definite (local minimum)\"\n",
    "    elif np.all(eigenvalues < 0):\n",
    "        curvature = \"Negative definite (local maximum)\"\n",
    "    elif np.any(eigenvalues > 0) and np.any(eigenvalues < 0):\n",
    "        curvature = \"Indefinite (saddle point)\"\n",
    "    else:\n",
    "        curvature = \"Positive/negative semidefinite (degenerate)\"\n",
    "    \n",
    "    print(f\"Curvature type: {curvature}\")\n",
    "    \n",
    "    # Condition number (optimization difficulty)\n",
    "    if np.min(eigenvalues) > 1e-12:\n",
    "        condition_number = np.max(eigenvalues) / np.min(eigenvalues)\n",
    "        print(f\"Condition number: {condition_number:.2f}\")\n",
    "        if condition_number > 100:\n",
    "            print(\"⚠️  High condition number - optimization may be slow\")\n",
    "        else:\n",
    "            print(\"✅ Good conditioning for optimization\")\n",
    "    \n",
    "    print(f\"Gradient at this point: {gradient_function(point)}\")\n",
    "    print(f\"Loss at this point: {loss_function(point):.6f}\")\n",
    "\n",
    "print(\"\\n✅ Hessian analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Critical Point Classification\n",
    "\n",
    "Let's find and classify all critical points where the gradient vanishes: ∇L(w*) = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find critical points using optimization\n",
    "from scipy.optimize import minimize, root\n",
    "\n",
    "def find_critical_points():\n",
    "    \"\"\"\n",
    "    Find critical points by solving ∇L(w) = 0 using multiple starting points.\n",
    "    \"\"\"\n",
    "    critical_points = []\n",
    "    \n",
    "    # Try multiple starting points to find different critical points\n",
    "    starting_points = [\n",
    "        np.array([0.0, 0.0, 0.0]),\n",
    "        np.array([1.0, 1.0, 1.0]),\n",
    "        np.array([-1.0, -1.0, -1.0]),\n",
    "        np.array([0.5, -0.5, 0.5]),\n",
    "        np.array([-0.5, 0.5, -0.5]),\n",
    "        np.random.randn(3),\n",
    "        np.random.randn(3),\n",
    "        np.random.randn(3)\n",
    "    ]\n",
    "    \n",
    "    for start in starting_points:\n",
    "        try:\n",
    "            # Find minimum (critical point where gradient = 0)\n",
    "            result = minimize(loss_function, start, jac=gradient_function, \n",
    "                            method='BFGS', options={'gtol': 1e-8})\n",
    "            \n",
    "            if result.success:\n",
    "                candidate = result.x\n",
    "                grad_norm = np.linalg.norm(gradient_function(candidate))\n",
    "                \n",
    "                # Check if this is truly a critical point\n",
    "                if grad_norm < 1e-6:\n",
    "                    # Check if we already found this point\n",
    "                    is_new = True\n",
    "                    for existing in critical_points:\n",
    "                        if np.linalg.norm(candidate - existing['point']) < 1e-4:\n",
    "                            is_new = False\n",
    "                            break\n",
    "                    \n",
    "                    if is_new:\n",
    "                        critical_points.append({\n",
    "                            'point': candidate,\n",
    "                            'loss': loss_function(candidate),\n",
    "                            'gradient_norm': grad_norm\n",
    "                        })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return critical_points\n",
    "\n",
    "def classify_critical_point(point):\n",
    "    \"\"\"\n",
    "    Classify a critical point using the Hessian matrix.\n",
    "    \"\"\"\n",
    "    hessian = hessian_function(point)\n",
    "    eigenvalues = np.real(eigvals(hessian))\n",
    "    \n",
    "    if np.all(eigenvalues > 1e-8):\n",
    "        return \"Local minimum\", eigenvalues\n",
    "    elif np.all(eigenvalues < -1e-8):\n",
    "        return \"Local maximum\", eigenvalues\n",
    "    elif np.any(eigenvalues > 1e-8) and np.any(eigenvalues < -1e-8):\n",
    "        return \"Saddle point\", eigenvalues\n",
    "    else:\n",
    "        return \"Degenerate critical point\", eigenvalues\n",
    "\n",
    "print(\"CRITICAL POINT ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Find all critical points\n",
    "critical_points = find_critical_points()\n",
    "\n",
    "print(f\"Found {len(critical_points)} critical point(s):\\n\")\n",
    "\n",
    "for i, cp in enumerate(critical_points):\n",
    "    point = cp['point']\n",
    "    loss_val = cp['loss']\n",
    "    grad_norm = cp['gradient_norm']\n",
    "    \n",
    "    print(f\"Critical Point {i+1}:\")\n",
    "    print(f\"  Location: [{point[0]:.6f}, {point[1]:.6f}, {point[2]:.6f}]\")\n",
    "    print(f\"  Loss value: {loss_val:.8f}\")\n",
    "    print(f\"  Gradient norm: {grad_norm:.2e}\")\n",
    "    \n",
    "    # Classify the critical point\n",
    "    classification, eigenvalues = classify_critical_point(point)\n",
    "    print(f\"  Type: {classification}\")\n",
    "    print(f\"  Hessian eigenvalues: {eigenvalues}\")\n",
    "    \n",
    "    # Compute condition number\n",
    "    if np.all(eigenvalues > 1e-12):\n",
    "        condition_number = np.max(eigenvalues) / np.min(eigenvalues)\n",
    "        print(f\"  Condition number: {condition_number:.2f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Find the global minimum\n",
    "if critical_points:\n",
    "    min_loss_idx = np.argmin([cp['loss'] for cp in critical_points])\n",
    "    global_min = critical_points[min_loss_idx]\n",
    "    \n",
    "    print(f\"🎯 GLOBAL MINIMUM (among found critical points):\")\n",
    "    print(f\"   Weights: {global_min['point']}\")\n",
    "    print(f\"   Loss: {global_min['loss']:.8f}\")\n",
    "    print(f\"   This represents the optimal 'Go Dolphins!' classifier weights!\")\n",
    "else:\n",
    "    print(\"⚠️  No critical points found - may need different search strategy\")\n",
    "\n",
    "print(\"\\n✅ Critical point analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Convergence Basin Analysis\n",
    "\n",
    "Let's map the regions of attraction - which starting points lead gradient descent to which critical points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze convergence basins\n",
    "def gradient_descent_convergence(start_point, learning_rate=0.1, max_iter=1000, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Run gradient descent and return convergence information.\n",
    "    \"\"\"\n",
    "    weights = start_point.copy()\n",
    "    path = [weights.copy()]\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        grad = gradient_function(weights)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        \n",
    "        weights = weights - learning_rate * grad\n",
    "        path.append(weights.copy())\n",
    "    \n",
    "    return {\n",
    "        'final_point': weights,\n",
    "        'final_loss': loss_function(weights),\n",
    "        'iterations': iteration + 1,\n",
    "        'converged': grad_norm < tol,\n",
    "        'path': np.array(path)\n",
    "    }\n",
    "\n",
    "def map_convergence_basins(w1_range, w2_range, fixed_w3=0.4, resolution=20):\n",
    "    \"\"\"\n",
    "    Map convergence basins by running gradient descent from a grid of starting points.\n",
    "    \"\"\"\n",
    "    w1_vals = np.linspace(w1_range[0], w1_range[1], resolution)\n",
    "    w2_vals = np.linspace(w2_range[0], w2_range[1], resolution)\n",
    "    \n",
    "    W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "    \n",
    "    # Store results\n",
    "    final_losses = np.zeros_like(W1)\n",
    "    convergence_iterations = np.zeros_like(W1)\n",
    "    final_w1 = np.zeros_like(W1)\n",
    "    final_w2 = np.zeros_like(W1)\n",
    "    \n",
    "    print(f\"Mapping convergence basins ({resolution}×{resolution} grid)...\")\n",
    "    \n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            start_point = np.array([W1[i, j], W2[i, j], fixed_w3])\n",
    "            result = gradient_descent_convergence(start_point, learning_rate=0.1)\n",
    "            \n",
    "            final_losses[i, j] = result['final_loss']\n",
    "            convergence_iterations[i, j] = result['iterations']\n",
    "            final_w1[i, j] = result['final_point'][0]\n",
    "            final_w2[i, j] = result['final_point'][1]\n",
    "    \n",
    "    return W1, W2, final_losses, convergence_iterations, final_w1, final_w2\n",
    "\n",
    "print(\"CONVERGENCE BASIN ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Map convergence basins\n",
    "W1, W2, final_losses, conv_iters, final_w1, final_w2 = map_convergence_basins(\n",
    "    w1_range=(-1.5, 1.5), \n",
    "    w2_range=(-1.5, 1.5), \n",
    "    fixed_w3=0.4,\n",
    "    resolution=15  # Reduced for computational efficiency\n",
    ")\n",
    "\n",
    "# Visualize convergence basins\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Final loss values\n",
    "im1 = axes[0, 0].contourf(W1, W2, final_losses, levels=20, cmap='viridis')\n",
    "fig.colorbar(im1, ax=axes[0, 0], label='Final Loss')\n",
    "axes[0, 0].set_xlabel('w₁ (starting)')\n",
    "axes[0, 0].set_ylabel('w₂ (starting)')\n",
    "axes[0, 0].set_title('Final Loss After Convergence')\n",
    "\n",
    "# Plot 2: Convergence speed\n",
    "im2 = axes[0, 1].contourf(W1, W2, conv_iters, levels=20, cmap='plasma')\n",
    "fig.colorbar(im2, ax=axes[0, 1], label='Iterations to Converge')\n",
    "axes[0, 1].set_xlabel('w₁ (starting)')\n",
    "axes[0, 1].set_ylabel('w₂ (starting)')\n",
    "axes[0, 1].set_title('Convergence Speed')\n",
    "\n",
    "# Plot 3: Final w1 values (shows basins of attraction)\n",
    "im3 = axes[1, 0].contourf(W1, W2, final_w1, levels=20, cmap='coolwarm')\n",
    "fig.colorbar(im3, ax=axes[1, 0], label='Final w₁')\n",
    "axes[1, 0].set_xlabel('w₁ (starting)')\n",
    "axes[1, 0].set_ylabel('w₂ (starting)')\n",
    "axes[1, 0].set_title('Convergence Basins (Final w₁)')\n",
    "\n",
    "# Plot 4: Final w2 values\n",
    "im4 = axes[1, 1].contourf(W1, W2, final_w2, levels=20, cmap='coolwarm')\n",
    "fig.colorbar(im4, ax=axes[1, 1], label='Final w₂')\n",
    "axes[1, 1].set_xlabel('w₁ (starting)')\n",
    "axes[1, 1].set_ylabel('w₂ (starting)')\n",
    "axes[1, 1].set_title('Convergence Basins (Final w₂)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze convergence statistics\n",
    "print(\"\\nCONVERGENCE STATISTICS:\")\n",
    "print(f\"Minimum final loss: {np.min(final_losses):.8f}\")\n",
    "print(f\"Maximum final loss: {np.max(final_losses):.8f}\")\n",
    "print(f\"Loss range: {np.max(final_losses) - np.min(final_losses):.8f}\")\n",
    "print(f\"Average convergence time: {np.mean(conv_iters):.1f} iterations\")\n",
    "print(f\"Max convergence time: {np.max(conv_iters):.0f} iterations\")\n",
    "print(f\"Min convergence time: {np.min(conv_iters):.0f} iterations\")\n",
    "\n",
    "# Count distinct convergence points\n",
    "unique_finals = []\n",
    "tolerance = 1e-3\n",
    "\n",
    "for i in range(final_w1.shape[0]):\n",
    "    for j in range(final_w1.shape[1]):\n",
    "        point = np.array([final_w1[i, j], final_w2[i, j]])\n",
    "        is_new = True\n",
    "        for existing in unique_finals:\n",
    "            if np.linalg.norm(point - existing) < tolerance:\n",
    "                is_new = False\n",
    "                break\n",
    "        if is_new:\n",
    "            unique_finals.append(point)\n",
    "\n",
    "print(f\"\\nNumber of distinct convergence points: {len(unique_finals)}\")\n",
    "print(\"Distinct final points (w₁, w₂):\")\n",
    "for i, point in enumerate(unique_finals):\n",
    "    print(f\"  {i+1}: ({point[0]:.4f}, {point[1]:.4f})\")\n",
    "\n",
    "print(\"\\n✅ Convergence basin analysis complete!\")\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"• Different colors in basin plots show regions of attraction\")\n",
    "print(\"• Similar final colors indicate convergence to same critical point\")\n",
    "print(\"• Convergence speed varies across the landscape\")\n",
    "print(\"• This explains why initialization matters in optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You've now completed a deep mathematical analysis of the \"Go Dolphins!\" loss landscape using vector calculus! Here's what we discovered:\n",
    "\n",
    "**🔑 Key Mathematical Insights:**\n",
    "1. **Gradient Field Analysis** - The vector field ∇L(w) shows steepest ascent directions throughout weight space\n",
    "2. **Hessian Curvature** - Second derivatives reveal local curvature properties (convex/concave/saddle)\n",
    "3. **Critical Point Classification** - Found and classified all points where ∇L(w*) = 0\n",
    "4. **Convergence Basins** - Mapped regions of attraction showing which starting points lead to which solutions\n",
    "\n",
    "**🧮 Mathematical Tools Mastered:**\n",
    "- **Vector calculus**: Gradients, Hessians, and optimization landscapes\n",
    "- **Linear algebra**: Eigenvalue analysis and matrix properties\n",
    "- **Critical point theory**: Classification using second derivative tests\n",
    "- **Dynamical systems**: Basin of attraction analysis\n",
    "\n",
    "**🎯 Why This Matters:**\n",
    "This analysis explains WHY gradient descent works so reliably for machine learning:\n",
    "- The loss landscape has favorable curvature properties\n",
    "- Most starting points converge to good solutions\n",
    "- The Hessian condition number indicates optimization difficulty\n",
    "- Critical point analysis guarantees convergence properties\n",
    "\n",
    "**🚀 Coming in Problem 2: Multi-Layer Chain Rule**\n",
    "- How do gradients flow through multiple layers?\n",
    "- What is backpropagation mathematically?\n",
    "- How does the chain rule enable deep learning?\n",
    "- Why do gradients vanish or explode?\n",
    "\n",
    "You're building the mathematical foundation that powers ALL of modern AI! 🐬➡️📊➡️🎯➡️⚡➡️🚀➡️🧮➡️🔗"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}