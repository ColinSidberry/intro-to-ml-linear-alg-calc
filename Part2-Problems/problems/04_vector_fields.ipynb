{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Vector Fields - Visualizing Optimization Dynamics\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this problem, you will:\n",
    "- Understand optimization as flow in vector fields defined by negative gradients\n",
    "- Visualize and analyze different optimizer behaviors through vector field theory\n",
    "- Connect mathematical concepts like divergence and curl to optimization properties\n",
    "- Apply dynamical systems theory to understand convergence and stability\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "1. **Vector Field Fundamentals** - Gradients as vector fields and flow dynamics\n",
    "2. **Optimizer Vector Fields** - How different optimizers create different flows\n",
    "3. **Divergence and Curl Analysis** - Mathematical properties of optimization flows\n",
    "4. **Dynamical Systems Theory** - Fixed points, stability, and basins of attraction\n",
    "\n",
    "---\n",
    "\n",
    "## From Static Analysis to Dynamic Flows\n",
    "\n",
    "In Problems 1-3, you analyzed static properties of the \"Go Dolphins!\" loss landscape:\n",
    "- **Problem 1**: Loss surface topology and critical points\n",
    "- **Problem 2**: Gradient flow through network layers\n",
    "- **Problem 3**: Sensitivity analysis via Jacobian matrices\n",
    "\n",
    "But optimization is fundamentally a **dynamic process** - weights evolve over time following paths through the landscape. To understand this evolution, we need **vector field theory**.\n",
    "\n",
    "**The Vector Field Perspective**:\n",
    "```\n",
    "Traditional view: w(t+1) = w(t) - α∇L(w(t))\n",
    "Vector field view: dw/dt = -∇L(w) (continuous gradient flow)\n",
    "```\n",
    "\n",
    "This transforms optimization from discrete updates to continuous flow in a vector field defined by the negative gradient.\n",
    "\n",
    "## Mathematical Foundation: Vector Fields\n",
    "\n",
    "**Definition**: A vector field $\\mathbf{F}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ assigns a vector to every point in space.\n",
    "\n",
    "**For optimization**: $\\mathbf{F}(\\mathbf{w}) = -\\nabla L(\\mathbf{w})$ (negative gradient field)\n",
    "\n",
    "**Flow lines**: Curves $\\mathbf{w}(t)$ satisfying $\\frac{d\\mathbf{w}}{dt} = \\mathbf{F}(\\mathbf{w})$\n",
    "\n",
    "**Key Properties**:\n",
    "- **Divergence**: $\\nabla \\cdot \\mathbf{F} = \\sum_i \\frac{\\partial F_i}{\\partial w_i}$ (expansion/contraction of flow)\n",
    "- **Curl**: $\\nabla \\times \\mathbf{F}$ (rotation in the flow)\n",
    "- **Fixed points**: Points where $\\mathbf{F}(\\mathbf{w}^*) = \\mathbf{0}$ (critical points)\n",
    "- **Stability**: Eigenvalues of Jacobian $\\mathbf{J}_F$ determine local behavior\n",
    "\n",
    "**Different Optimizers = Different Vector Fields**:\n",
    "- **SGD**: $\\mathbf{F} = -\\nabla L$\n",
    "- **Momentum**: $\\mathbf{F} = -\\nabla L + \\beta \\mathbf{v}$ (includes velocity)\n",
    "- **Adam**: $\\mathbf{F} = -\\frac{\\hat{\\mathbf{m}}}{\\sqrt{\\hat{\\mathbf{v}}} + \\epsilon}$ (adaptive field)\n",
    "\n",
    "## Why Vector Fields Matter for \"Go Dolphins!\"\n",
    "\n",
    "Understanding optimization as vector field flow reveals:\n",
    "- **Convergence patterns**: Which paths lead to optimal weights?\n",
    "- **Optimizer differences**: How do SGD, momentum, and Adam create different flows?\n",
    "- **Stability regions**: Where is the flow stable vs chaotic?\n",
    "- **Escape mechanisms**: How do optimizers escape local minima?\n",
    "\n",
    "This connects our sentiment classifier to the deepest mathematical principles governing learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for vector field analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import seaborn as sns\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# Import our utilities\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from data_generators import load_sports_dataset\n",
    "\n",
    "# Load our \"Go Dolphins!\" dataset\n",
    "features, labels, feature_names, texts = load_sports_dataset()\n",
    "\n",
    "print(\"VECTOR FIELD ANALYSIS OF 'GO DOLPHINS!' OPTIMIZATION\")\n",
    "print(\"=\" * 58)\n",
    "print(f\"Dataset: {len(texts)} sports tweets\")\n",
    "print(f\"Parameter space: ℝ³ (weights for {feature_names})\")\n",
    "print()\n",
    "print(\"Vector fields we'll analyze:\")\n",
    "print(\"• Gradient field: F(w) = -∇L(w)\")\n",
    "print(\"• Momentum field: F(w,v) = -∇L(w) + βv\")\n",
    "print(\"• Adam field: F(w) = -m̂/(√v̂ + ε)\")\n",
    "print(\"• RMSprop field: F(w) = -∇L(w)/√v\")\n",
    "print()\n",
    "print(\"Mathematical analysis:\")\n",
    "print(\"• Flow line integration\")\n",
    "print(\"• Divergence and curl computation\")\n",
    "print(\"• Fixed point stability analysis\")\n",
    "print(\"• Basin of attraction mapping\")\n",
    "\n",
    "# Define our loss function and gradient for vector field analysis\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid with numerical stability\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def loss_function(weights):\n",
    "    \"\"\"Binary cross-entropy loss for our sentiment classifier\"\"\"\n",
    "    total_loss = 0.0\n",
    "    for i in range(len(features)):\n",
    "        z = np.dot(features[i], weights)\n",
    "        p = sigmoid(z)\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        loss = -labels[i] * np.log(p) - (1 - labels[i]) * np.log(1 - p)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(features)\n",
    "\n",
    "def gradient_function(weights):\n",
    "    \"\"\"Analytical gradient of BCE loss\"\"\"\n",
    "    total_gradient = np.zeros_like(weights)\n",
    "    for i in range(len(features)):\n",
    "        z = np.dot(features[i], weights)\n",
    "        p = sigmoid(z)\n",
    "        error = p - labels[i]\n",
    "        total_gradient += error * features[i]\n",
    "    return total_gradient / len(features)\n",
    "\n",
    "def gradient_vector_field(weights):\n",
    "    \"\"\"Vector field F(w) = -∇L(w) for gradient descent\"\"\"\n",
    "    return -gradient_function(weights)\n",
    "\n",
    "# Test the vector field\n",
    "test_point = np.array([0.3, 0.5, 0.4])\n",
    "test_gradient = gradient_function(test_point)\n",
    "test_field = gradient_vector_field(test_point)\n",
    "\n",
    "print(f\"\\nTest evaluation at w = {test_point}:\")\n",
    "print(f\"Loss: {loss_function(test_point):.6f}\")\n",
    "print(f\"Gradient: {test_gradient}\")\n",
    "print(f\"Vector field: {test_field}\")\n",
    "print(f\"Field magnitude: {np.linalg.norm(test_field):.6f}\")\n",
    "\n",
    "print(\"\\n✅ Vector field analysis setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Vector Field Fundamentals\n",
    "\n",
    "Let's start by visualizing the gradient vector field and understanding flow dynamics in our loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize gradient vector field\n",
    "def create_2d_vector_field_slice(w1_range, w2_range, fixed_w3=0.4, resolution=15):\n",
    "    \"\"\"\n",
    "    Create 2D slice of the 3D vector field for visualization.\n",
    "    \"\"\"\n",
    "    w1_vals = np.linspace(w1_range[0], w1_range[1], resolution)\n",
    "    w2_vals = np.linspace(w2_range[0], w2_range[1], resolution)\n",
    "    \n",
    "    W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "    \n",
    "    # Compute vector field and loss at each point\n",
    "    U = np.zeros_like(W1)  # w1 component of vector field\n",
    "    V = np.zeros_like(W1)  # w2 component of vector field\n",
    "    Loss = np.zeros_like(W1)\n",
    "    \n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            w = np.array([W1[i, j], W2[i, j], fixed_w3])\n",
    "            field_vector = gradient_vector_field(w)\n",
    "            loss_val = loss_function(w)\n",
    "            \n",
    "            U[i, j] = field_vector[0]\n",
    "            V[i, j] = field_vector[1]\n",
    "            Loss[i, j] = loss_val\n",
    "    \n",
    "    return W1, W2, U, V, Loss\n",
    "\n",
    "def plot_vector_field_analysis():\n",
    "    \"\"\"\n",
    "    Create comprehensive vector field visualization.\n",
    "    \"\"\"\n",
    "    print(\"GRADIENT VECTOR FIELD VISUALIZATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Generate vector field data\n",
    "    W1, W2, U, V, Loss = create_2d_vector_field_slice(\n",
    "        w1_range=(-1.0, 1.5), w2_range=(-1.0, 1.5), resolution=12\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Vector field with loss contours\n",
    "    ax = axes[0, 0]\n",
    "    contour = ax.contourf(W1, W2, Loss, levels=20, cmap='viridis', alpha=0.6)\n",
    "    ax.contour(W1, W2, Loss, levels=15, colors='white', alpha=0.8, linewidths=0.5)\n",
    "    \n",
    "    # Add vector field arrows\n",
    "    skip = 1  # Show every arrow\n",
    "    ax.quiver(W1[::skip, ::skip], W2[::skip, ::skip], \n",
    "              U[::skip, ::skip], V[::skip, ::skip],\n",
    "              color='red', alpha=0.8, scale=15, width=0.003, headwidth=3)\n",
    "    \n",
    "    fig.colorbar(contour, ax=ax, label='Loss Value')\n",
    "    ax.set_xlabel('w₁ (word_count weight)')\n",
    "    ax.set_ylabel('w₂ (has_team weight)')\n",
    "    ax.set_title('Gradient Vector Field\\n(Red arrows = -∇L direction)')\n",
    "    \n",
    "    # Plot 2: Vector field magnitude\n",
    "    ax = axes[0, 1]\n",
    "    magnitude = np.sqrt(U**2 + V**2)\n",
    "    magnitude_plot = ax.contourf(W1, W2, magnitude, levels=20, cmap='plasma')\n",
    "    fig.colorbar(magnitude_plot, ax=ax, label='Field Magnitude')\n",
    "    ax.set_xlabel('w₁ (word_count weight)')\n",
    "    ax.set_ylabel('w₂ (has_team weight)')\n",
    "    ax.set_title('Vector Field Magnitude\\n|F(w)| = |∇L(w)|')\n",
    "    \n",
    "    # Plot 3: Streamlines (flow lines)\n",
    "    ax = axes[1, 0]\n",
    "    ax.contourf(W1, W2, Loss, levels=20, cmap='viridis', alpha=0.3)\n",
    "    \n",
    "    # Create streamlines\n",
    "    ax.streamplot(W1, W2, U, V, color='red', linewidth=2, density=1.5, arrowsize=1.5)\n",
    "    ax.set_xlabel('w₁ (word_count weight)')\n",
    "    ax.set_ylabel('w₂ (has_team weight)')\n",
    "    ax.set_title('Flow Lines (Streamlines)\\nPaths of gradient descent')\n",
    "    \n",
    "    # Plot 4: Divergence of vector field\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Compute divergence numerically\n",
    "    dU_dw1 = np.gradient(U, axis=1)\n",
    "    dV_dw2 = np.gradient(V, axis=0)\n",
    "    divergence = dU_dw1 + dV_dw2\n",
    "    \n",
    "    div_plot = ax.contourf(W1, W2, divergence, levels=20, cmap='RdBu_r', \n",
    "                          vmin=-np.max(np.abs(divergence)), vmax=np.max(np.abs(divergence)))\n",
    "    fig.colorbar(div_plot, ax=ax, label='Divergence')\n",
    "    ax.set_xlabel('w₁ (word_count weight)')\n",
    "    ax.set_ylabel('w₂ (has_team weight)')\n",
    "    ax.set_title('Divergence of Vector Field\\n∇·F (expansion/contraction)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return W1, W2, U, V, Loss, magnitude, divergence\n",
    "\n",
    "# Create vector field visualization\n",
    "field_data = plot_vector_field_analysis()\n",
    "W1, W2, U, V, Loss, magnitude, divergence = field_data\n",
    "\n",
    "# Analyze vector field properties\n",
    "print(\"\\nVECTOR FIELD ANALYSIS:\")\n",
    "print(f\"• Average field magnitude: {np.mean(magnitude):.6f}\")\n",
    "print(f\"• Maximum field magnitude: {np.max(magnitude):.6f}\")\n",
    "print(f\"• Minimum field magnitude: {np.min(magnitude):.6f}\")\n",
    "print(f\"• Average divergence: {np.mean(divergence):.6f}\")\n",
    "print(f\"• Divergence range: [{np.min(divergence):.6f}, {np.max(divergence):.6f}]\")\n",
    "\n",
    "# Find regions of high/low field magnitude\n",
    "high_magnitude_threshold = np.percentile(magnitude, 90)\n",
    "low_magnitude_threshold = np.percentile(magnitude, 10)\n",
    "\n",
    "print(f\"\\nREGION ANALYSIS:\")\n",
    "print(f\"• High magnitude regions (>90th percentile): Strong gradients, fast optimization\")\n",
    "print(f\"• Low magnitude regions (<10th percentile): Weak gradients, slow optimization\")\n",
    "print(f\"• High magnitude threshold: {high_magnitude_threshold:.6f}\")\n",
    "print(f\"• Low magnitude threshold: {low_magnitude_threshold:.6f}\")\n",
    "\n",
    "print(\"\\n✅ Vector field fundamentals complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Integrate flow lines to show optimization paths\n",
    "def integrate_flow_lines(starting_points, field_function, t_span=(0, 10), method='RK45'):\n",
    "    \"\"\"\n",
    "    Integrate flow lines in the vector field to show optimization trajectories.\n",
    "    \"\"\"\n",
    "    def vector_field_ode(t, w):\n",
    "        \"\"\"ODE for the vector field: dw/dt = F(w)\"\"\"\n",
    "        if len(w) == 2:\n",
    "            # 2D slice - add fixed third component\n",
    "            w_3d = np.array([w[0], w[1], 0.4])\n",
    "            field_3d = field_function(w_3d)\n",
    "            return field_3d[:2]  # Return only first two components\n",
    "        else:\n",
    "            return field_function(w)\n",
    "    \n",
    "    trajectories = []\n",
    "    \n",
    "    for start_point in starting_points:\n",
    "        try:\n",
    "            # Integrate the ODE\n",
    "            sol = solve_ivp(vector_field_ode, t_span, start_point, \n",
    "                          method=method, dense_output=True, \n",
    "                          rtol=1e-8, atol=1e-10)\n",
    "            \n",
    "            if sol.success:\n",
    "                # Evaluate at regular time intervals\n",
    "                t_eval = np.linspace(t_span[0], t_span[1], 100)\n",
    "                trajectory = sol.sol(t_eval).T\n",
    "                \n",
    "                trajectories.append({\n",
    "                    'start': start_point,\n",
    "                    'trajectory': trajectory,\n",
    "                    'times': t_eval,\n",
    "                    'success': True\n",
    "                })\n",
    "            else:\n",
    "                trajectories.append({\n",
    "                    'start': start_point,\n",
    "                    'success': False,\n",
    "                    'message': sol.message\n",
    "                })\n",
    "        except Exception as e:\n",
    "            trajectories.append({\n",
    "                'start': start_point,\n",
    "                'success': False,\n",
    "                'message': str(e)\n",
    "            })\n",
    "    \n",
    "    return trajectories\n",
    "\n",
    "def visualize_optimization_trajectories():\n",
    "    \"\"\"\n",
    "    Visualize optimization trajectories as integrated flow lines.\n",
    "    \"\"\"\n",
    "    print(\"\\nOPTIMIZATION TRAJECTORY ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Define starting points for different optimization runs\n",
    "    starting_points = [\n",
    "        np.array([-0.5, -0.5]),  # Bottom left\n",
    "        np.array([1.0, 1.0]),    # Top right\n",
    "        np.array([-0.5, 1.0]),   # Top left\n",
    "        np.array([1.0, -0.5]),   # Bottom right\n",
    "        np.array([0.0, 0.0]),    # Center\n",
    "        np.array([0.5, 0.5]),    # Off-center\n",
    "    ]\n",
    "    \n",
    "    # Integrate trajectories\n",
    "    print(\"Integrating flow lines...\")\n",
    "    trajectories = integrate_flow_lines(starting_points, gradient_vector_field, t_span=(0, 5))\n",
    "    \n",
    "    # Visualize trajectories\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Trajectories on loss surface\n",
    "    ax = axes[0]\n",
    "    \n",
    "    # Background: loss contours\n",
    "    ax.contourf(W1, W2, Loss, levels=20, cmap='viridis', alpha=0.6)\n",
    "    ax.contour(W1, W2, Loss, levels=15, colors='white', alpha=0.8, linewidths=0.5)\n",
    "    \n",
    "    # Plot trajectories\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    for i, (traj, color) in enumerate(zip(trajectories, colors)):\n",
    "        if traj['success']:\n",
    "            trajectory = traj['trajectory']\n",
    "            start = traj['start']\n",
    "            \n",
    "            # Plot trajectory line\n",
    "            ax.plot(trajectory[:, 0], trajectory[:, 1], color=color, \n",
    "                   linewidth=2, alpha=0.8, label=f'Start {i+1}')\n",
    "            \n",
    "            # Mark starting point\n",
    "            ax.plot(start[0], start[1], 'o', color=color, markersize=8, \n",
    "                   markeredgecolor='black', markeredgewidth=1)\n",
    "            \n",
    "            # Mark ending point\n",
    "            ax.plot(trajectory[-1, 0], trajectory[-1, 1], 's', color=color, \n",
    "                   markersize=8, markeredgecolor='black', markeredgewidth=1)\n",
    "            \n",
    "            print(f\"Trajectory {i+1}: {start} → {trajectory[-1]}\")\n",
    "        else:\n",
    "            print(f\"Trajectory {i+1}: Failed - {traj.get('message', 'Unknown error')}\")\n",
    "    \n",
    "    ax.set_xlabel('w₁ (word_count weight)')\n",
    "    ax.set_ylabel('w₂ (has_team weight)')\n",
    "    ax.set_title('Optimization Trajectories\\n(Circles = start, Squares = end)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot 2: Loss evolution along trajectories\n",
    "    ax = axes[1]\n",
    "    \n",
    "    for i, (traj, color) in enumerate(zip(trajectories, colors)):\n",
    "        if traj['success']:\n",
    "            trajectory = traj['trajectory']\n",
    "            times = traj['times']\n",
    "            \n",
    "            # Compute loss along trajectory\n",
    "            losses = []\n",
    "            for point_2d in trajectory:\n",
    "                point_3d = np.array([point_2d[0], point_2d[1], 0.4])\n",
    "                loss_val = loss_function(point_3d)\n",
    "                losses.append(loss_val)\n",
    "            \n",
    "            ax.plot(times, losses, color=color, linewidth=2, label=f'Start {i+1}')\n",
    "    \n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Loss Value')\n",
    "    ax.set_title('Loss Evolution Along Trajectories')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return trajectories\n",
    "\n",
    "# Visualize optimization trajectories\n",
    "optimization_trajectories = visualize_optimization_trajectories()\n",
    "\n",
    "print(\"\\n✅ Flow line integration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Optimizer Vector Fields\n",
    "\n",
    "Different optimizers create different vector fields. Let's compare SGD, momentum, and adaptive optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement different optimizer vector fields\n",
    "class OptimizerVectorField:\n",
    "    \"\"\"\n",
    "    Base class for optimizer vector fields.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, grad_fn):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.grad_fn = grad_fn\n",
    "        self.reset_state()\n",
    "    \n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset optimizer state\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def field(self, weights):\n",
    "        \"\"\"Compute vector field at given point\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGDField(OptimizerVectorField):\n",
    "    \"\"\"Standard gradient descent: F(w) = -∇L(w)\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, grad_fn, lr=0.1):\n",
    "        super().__init__(loss_fn, grad_fn)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def field(self, weights):\n",
    "        return -self.lr * self.grad_fn(weights)\n",
    "\n",
    "class MomentumField(OptimizerVectorField):\n",
    "    \"\"\"Momentum: F(w,v) = -∇L(w) + βv\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, grad_fn, lr=0.1, momentum=0.9):\n",
    "        super().__init__(loss_fn, grad_fn)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.velocity = None\n",
    "    \n",
    "    def field(self, weights):\n",
    "        grad = self.grad_fn(weights)\n",
    "        \n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(weights)\n",
    "        \n",
    "        # Update velocity: v = βv - α∇L\n",
    "        self.velocity = self.momentum * self.velocity - self.lr * grad\n",
    "        \n",
    "        return self.velocity\n",
    "\n",
    "class AdamField(OptimizerVectorField):\n",
    "    \"\"\"Adam: F(w) = -α * m̂/(√v̂ + ε)\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, grad_fn, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(loss_fn, grad_fn)\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.reset_state()\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.m = None  # First moment\n",
    "        self.v = None  # Second moment\n",
    "        self.t = 0     # Time step\n",
    "    \n",
    "    def field(self, weights):\n",
    "        grad = self.grad_fn(weights)\n",
    "        self.t += 1\n",
    "        \n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(weights)\n",
    "            self.v = np.zeros_like(weights)\n",
    "        \n",
    "        # Update moments\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * grad**2\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "        \n",
    "        # Adam update\n",
    "        return -self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "class RMSpropField(OptimizerVectorField):\n",
    "    \"\"\"RMSprop: F(w) = -α * ∇L/(√v + ε)\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, grad_fn, lr=0.01, alpha=0.99, eps=1e-8):\n",
    "        super().__init__(loss_fn, grad_fn)\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.reset_state()\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.v = None\n",
    "    \n",
    "    def field(self, weights):\n",
    "        grad = self.grad_fn(weights)\n",
    "        \n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(weights)\n",
    "        \n",
    "        # Update running average of squared gradients\n",
    "        self.v = self.alpha * self.v + (1 - self.alpha) * grad**2\n",
    "        \n",
    "        # RMSprop update\n",
    "        return -self.lr * grad / (np.sqrt(self.v) + self.eps)\n",
    "\n",
    "# Create optimizer vector fields\n",
    "print(\"\\nCREATING OPTIMIZER VECTOR FIELDS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "optimizers = {\n",
    "    'SGD': SGDField(loss_function, gradient_function, lr=0.5),\n",
    "    'Momentum': MomentumField(loss_function, gradient_function, lr=0.1, momentum=0.9),\n",
    "    'Adam': AdamField(loss_function, gradient_function, lr=0.1),\n",
    "    'RMSprop': RMSpropField(loss_function, gradient_function, lr=0.1)\n",
    "}\n",
    "\n",
    "# Test each optimizer at a test point\n",
    "test_point = np.array([0.5, 0.5, 0.4])\n",
    "print(f\"\\nTesting optimizers at w = {test_point}:\")\n",
    "\n",
    "for name, optimizer in optimizers.items():\n",
    "    optimizer.reset_state()\n",
    "    field_value = optimizer.field(test_point)\n",
    "    field_magnitude = np.linalg.norm(field_value)\n",
    "    \n",
    "    print(f\"{name:<10}: F(w) = {field_value}, |F| = {field_magnitude:.6f}\")\n",
    "\n",
    "print(\"\\n✅ Optimizer vector fields created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare optimizer vector fields visually\n",
    "def compare_optimizer_fields():\n",
    "    \"\"\"\n",
    "    Visualize and compare different optimizer vector fields.\n",
    "    \"\"\"\n",
    "    print(\"\\nCOMPARING OPTIMIZER VECTOR FIELDS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create grid for visualization\n",
    "    resolution = 10\n",
    "    w1_range, w2_range = (-1.0, 1.5), (-1.0, 1.5)\n",
    "    w1_vals = np.linspace(w1_range[0], w1_range[1], resolution)\n",
    "    w2_vals = np.linspace(w2_range[0], w2_range[1], resolution)\n",
    "    W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    optimizer_names = ['SGD', 'Momentum', 'Adam', 'RMSprop']\n",
    "    \n",
    "    for idx, name in enumerate(optimizer_names):\n",
    "        ax = axes[idx]\n",
    "        optimizer = optimizers[name]\n",
    "        optimizer.reset_state()\n",
    "        \n",
    "        # Compute vector field\n",
    "        U = np.zeros_like(W1)\n",
    "        V = np.zeros_like(W1)\n",
    "        Magnitude = np.zeros_like(W1)\n",
    "        \n",
    "        for i in range(resolution):\n",
    "            for j in range(resolution):\n",
    "                w = np.array([W1[i, j], W2[i, j], 0.4])\n",
    "                field = optimizer.field(w)\n",
    "                \n",
    "                U[i, j] = field[0]\n",
    "                V[i, j] = field[1]\n",
    "                Magnitude[i, j] = np.linalg.norm(field[:2])\n",
    "        \n",
    "        # Plot background loss contours\n",
    "        ax.contourf(W1, W2, Loss, levels=15, cmap='viridis', alpha=0.3)\n",
    "        \n",
    "        # Plot vector field\n",
    "        ax.quiver(W1, W2, U, V, Magnitude, cmap='plasma', \n",
    "                 scale=20, width=0.004, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('w₁')\n",
    "        ax.set_ylabel('w₂')\n",
    "        ax.set_title(f'{name} Vector Field')\n",
    "        \n",
    "        # Add some analysis\n",
    "        avg_magnitude = np.mean(Magnitude)\n",
    "        max_magnitude = np.max(Magnitude)\n",
    "        print(f\"{name} field - Avg magnitude: {avg_magnitude:.4f}, Max: {max_magnitude:.4f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def simulate_optimizer_trajectories():\n",
    "    \"\"\"\n",
    "    Simulate and compare optimization trajectories for different optimizers.\n",
    "    \"\"\"\n",
    "    print(\"\\nSIMULATING OPTIMIZER TRAJECTORIES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Starting point\n",
    "    start_point = np.array([1.0, -0.5, 0.4])\n",
    "    num_steps = 50\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Simulate each optimizer\n",
    "    trajectories = {}\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    for i, (name, optimizer) in enumerate(optimizers.items()):\n",
    "        optimizer.reset_state()\n",
    "        \n",
    "        # Simulate trajectory\n",
    "        trajectory = [start_point.copy()]\n",
    "        losses = [loss_function(start_point)]\n",
    "        current_point = start_point.copy()\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            field_vector = optimizer.field(current_point)\n",
    "            current_point = current_point + field_vector\n",
    "            trajectory.append(current_point.copy())\n",
    "            losses.append(loss_function(current_point))\n",
    "        \n",
    "        trajectory = np.array(trajectory)\n",
    "        trajectories[name] = {'path': trajectory, 'losses': losses}\n",
    "        \n",
    "        # Plot trajectory (2D projection)\n",
    "        ax = axes[0]\n",
    "        ax.plot(trajectory[:, 0], trajectory[:, 1], color=colors[i], \n",
    "               linewidth=2, marker='o', markersize=3, label=name, alpha=0.8)\n",
    "        \n",
    "        # Mark start and end\n",
    "        ax.plot(start_point[0], start_point[1], 'o', color=colors[i], \n",
    "               markersize=10, markeredgecolor='black')\n",
    "        ax.plot(trajectory[-1, 0], trajectory[-1, 1], 's', color=colors[i], \n",
    "               markersize=8, markeredgecolor='black')\n",
    "        \n",
    "        # Plot loss evolution\n",
    "        ax = axes[1]\n",
    "        ax.plot(range(len(losses)), losses, color=colors[i], \n",
    "               linewidth=2, label=name, alpha=0.8)\n",
    "        \n",
    "        print(f\"{name}: Final point = {trajectory[-1]}, Final loss = {losses[-1]:.6f}\")\n",
    "    \n",
    "    # Customize plots\n",
    "    axes[0].contourf(W1, W2, Loss, levels=15, cmap='viridis', alpha=0.3)\n",
    "    axes[0].set_xlabel('w₁')\n",
    "    axes[0].set_ylabel('w₂')\n",
    "    axes[0].set_title('Optimizer Trajectories\\n(Circles = start, Squares = end)')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Loss Evolution by Optimizer')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return trajectories\n",
    "\n",
    "# Compare optimizer fields and trajectories\n",
    "compare_optimizer_fields()\n",
    "optimizer_trajectories = simulate_optimizer_trajectories()\n",
    "\n",
    "print(\"\\n✅ Optimizer comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Divergence and Curl Analysis\n",
    "\n",
    "Let's analyze the mathematical properties of our vector fields using divergence and curl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute divergence and curl of vector fields\n",
    "def compute_field_properties(field_function, w1_range=(-1, 1.5), w2_range=(-1, 1.5), \n",
    "                            fixed_w3=0.4, resolution=20):\n",
    "    \"\"\"\n",
    "    Compute divergence and curl of a 2D vector field slice.\n",
    "    \"\"\"\n",
    "    w1_vals = np.linspace(w1_range[0], w1_range[1], resolution)\n",
    "    w2_vals = np.linspace(w2_range[0], w2_range[1], resolution)\n",
    "    W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "    \n",
    "    h = w1_vals[1] - w1_vals[0]  # Grid spacing\n",
    "    \n",
    "    # Compute vector field components\n",
    "    U = np.zeros_like(W1)\n",
    "    V = np.zeros_like(W1)\n",
    "    \n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            w = np.array([W1[i, j], W2[i, j], fixed_w3])\n",
    "            field = field_function(w)\n",
    "            U[i, j] = field[0]\n",
    "            V[i, j] = field[1]\n",
    "    \n",
    "    # Compute divergence: ∇·F = ∂U/∂w1 + ∂V/∂w2\n",
    "    dU_dw1 = np.gradient(U, h, axis=1)\n",
    "    dV_dw2 = np.gradient(V, h, axis=0)\n",
    "    divergence = dU_dw1 + dV_dw2\n",
    "    \n",
    "    # Compute curl (2D): ∇×F = ∂V/∂w1 - ∂U/∂w2\n",
    "    dV_dw1 = np.gradient(V, h, axis=1)\n",
    "    dU_dw2 = np.gradient(U, h, axis=0)\n",
    "    curl = dV_dw1 - dU_dw2\n",
    "    \n",
    "    return W1, W2, U, V, divergence, curl\n",
    "\n",
    "def analyze_vector_field_properties():\n",
    "    \"\"\"\n",
    "    Analyze divergence and curl properties of different optimizers.\n",
    "    \"\"\"\n",
    "    print(\"\\nVECTOR FIELD MATHEMATICAL ANALYSIS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    optimizer_names = ['SGD', 'Momentum', 'Adam', 'RMSprop']\n",
    "    properties = {}\n",
    "    \n",
    "    for idx, name in enumerate(optimizer_names):\n",
    "        optimizer = optimizers[name]\n",
    "        optimizer.reset_state()\n",
    "        \n",
    "        # Compute field properties\n",
    "        W1, W2, U, V, div, curl = compute_field_properties(optimizer.field)\n",
    "        \n",
    "        properties[name] = {\n",
    "            'divergence': div,\n",
    "            'curl': curl,\n",
    "            'field_magnitude': np.sqrt(U**2 + V**2)\n",
    "        }\n",
    "        \n",
    "        # Plot divergence\n",
    "        ax = axes[0, idx]\n",
    "        div_max = np.max(np.abs(div))\n",
    "        div_plot = ax.contourf(W1, W2, div, levels=20, cmap='RdBu_r', \n",
    "                              vmin=-div_max, vmax=div_max)\n",
    "        ax.contour(W1, W2, div, levels=10, colors='black', alpha=0.3, linewidths=0.5)\n",
    "        plt.colorbar(div_plot, ax=ax, label='Divergence')\n",
    "        ax.set_title(f'{name} Divergence\\n∇·F')\n",
    "        ax.set_xlabel('w₁')\n",
    "        ax.set_ylabel('w₂')\n",
    "        \n",
    "        # Plot curl\n",
    "        ax = axes[1, idx]\n",
    "        curl_max = np.max(np.abs(curl))\n",
    "        curl_plot = ax.contourf(W1, W2, curl, levels=20, cmap='RdBu_r', \n",
    "                               vmin=-curl_max, vmax=curl_max)\n",
    "        ax.contour(W1, W2, curl, levels=10, colors='black', alpha=0.3, linewidths=0.5)\n",
    "        plt.colorbar(curl_plot, ax=ax, label='Curl')\n",
    "        ax.set_title(f'{name} Curl\\n∇×F')\n",
    "        ax.set_xlabel('w₁')\n",
    "        ax.set_ylabel('w₂')\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\n{name} Field Properties:\")\n",
    "        print(f\"  Divergence - Mean: {np.mean(div):.6f}, Std: {np.std(div):.6f}\")\n",
    "        print(f\"  Curl - Mean: {np.mean(curl):.6f}, Std: {np.std(curl):.6f}\")\n",
    "        print(f\"  Max |divergence|: {div_max:.6f}\")\n",
    "        print(f\"  Max |curl|: {curl_max:.6f}\")\n",
    "        \n",
    "        # Interpret the results\n",
    "        if np.mean(div) > 0.01:\n",
    "            print(f\"  → Predominantly expanding flow (sources)\")\n",
    "        elif np.mean(div) < -0.01:\n",
    "            print(f\"  → Predominantly contracting flow (sinks)\")\n",
    "        else:\n",
    "            print(f\"  → Nearly incompressible flow\")\n",
    "        \n",
    "        if np.std(curl) > 0.01:\n",
    "            print(f\"  → Significant rotational components\")\n",
    "        else:\n",
    "            print(f\"  → Mostly irrotational flow\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return properties\n",
    "\n",
    "# Analyze vector field properties\n",
    "field_properties = analyze_vector_field_properties()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MATHEMATICAL INTERPRETATION\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n• DIVERGENCE (∇·F):\")\n",
    "print(\"  - Positive: Flow expansion (sources)\")\n",
    "print(\"  - Negative: Flow contraction (sinks)\")\n",
    "print(\"  - Zero: Incompressible flow\")\n",
    "print(\"\\n• CURL (∇×F):\")\n",
    "print(\"  - Positive: Counterclockwise rotation\")\n",
    "print(\"  - Negative: Clockwise rotation\")\n",
    "print(\"  - Zero: Irrotational flow\")\n",
    "print(\"\\n• OPTIMIZATION IMPLICATIONS:\")\n",
    "print(\"  - High divergence: Flow focusing/dispersing\")\n",
    "print(\"  - High curl: Potential oscillatory behavior\")\n",
    "print(\"  - Low curl: Direct convergence to minima\")\n",
    "\n",
    "print(\"\\n✅ Divergence and curl analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Dynamical Systems Theory\n",
    "\n",
    "Finally, let's apply dynamical systems theory to understand fixed points, stability, and long-term behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply dynamical systems theory to optimization\n",
    "def analyze_fixed_points_stability():\n",
    "    \"\"\"\n",
    "    Analyze fixed points and their stability using dynamical systems theory.\n",
    "    \"\"\"\n",
    "    print(\"\\nDYNAMICAL SYSTEMS ANALYSIS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # For gradient descent, fixed points are where ∇L(w) = 0\n",
    "    # Let's find approximate fixed points\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    # Find critical points (fixed points of gradient field)\n",
    "    starting_points = [\n",
    "        np.array([0.0, 0.0, 0.0]),\n",
    "        np.array([0.5, 0.5, 0.5]),\n",
    "        np.array([1.0, 1.0, 1.0]),\n",
    "        np.array([-0.5, -0.5, -0.5])\n",
    "    ]\n",
    "    \n",
    "    fixed_points = []\n",
    "    \n",
    "    for start in starting_points:\n",
    "        try:\n",
    "            result = minimize(loss_function, start, jac=gradient_function, \n",
    "                            method='BFGS', options={'gtol': 1e-8})\n",
    "            \n",
    "            if result.success:\n",
    "                candidate = result.x\n",
    "                grad_norm = np.linalg.norm(gradient_function(candidate))\n",
    "                \n",
    "                if grad_norm < 1e-6:\n",
    "                    # Check if we already found this point\n",
    "                    is_new = True\n",
    "                    for existing in fixed_points:\n",
    "                        if np.linalg.norm(candidate - existing['point']) < 1e-4:\n",
    "                            is_new = False\n",
    "                            break\n",
    "                    \n",
    "                    if is_new:\n",
    "                        fixed_points.append({\n",
    "                            'point': candidate,\n",
    "                            'loss': loss_function(candidate),\n",
    "                            'gradient_norm': grad_norm\n",
    "                        })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Found {len(fixed_points)} fixed point(s):\")\n",
    "    \n",
    "    for i, fp in enumerate(fixed_points):\n",
    "        point = fp['point']\n",
    "        loss_val = fp['loss']\n",
    "        \n",
    "        print(f\"\\nFixed Point {i+1}:\")\n",
    "        print(f\"  Location: {point}\")\n",
    "        print(f\"  Loss: {loss_val:.8f}\")\n",
    "        print(f\"  Gradient norm: {fp['gradient_norm']:.2e}\")\n",
    "        \n",
    "        # Analyze stability using Hessian eigenvalues\n",
    "        hessian = compute_hessian_numerical(point)\n",
    "        eigenvalues = np.linalg.eigvals(hessian)\n",
    "        \n",
    "        print(f\"  Hessian eigenvalues: {eigenvalues}\")\n",
    "        \n",
    "        # Stability classification\n",
    "        if np.all(eigenvalues > 1e-8):\n",
    "            stability = \"Stable (local minimum)\"\n",
    "        elif np.all(eigenvalues < -1e-8):\n",
    "            stability = \"Unstable (local maximum)\"\n",
    "        elif np.any(eigenvalues > 1e-8) and np.any(eigenvalues < -1e-8):\n",
    "            stability = \"Saddle point (unstable)\"\n",
    "        else:\n",
    "            stability = \"Marginal stability\"\n",
    "        \n",
    "        print(f\"  Stability: {stability}\")\n",
    "    \n",
    "    return fixed_points\n",
    "\n",
    "def compute_hessian_numerical(weights, h=1e-6):\n",
    "    \"\"\"Compute Hessian matrix numerically\"\"\"\n",
    "    n = len(weights)\n",
    "    hessian = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Compute ∂²f/∂wi∂wj using finite differences\n",
    "            w_pp = weights.copy()\n",
    "            w_pm = weights.copy()\n",
    "            w_mp = weights.copy()\n",
    "            w_mm = weights.copy()\n",
    "            \n",
    "            w_pp[i] += h\n",
    "            w_pp[j] += h\n",
    "            \n",
    "            w_pm[i] += h\n",
    "            w_pm[j] -= h\n",
    "            \n",
    "            w_mp[i] -= h\n",
    "            w_mp[j] += h\n",
    "            \n",
    "            w_mm[i] -= h\n",
    "            w_mm[j] -= h\n",
    "            \n",
    "            hessian[i, j] = (loss_function(w_pp) - loss_function(w_pm) - \n",
    "                           loss_function(w_mp) + loss_function(w_mm)) / (4 * h**2)\n",
    "    \n",
    "    return hessian\n",
    "\n",
    "def analyze_basins_of_attraction(fixed_points):\n",
    "    \"\"\"\n",
    "    Analyze basins of attraction for different fixed points.\n",
    "    \"\"\"\n",
    "    if not fixed_points:\n",
    "        print(\"No fixed points found for basin analysis.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nBASIN OF ATTRACTION ANALYSIS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create grid of starting points\n",
    "    resolution = 15\n",
    "    w1_range, w2_range = (-1.5, 1.5), (-1.5, 1.5)\n",
    "    w1_vals = np.linspace(w1_range[0], w1_range[1], resolution)\n",
    "    w2_vals = np.linspace(w2_range[0], w2_range[1], resolution)\n",
    "    W1, W2 = np.meshgrid(w1_vals, w2_vals)\n",
    "    \n",
    "    # For each starting point, see which fixed point it converges to\n",
    "    basin_map = np.zeros_like(W1, dtype=int)\n",
    "    convergence_time = np.zeros_like(W1)\n",
    "    \n",
    "    print(\"Computing basins of attraction...\")\n",
    "    \n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            start_2d = np.array([W1[i, j], W2[i, j]])\n",
    "            start_3d = np.array([W1[i, j], W2[i, j], 0.4])\n",
    "            \n",
    "            # Simulate gradient descent\n",
    "            current_point = start_3d.copy()\n",
    "            max_steps = 100\n",
    "            lr = 0.1\n",
    "            tolerance = 1e-4\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                grad = gradient_function(current_point)\n",
    "                if np.linalg.norm(grad) < tolerance:\n",
    "                    break\n",
    "                current_point -= lr * grad\n",
    "            \n",
    "            # Find which fixed point this converged to\n",
    "            min_distance = float('inf')\n",
    "            closest_fp = 0\n",
    "            \n",
    "            for fp_idx, fp in enumerate(fixed_points):\n",
    "                distance = np.linalg.norm(current_point - fp['point'])\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    closest_fp = fp_idx\n",
    "            \n",
    "            basin_map[i, j] = closest_fp\n",
    "            convergence_time[i, j] = step\n",
    "    \n",
    "    # Visualize basins\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Basin map\n",
    "    ax = axes[0]\n",
    "    basin_plot = ax.contourf(W1, W2, basin_map, levels=len(fixed_points), \n",
    "                            cmap='tab10', alpha=0.7)\n",
    "    \n",
    "    # Mark fixed points\n",
    "    for i, fp in enumerate(fixed_points):\n",
    "        ax.plot(fp['point'][0], fp['point'][1], 'o', color='black', \n",
    "               markersize=12, markeredgewidth=2, markerfacecolor='white')\n",
    "        ax.text(fp['point'][0], fp['point'][1], str(i+1), \n",
    "               ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('w₁')\n",
    "    ax.set_ylabel('w₂')\n",
    "    ax.set_title('Basins of Attraction\\n(Numbers = Fixed Points)')\n",
    "    \n",
    "    # Plot 2: Convergence time\n",
    "    ax = axes[1]\n",
    "    time_plot = ax.contourf(W1, W2, convergence_time, levels=20, cmap='viridis')\n",
    "    plt.colorbar(time_plot, ax=ax, label='Steps to Converge')\n",
    "    ax.set_xlabel('w₁')\n",
    "    ax.set_ylabel('w₂')\n",
    "    ax.set_title('Convergence Time Map')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nBasin Analysis Summary:\")\n",
    "    for i, fp in enumerate(fixed_points):\n",
    "        basin_size = np.sum(basin_map == i)\n",
    "        basin_percentage = basin_size / basin_map.size * 100\n",
    "        avg_convergence = np.mean(convergence_time[basin_map == i])\n",
    "        \n",
    "        print(f\"Fixed Point {i+1}: {basin_percentage:.1f}% of space, \"\n",
    "              f\"avg convergence: {avg_convergence:.1f} steps\")\n",
    "\n",
    "# Perform dynamical systems analysis\n",
    "fixed_points = analyze_fixed_points_stability()\n",
    "analyze_basins_of_attraction(fixed_points)\n",
    "\n",
    "print(\"\\n✅ Dynamical systems analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Summarize vector field insights\n",
    "def summarize_vector_field_insights():\n",
    "    \"\"\"\n",
    "    Summarize key insights from vector field analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VECTOR FIELD ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n🌊 WHAT WE DISCOVERED:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    print(\"\\n1. OPTIMIZATION AS FLOW:\")\n",
    "    print(\"   • Gradient descent = flow in vector field F(w) = -∇L(w)\")\n",
    "    print(\"   • Different optimizers create different vector fields\")\n",
    "    print(\"   • Flow lines reveal optimization paths and convergence\")\n",
    "    \n",
    "    print(\"\\n2. OPTIMIZER DIFFERENCES:\")\n",
    "    print(\"   • SGD: Direct gradient flow, can be slow\")\n",
    "    print(\"   • Momentum: Smoothed flow with inertia, faster convergence\")\n",
    "    print(\"   • Adam: Adaptive flow scaling, good for varied landscapes\")\n",
    "    print(\"   • RMSprop: Normalized flow, handles different scales\")\n",
    "    \n",
    "    print(\"\\n3. MATHEMATICAL PROPERTIES:\")\n",
    "    print(\"   • Divergence reveals flow expansion/contraction\")\n",
    "    print(\"   • Curl indicates rotational behavior\")\n",
    "    print(\"   • Fixed points correspond to critical points\")\n",
    "    print(\"   • Eigenvalues determine local stability\")\n",
    "    \n",
    "    print(\"\\n4. DYNAMICAL BEHAVIOR:\")\n",
    "    print(\"   • Basins of attraction show convergence regions\")\n",
    "    print(\"   • Stability analysis predicts long-term behavior\")\n",
    "    print(\"   • Flow patterns explain optimization efficiency\")\n",
    "    \n",
    "    print(\"\\n🧮 MATHEMATICAL SIGNIFICANCE:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\n• VECTOR FIELDS unify optimization and dynamical systems\")\n",
    "    print(\"• FLOW INTEGRATION shows actual optimization paths\")\n",
    "    print(\"• DIVERGENCE/CURL reveal geometric properties\")\n",
    "    print(\"• STABILITY THEORY predicts convergence behavior\")\n",
    "    \n",
    "    print(\"\\n🎯 PRACTICAL IMPLICATIONS:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    print(\"\\n1. OPTIMIZER SELECTION:\")\n",
    "    print(\"   → Vector field analysis reveals why certain optimizers work better\")\n",
    "    print(\"   → Flow patterns predict convergence speed and stability\")\n",
    "    \n",
    "    print(\"\\n2. HYPERPARAMETER TUNING:\")\n",
    "    print(\"   → Learning rate affects flow magnitude and stability\")\n",
    "    print(\"   → Momentum parameters change flow smoothness\")\n",
    "    \n",
    "    print(\"\\n3. ARCHITECTURE DESIGN:\")\n",
    "    print(\"   → Loss landscape topology affects vector field properties\")\n",
    "    print(\"   → Network design influences optimization dynamics\")\n",
    "    \n",
    "    print(\"\\n4. CONVERGENCE PREDICTION:\")\n",
    "    print(\"   → Basin analysis shows which initializations lead where\")\n",
    "    print(\"   → Stability analysis predicts robustness to perturbations\")\n",
    "    \n",
    "    print(\"\\n🚀 CONNECTION TO MODERN AI:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    print(\"\\n• NEURAL ODE: Explicit continuous optimization flows\")\n",
    "    print(\"• ADVERSARIAL TRAINING: Understanding optimization conflicts\")\n",
    "    print(\"• META-LEARNING: Optimizing the optimization process\")\n",
    "    print(\"• DISTRIBUTED OPTIMIZATION: Coordinated multi-agent flows\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Vector field theory reveals optimization as a beautiful\")\n",
    "    print(\"mathematical flow process, connecting AI to the deepest\")\n",
    "    print(\"principles of dynamical systems and differential geometry!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "summarize_vector_field_insights()\n",
    "print(\"\\n✅ Vector field analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You've now mastered vector field analysis - the mathematical framework that reveals optimization as dynamic flow! Here's what we discovered:\n",
    "\n",
    "**🔑 Key Mathematical Insights:**\n",
    "1. **Optimization as Flow** - Gradient descent is flow in the vector field F(w) = -∇L(w)\n",
    "2. **Optimizer Differences** - Each optimizer creates a unique vector field with distinct properties\n",
    "3. **Divergence and Curl** - Reveal expansion/contraction and rotational behavior\n",
    "4. **Dynamical Systems Theory** - Fixed points, stability, and basins of attraction\n",
    "\n",
    "**🧮 Mathematical Tools Mastered:**\n",
    "- **Vector field theory** for continuous optimization analysis\n",
    "- **Flow line integration** for trajectory prediction\n",
    "- **Divergence and curl** for geometric property analysis\n",
    "- **Stability theory** for convergence prediction\n",
    "\n",
    "**🎯 Why This Matters:**\n",
    "Vector field analysis transforms our understanding of optimization:\n",
    "- **Optimizer Selection**: Understanding why certain optimizers work better\n",
    "- **Hyperparameter Tuning**: Predicting the effects of learning rates and momentum\n",
    "- **Convergence Prediction**: Knowing which initializations lead to which solutions\n",
    "- **Robustness Analysis**: Understanding stability to perturbations\n",
    "\n",
    "**🚀 Coming in Problem 5: Optimization Landscapes**\n",
    "- How do we analyze the complete topology of loss landscapes?\n",
    "- What are the mathematical principles governing convergence guarantees?\n",
    "- How do modern optimization techniques escape local minima?\n",
    "- What can advanced analysis tell us about deep learning theory?\n",
    "\n",
    "You're about to complete the deepest mathematical journey through AI! 🐬➡️📊➡️🎯➡️⚡➡️🚀➡️🧮➡️🔗➡️📐➡️🌊➡️🏔️"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}