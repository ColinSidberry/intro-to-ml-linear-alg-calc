{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Jacobian Analysis - Understanding System Sensitivity\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this problem, you will:\n",
    "- Understand Jacobian matrices as complete sensitivity analysis tools\n",
    "- Analyze how input perturbations propagate through neural networks\n",
    "- Connect Jacobian eigenvalues to optimization stability and conditioning\n",
    "- Apply Jacobian analysis to understand model robustness and interpretability\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "1. **Jacobian Fundamentals** - From single derivatives to matrix of all partial derivatives\n",
    "2. **Network Jacobian Computation** - Sensitivity analysis for \"Go Dolphins!\" classifier\n",
    "3. **Eigenvalue Analysis** - Understanding conditioning and stability through spectra\n",
    "4. **Robustness Assessment** - How sensitive are predictions to input changes?\n",
    "\n",
    "---\n",
    "\n",
    "## From Gradients to Jacobians\n",
    "\n",
    "In Problems 1-2, you analyzed gradients - derivatives that show how outputs change with respect to parameters. But machine learning systems involve complex transformations with multiple inputs and outputs:\n",
    "\n",
    "**Single Output (Gradient)**:\n",
    "```\n",
    "\"Go Dolphins!\" → [features] → [weights] → prediction\n",
    "∇L(w) = [∂L/∂w₁, ∂L/∂w₂, ∂L/∂w₃]\n",
    "```\n",
    "\n",
    "**Multiple Outputs (Jacobian)**:\n",
    "```\n",
    "\"Go Dolphins!\" → [features] → [hidden activations] → [outputs]\n",
    "J = [∂yᵢ/∂xⱼ] for all input-output pairs\n",
    "```\n",
    "\n",
    "**The Challenge**: How do we understand the complete sensitivity structure of neural networks? How do small changes in inputs (or parameters) affect all aspects of the network's behavior?\n",
    "\n",
    "**The Solution**: **Jacobian matrices** - complete maps of how every output depends on every input.\n",
    "\n",
    "## Mathematical Foundation: The Jacobian Matrix\n",
    "\n",
    "**Definition**: For function $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, the Jacobian is:\n",
    "\n",
    "$$\\mathbf{J} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Key Properties**:\n",
    "- **Size**: $m \\times n$ matrix (outputs × inputs)\n",
    "- **Linear approximation**: $\\mathbf{f}(\\mathbf{x} + \\boldsymbol{\\epsilon}) \\approx \\mathbf{f}(\\mathbf{x}) + \\mathbf{J}\\boldsymbol{\\epsilon}$\n",
    "- **Chain rule**: $\\mathbf{J}_{f \\circ g} = \\mathbf{J}_f \\mathbf{J}_g$ (composition of transformations)\n",
    "- **Eigenvalues**: Reveal stability and conditioning properties\n",
    "\n",
    "**Applications in ML**:\n",
    "1. **Sensitivity Analysis**: How robust are predictions to input noise?\n",
    "2. **Feature Importance**: Which inputs most affect which outputs?\n",
    "3. **Optimization Conditioning**: Is the loss landscape well-behaved?\n",
    "4. **Adversarial Robustness**: Can small input changes fool the model?\n",
    "\n",
    "## Why Jacobians Matter for \"Go Dolphins!\"\n",
    "\n",
    "Understanding the Jacobian of our sentiment classifier reveals:\n",
    "- **Input sensitivity**: How much does each word feature affect the prediction?\n",
    "- **Layer interactions**: How do hidden representations depend on inputs?\n",
    "- **Robustness**: Would small changes in tweet features change the classification?\n",
    "- **Interpretability**: Which aspects of the input drive the decision?\n",
    "\n",
    "Let's dive into the mathematics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Jacobian analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd, eigvals\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our utilities\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from data_generators import load_sports_dataset\n",
    "\n",
    "# Load our \"Go Dolphins!\" dataset\n",
    "features, labels, feature_names, texts = load_sports_dataset()\n",
    "\n",
    "print(\"JACOBIAN ANALYSIS OF 'GO DOLPHINS!' SENTIMENT CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: {len(texts)} sports tweets\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print()\n",
    "print(\"Jacobian matrices we'll compute:\")\n",
    "print(\"• ∂(output)/∂(input): How predictions depend on tweet features\")\n",
    "print(\"• ∂(hidden)/∂(input): How internal representations depend on inputs\")\n",
    "print(\"• ∂(output)/∂(weights): How predictions depend on parameters\")\n",
    "print(\"• ∂(loss)/∂(weights): Complete parameter sensitivity (gradient)\")\n",
    "print()\n",
    "print(\"Analysis focus: Sensitivity, robustness, and conditioning\")\n",
    "\n",
    "# Define our network architecture for Jacobian analysis\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation with numerical stability\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class JacobianAnalysisNetwork:\n",
    "    \"\"\"Network designed for comprehensive Jacobian analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        \n",
    "        # Initialize weights with careful scaling\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Xavier initialization\n",
    "            fan_in, fan_out = layer_sizes[i], layer_sizes[i+1]\n",
    "            scale = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "            \n",
    "            w = np.random.normal(0, scale, (fan_in, fan_out))\n",
    "            b = np.zeros(fan_out)\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        print(f\"Initialized network: {layer_sizes}\")\n",
    "        print(f\"Total parameters: {sum(w.size + b.size for w, b in zip(self.weights, self.biases))}\")\n",
    "    \n",
    "    def forward_detailed(self, x):\n",
    "        \"\"\"Forward pass storing all intermediate values for Jacobian computation.\"\"\"\n",
    "        activations = [x]  # a^(0) = x\n",
    "        z_values = []      # z^(l) = W^(l)^T a^(l-1) + b^(l)\n",
    "        \n",
    "        current_activation = x\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Linear transformation\n",
    "            z = np.dot(current_activation, self.weights[i]) + self.biases[i]\n",
    "            z_values.append(z)\n",
    "            \n",
    "            # Nonlinear activation\n",
    "            a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "            \n",
    "            current_activation = a\n",
    "        \n",
    "        return {\n",
    "            'output': current_activation,\n",
    "            'activations': activations,\n",
    "            'z_values': z_values\n",
    "        }\n",
    "\n",
    "# Create a test network\n",
    "net = JacobianAnalysisNetwork([3, 4, 2, 1])  # Input → Hidden → Hidden → Output\n",
    "\n",
    "# Test forward pass\n",
    "test_input = features[0]  # \"Go Dolphins!\" features\n",
    "result = net.forward_detailed(test_input)\n",
    "\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Output: {result['output']:.6f}\")\n",
    "print(f\"Hidden layer activations:\")\n",
    "for i, activation in enumerate(result['activations'][1:-1], 1):\n",
    "    print(f\"  Layer {i}: {activation}\")\n",
    "\n",
    "print(\"\\n✅ Jacobian analysis network ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Jacobian Fundamentals\n",
    "\n",
    "Let's start by computing basic Jacobian matrices to understand the mathematical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement basic Jacobian computation\n",
    "def compute_output_input_jacobian(network, x, h=1e-8):\n",
    "    \"\"\"\n",
    "    Compute Jacobian matrix ∂(output)/∂(input) using finite differences.\n",
    "    \n",
    "    Returns:\n",
    "        jacobian: Matrix of size (output_dim, input_dim)\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    input_dim = len(x)\n",
    "    \n",
    "    # Get baseline output\n",
    "    baseline = network.forward_detailed(x)['output']\n",
    "    output_dim = 1 if np.isscalar(baseline) else len(baseline)\n",
    "    \n",
    "    # Initialize Jacobian\n",
    "    jacobian = np.zeros((output_dim, input_dim))\n",
    "    \n",
    "    # Compute partial derivatives\n",
    "    for i in range(input_dim):\n",
    "        # Perturb input\n",
    "        x_plus = x.copy()\n",
    "        x_minus = x.copy()\n",
    "        x_plus[i] += h\n",
    "        x_minus[i] -= h\n",
    "        \n",
    "        # Compute outputs\n",
    "        output_plus = network.forward_detailed(x_plus)['output']\n",
    "        output_minus = network.forward_detailed(x_minus)['output']\n",
    "        \n",
    "        # Finite difference\n",
    "        if output_dim == 1:\n",
    "            jacobian[0, i] = (output_plus - output_minus) / (2 * h)\n",
    "        else:\n",
    "            jacobian[:, i] = (output_plus - output_minus) / (2 * h)\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "def compute_hidden_input_jacobian(network, x, layer_idx, h=1e-8):\n",
    "    \"\"\"\n",
    "    Compute Jacobian matrix ∂(hidden_layer)/∂(input).\n",
    "    \"\"\"\n",
    "    input_dim = len(x)\n",
    "    \n",
    "    # Get baseline hidden activations\n",
    "    baseline_result = network.forward_detailed(x)\n",
    "    baseline_hidden = baseline_result['activations'][layer_idx + 1]  # +1 because activations[0] is input\n",
    "    hidden_dim = len(baseline_hidden) if hasattr(baseline_hidden, '__len__') else 1\n",
    "    \n",
    "    # Initialize Jacobian\n",
    "    jacobian = np.zeros((hidden_dim, input_dim))\n",
    "    \n",
    "    # Compute partial derivatives\n",
    "    for i in range(input_dim):\n",
    "        x_plus = x.copy()\n",
    "        x_minus = x.copy()\n",
    "        x_plus[i] += h\n",
    "        x_minus[i] -= h\n",
    "        \n",
    "        hidden_plus = network.forward_detailed(x_plus)['activations'][layer_idx + 1]\n",
    "        hidden_minus = network.forward_detailed(x_minus)['activations'][layer_idx + 1]\n",
    "        \n",
    "        if hidden_dim == 1:\n",
    "            jacobian[0, i] = (hidden_plus - hidden_minus) / (2 * h)\n",
    "        else:\n",
    "            jacobian[:, i] = (hidden_plus - hidden_minus) / (2 * h)\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "print(\"COMPUTING JACOBIAN MATRICES FOR 'GO DOLPHINS!'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test input: \"Go Dolphins!\" features\n",
    "x = features[0]  # [2, 1, 1]\n",
    "print(f\"Input features: {x} ({feature_names})\")\n",
    "print(f\"Tweet: '{texts[0]}'\")\n",
    "print()\n",
    "\n",
    "# Compute output-input Jacobian\n",
    "print(\"1. Computing ∂(output)/∂(input) Jacobian...\")\n",
    "J_output_input = compute_output_input_jacobian(net, x)\n",
    "print(f\"Jacobian shape: {J_output_input.shape}\")\n",
    "print(f\"Jacobian matrix:\")\n",
    "print(J_output_input)\n",
    "print()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    sensitivity = J_output_input[0, i]\n",
    "    print(f\"  ∂(prediction)/∂({feature_name}) = {sensitivity:.6f}\")\n",
    "    if abs(sensitivity) > 0.1:\n",
    "        print(f\"    → High sensitivity! Small changes in {feature_name} strongly affect prediction\")\n",
    "    elif abs(sensitivity) > 0.01:\n",
    "        print(f\"    → Moderate sensitivity\")\n",
    "    else:\n",
    "        print(f\"    → Low sensitivity\")\n",
    "print()\n",
    "\n",
    "# Compute hidden layer Jacobians\n",
    "print(\"2. Computing ∂(hidden_layers)/∂(input) Jacobians...\")\n",
    "hidden_jacobians = []\n",
    "\n",
    "for layer_idx in range(net.num_layers - 1):  # Exclude output layer\n",
    "    print(f\"\\nHidden Layer {layer_idx + 1}:\")\n",
    "    J_hidden = compute_hidden_input_jacobian(net, x, layer_idx)\n",
    "    hidden_jacobians.append(J_hidden)\n",
    "    \n",
    "    print(f\"  Jacobian shape: {J_hidden.shape}\")\n",
    "    print(f\"  Jacobian matrix:\")\n",
    "    print(f\"  {J_hidden}\")\n",
    "    \n",
    "    # Analyze sensitivity patterns\n",
    "    max_sensitivity = np.max(np.abs(J_hidden))\n",
    "    print(f\"  Maximum sensitivity: {max_sensitivity:.6f}\")\n",
    "    \n",
    "    # Find most sensitive neuron-input pairs\n",
    "    if J_hidden.size > 1:\n",
    "        max_idx = np.unravel_index(np.argmax(np.abs(J_hidden)), J_hidden.shape)\n",
    "        neuron_idx, input_idx = max_idx\n",
    "        print(f\"  Most sensitive: Neuron {neuron_idx} to {feature_names[input_idx]}\")\n",
    "        print(f\"                  (∂h{neuron_idx}/∂{feature_names[input_idx]} = {J_hidden[neuron_idx, input_idx]:.6f})\")\n",
    "\n",
    "print(\"\\n✅ Basic Jacobian computation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze Jacobian properties and structure\n",
    "def analyze_jacobian_properties(jacobian, name=\"Jacobian\"):\n",
    "    \"\"\"\n",
    "    Analyze mathematical properties of a Jacobian matrix.\n",
    "    \"\"\"\n",
    "    print(f\"\\nANALYSIS: {name}\")\n",
    "    print(\"=\" * (10 + len(name)))\n",
    "    \n",
    "    print(f\"Shape: {jacobian.shape} (outputs × inputs)\")\n",
    "    print(f\"Rank: {np.linalg.matrix_rank(jacobian)}\")\n",
    "    \n",
    "    # Singular Value Decomposition\n",
    "    if jacobian.size > 1 and min(jacobian.shape) > 1:\n",
    "        U, s, Vt = svd(jacobian)\n",
    "        print(f\"Singular values: {s}\")\n",
    "        print(f\"Condition number: {s[0]/s[-1]:.2e}\")\n",
    "        \n",
    "        if s[0]/s[-1] > 1000:\n",
    "            print(\"⚠️  High condition number - poorly conditioned transformation\")\n",
    "        else:\n",
    "            print(\"✅ Well-conditioned transformation\")\n",
    "    \n",
    "    # Frobenius norm (overall sensitivity)\n",
    "    frobenius_norm = np.linalg.norm(jacobian, 'fro')\n",
    "    print(f\"Frobenius norm: {frobenius_norm:.6f}\")\n",
    "    print(f\"Average sensitivity: {frobenius_norm / np.sqrt(jacobian.size):.6f}\")\n",
    "    \n",
    "    # Element statistics\n",
    "    flat_jacobian = jacobian.flatten()\n",
    "    print(f\"Min element: {np.min(flat_jacobian):.6f}\")\n",
    "    print(f\"Max element: {np.max(flat_jacobian):.6f}\")\n",
    "    print(f\"Mean absolute: {np.mean(np.abs(flat_jacobian)):.6f}\")\n",
    "    print(f\"Std deviation: {np.std(flat_jacobian):.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'frobenius_norm': frobenius_norm,\n",
    "        'rank': np.linalg.matrix_rank(jacobian),\n",
    "        'mean_abs': np.mean(np.abs(flat_jacobian)),\n",
    "        'max_abs': np.max(np.abs(flat_jacobian))\n",
    "    }\n",
    "\n",
    "# Analyze all computed Jacobians\n",
    "print(\"JACOBIAN PROPERTY ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Output-input Jacobian analysis\n",
    "output_props = analyze_jacobian_properties(J_output_input, \"∂(output)/∂(input)\")\n",
    "\n",
    "# Hidden layer Jacobian analysis\n",
    "hidden_props = []\n",
    "for i, J_hidden in enumerate(hidden_jacobians):\n",
    "    props = analyze_jacobian_properties(J_hidden, f\"∂(hidden_layer_{i+1})/∂(input)\")\n",
    "    hidden_props.append(props)\n",
    "\n",
    "# Compare sensitivity across layers\n",
    "print(\"\\nSENSITIVITY COMPARISON ACROSS LAYERS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"{'Layer':<15} | {'Frobenius Norm':<15} | {'Max Sensitivity':<15} | {'Mean |Sensitivity|':<18}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "print(f\"{'Output':<15} | {output_props['frobenius_norm']:<15.6f} | {output_props['max_abs']:<15.6f} | {output_props['mean_abs']:<18.6f}\")\n",
    "\n",
    "for i, props in enumerate(hidden_props):\n",
    "    layer_name = f\"Hidden {i+1}\"\n",
    "    print(f\"{layer_name:<15} | {props['frobenius_norm']:<15.6f} | {props['max_abs']:<15.6f} | {props['mean_abs']:<18.6f}\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"• Frobenius norm measures overall network sensitivity\")\n",
    "print(\"• Max sensitivity shows most responsive input-output pairs\")\n",
    "print(\"• Layer comparison reveals sensitivity patterns through depth\")\n",
    "print(\"• Higher values indicate more sensitive/responsive transformations\")\n",
    "\n",
    "print(\"\\n✅ Jacobian property analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Network Jacobian Computation\n",
    "\n",
    "Now let's compute Jacobians efficiently using analytical methods and understand their structure across different network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement analytical Jacobian computation\n",
    "def compute_analytical_jacobians(network, x):\n",
    "    \"\"\"\n",
    "    Compute Jacobians analytically using chain rule.\n",
    "    More efficient and accurate than finite differences.\n",
    "    \"\"\"\n",
    "    # Forward pass to get all intermediate values\n",
    "    result = network.forward_detailed(x)\n",
    "    activations = result['activations']\n",
    "    z_values = result['z_values']\n",
    "    \n",
    "    jacobians = {}\n",
    "    \n",
    "    # 1. Compute ∂(output)/∂(input) analytically\n",
    "    # This requires propagating derivatives backward through all layers\n",
    "    \n",
    "    # Start with identity for output layer\n",
    "    current_jacobian = np.array([[1.0]])  # ∂(output)/∂(output) = 1\n",
    "    \n",
    "    # Work backwards through layers\n",
    "    for layer_idx in reversed(range(network.num_layers)):\n",
    "        # Get activation derivative for this layer\n",
    "        z = z_values[layer_idx]\n",
    "        activation_deriv = sigmoid_derivative(z)\n",
    "        \n",
    "        # Create diagonal matrix of activation derivatives\n",
    "        if np.isscalar(activation_deriv):\n",
    "            D_activation = np.array([[activation_deriv]])\n",
    "        else:\n",
    "            D_activation = np.diag(activation_deriv)\n",
    "        \n",
    "        # Get weight matrix for this layer\n",
    "        W = network.weights[layer_idx]\n",
    "        \n",
    "        # Apply chain rule: J_new = J_current × D_activation × W^T\n",
    "        current_jacobian = current_jacobian @ D_activation @ W.T\n",
    "        \n",
    "        # Store Jacobian for this layer's input (which is previous layer's output)\n",
    "        if layer_idx == 0:\n",
    "            jacobians['output_to_input'] = current_jacobian\n",
    "        else:\n",
    "            jacobians[f'output_to_layer_{layer_idx}'] = current_jacobian\n",
    "    \n",
    "    # 2. Compute ∂(hidden_layer_k)/∂(input) for each hidden layer\n",
    "    for hidden_layer in range(network.num_layers - 1):\n",
    "        # Start from hidden layer output\n",
    "        hidden_size = len(activations[hidden_layer + 1])\n",
    "        if hidden_size == 1:\n",
    "            layer_jacobian = np.array([[1.0]])\n",
    "        else:\n",
    "            layer_jacobian = np.eye(hidden_size)\n",
    "        \n",
    "        # Work backwards to input\n",
    "        for layer_idx in reversed(range(hidden_layer + 1)):\n",
    "            z = z_values[layer_idx]\n",
    "            activation_deriv = sigmoid_derivative(z)\n",
    "            \n",
    "            if np.isscalar(activation_deriv):\n",
    "                D_activation = np.array([[activation_deriv]])\n",
    "            else:\n",
    "                D_activation = np.diag(activation_deriv)\n",
    "            \n",
    "            W = network.weights[layer_idx]\n",
    "            layer_jacobian = layer_jacobian @ D_activation @ W.T\n",
    "        \n",
    "        jacobians[f'hidden_{hidden_layer+1}_to_input'] = layer_jacobian\n",
    "    \n",
    "    return jacobians\n",
    "\n",
    "print(\"ANALYTICAL JACOBIAN COMPUTATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Compute analytical Jacobians\n",
    "analytical_jacobians = compute_analytical_jacobians(net, x)\n",
    "\n",
    "print(\"Computed Jacobians:\")\n",
    "for name, jacobian in analytical_jacobians.items():\n",
    "    print(f\"  {name}: shape {jacobian.shape}\")\n",
    "\n",
    "# Compare analytical vs numerical (for verification)\n",
    "print(\"\\nVERIFICATION: Analytical vs Numerical\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Compare output-to-input Jacobian\n",
    "analytical_output_input = analytical_jacobians['output_to_input']\n",
    "numerical_output_input = J_output_input\n",
    "\n",
    "print(f\"Output-to-input Jacobian comparison:\")\n",
    "print(f\"Analytical: {analytical_output_input}\")\n",
    "print(f\"Numerical:  {numerical_output_input}\")\n",
    "print(f\"Max difference: {np.max(np.abs(analytical_output_input - numerical_output_input)):.2e}\")\n",
    "\n",
    "if np.max(np.abs(analytical_output_input - numerical_output_input)) < 1e-6:\n",
    "    print(\"✅ Analytical and numerical Jacobians match perfectly!\")\n",
    "elif np.max(np.abs(analytical_output_input - numerical_output_input)) < 1e-4:\n",
    "    print(\"✅ Analytical and numerical Jacobians match well\")\n",
    "else:\n",
    "    print(\"⚠️  Jacobians don't match - check implementation\")\n",
    "\n",
    "print(\"\\n✅ Analytical Jacobian computation verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize Jacobian structure across different inputs\n",
    "def visualize_jacobian_heatmaps(network, test_inputs, test_labels, test_texts):\n",
    "    \"\"\"\n",
    "    Visualize Jacobian matrices as heatmaps for different inputs.\n",
    "    \"\"\"\n",
    "    print(\"JACOBIAN VISUALIZATION ACROSS DIFFERENT INPUTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Select a few representative examples\n",
    "    indices = [0, 1, 4, 7]  # Different sentiment examples\n",
    "    \n",
    "    fig, axes = plt.subplots(len(indices), 2, figsize=(12, 3*len(indices)))\n",
    "    if len(indices) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    jacobian_data = []\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        x_test = test_inputs[idx]\n",
    "        y_test = test_labels[idx]\n",
    "        text = test_texts[idx]\n",
    "        \n",
    "        # Compute Jacobians for this input\n",
    "        jacobians = compute_analytical_jacobians(network, x_test)\n",
    "        \n",
    "        # Store for analysis\n",
    "        jacobian_data.append({\n",
    "            'input': x_test,\n",
    "            'label': y_test,\n",
    "            'text': text,\n",
    "            'jacobians': jacobians\n",
    "        })\n",
    "        \n",
    "        # Plot output-to-input Jacobian\n",
    "        J_output = jacobians['output_to_input']\n",
    "        \n",
    "        # Heatmap 1: Output-to-input sensitivity\n",
    "        sns.heatmap(J_output, annot=True, fmt='.4f', \n",
    "                   xticklabels=feature_names, yticklabels=['Output'],\n",
    "                   cmap='RdBu_r', center=0, ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f'∂(output)/∂(input)\\n\"{text[:20]}...\" (Label: {y_test})')\n",
    "        \n",
    "        # Heatmap 2: First hidden layer sensitivity\n",
    "        if 'hidden_1_to_input' in jacobians:\n",
    "            J_hidden = jacobians['hidden_1_to_input']\n",
    "            \n",
    "            sns.heatmap(J_hidden, annot=True, fmt='.3f',\n",
    "                       xticklabels=feature_names, \n",
    "                       yticklabels=[f'H1_{j}' for j in range(J_hidden.shape[0])],\n",
    "                       cmap='RdBu_r', center=0, ax=axes[i, 1])\n",
    "            axes[i, 1].set_title(f'∂(hidden_1)/∂(input)\\n\"{text[:20]}...\"')\n",
    "        else:\n",
    "            axes[i, 1].text(0.5, 0.5, 'No hidden layer', ha='center', va='center', \n",
    "                           transform=axes[i, 1].transAxes)\n",
    "            axes[i, 1].set_title('No hidden layer')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return jacobian_data\n",
    "\n",
    "# Visualize Jacobians for different tweet examples\n",
    "jacobian_examples = visualize_jacobian_heatmaps(net, features, labels, texts)\n",
    "\n",
    "# Analyze patterns across examples\n",
    "print(\"\\nPATTERN ANALYSIS ACROSS EXAMPLES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, data in enumerate(jacobian_examples):\n",
    "    text = data['text']\n",
    "    label = data['label']\n",
    "    J_output = data['jacobians']['output_to_input']\n",
    "    \n",
    "    print(f\"\\nExample {i+1}: \\\"{text}\\\" (Label: {label})\")\n",
    "    print(f\"Input features: {data['input']}\")\n",
    "    print(f\"Sensitivities: {J_output.flatten()}\")\n",
    "    \n",
    "    # Find most influential feature\n",
    "    max_sensitivity_idx = np.argmax(np.abs(J_output.flatten()))\n",
    "    max_sensitivity = J_output.flatten()[max_sensitivity_idx]\n",
    "    most_influential = feature_names[max_sensitivity_idx]\n",
    "    \n",
    "    print(f\"Most influential feature: {most_influential} (sensitivity: {max_sensitivity:.6f})\")\n",
    "    \n",
    "    # Interpret the sensitivity\n",
    "    if abs(max_sensitivity) > 0.1:\n",
    "        direction = \"increase\" if max_sensitivity > 0 else \"decrease\"\n",
    "        print(f\"  → Small increases in {most_influential} will {direction} prediction confidence\")\n",
    "\n",
    "print(\"\\n✅ Jacobian visualization and analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Eigenvalue Analysis\n",
    "\n",
    "Let's analyze the eigenvalues and eigenvectors of our Jacobian matrices to understand stability and conditioning properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform eigenvalue analysis of Jacobian matrices\n",
    "def analyze_jacobian_eigenvalues(jacobian, name=\"Jacobian\"):\n",
    "    \"\"\"\n",
    "    Analyze eigenvalues and eigenvectors of a Jacobian matrix.\n",
    "    \"\"\"\n",
    "    print(f\"\\nEIGENVALUE ANALYSIS: {name}\")\n",
    "    print(\"=\" * (20 + len(name)))\n",
    "    \n",
    "    # For non-square matrices, analyze J @ J^T and J^T @ J\n",
    "    if jacobian.shape[0] != jacobian.shape[1]:\n",
    "        print(f\"Non-square matrix {jacobian.shape} - analyzing related square matrices\")\n",
    "        \n",
    "        # J @ J^T (output space analysis)\n",
    "        JJT = jacobian @ jacobian.T\n",
    "        eigenvals_output, eigenvecs_output = np.linalg.eig(JJT)\n",
    "        \n",
    "        # J^T @ J (input space analysis)\n",
    "        JTJ = jacobian.T @ jacobian\n",
    "        eigenvals_input, eigenvecs_input = np.linalg.eig(JTJ)\n",
    "        \n",
    "        print(f\"Output space eigenvalues (J @ J^T): {eigenvals_output}\")\n",
    "        print(f\"Input space eigenvalues (J^T @ J): {eigenvals_input}\")\n",
    "        \n",
    "        # Singular values (more fundamental for rectangular matrices)\n",
    "        U, s, Vt = svd(jacobian)\n",
    "        print(f\"Singular values: {s}\")\n",
    "        print(f\"Condition number: {s[0]/s[-1]:.2e}\")\n",
    "        \n",
    "        # Principal directions\n",
    "        print(f\"\\nPrincipal input directions (V):\")\n",
    "        V = Vt.T\n",
    "        for i, (sv, direction) in enumerate(zip(s, V.T)):\n",
    "            print(f\"  Direction {i+1} (σ={sv:.4f}): {direction}\")\n",
    "            \n",
    "            # Interpret in terms of features\n",
    "            max_component_idx = np.argmax(np.abs(direction))\n",
    "            max_component = direction[max_component_idx]\n",
    "            dominant_feature = feature_names[max_component_idx]\n",
    "            print(f\"    Dominated by: {dominant_feature} (weight: {max_component:.4f})\")\n",
    "        \n",
    "        return {\n",
    "            'singular_values': s,\n",
    "            'condition_number': s[0]/s[-1] if len(s) > 1 else 1.0,\n",
    "            'rank': np.sum(s > 1e-10),\n",
    "            'principal_directions': V\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        # Square matrix - direct eigenvalue analysis\n",
    "        eigenvals, eigenvecs = np.linalg.eig(jacobian)\n",
    "        \n",
    "        print(f\"Eigenvalues: {eigenvals}\")\n",
    "        print(f\"Spectral radius: {np.max(np.abs(eigenvals)):.6f}\")\n",
    "        \n",
    "        # Stability analysis\n",
    "        max_eigenval = np.max(np.abs(eigenvals))\n",
    "        if max_eigenval > 1.0:\n",
    "            print(\"⚠️  Spectral radius > 1: Potentially unstable dynamics\")\n",
    "        else:\n",
    "            print(\"✅ Spectral radius ≤ 1: Stable dynamics\")\n",
    "        \n",
    "        return {\n",
    "            'eigenvalues': eigenvals,\n",
    "            'spectral_radius': max_eigenval,\n",
    "            'eigenvectors': eigenvecs\n",
    "        }\n",
    "\n",
    "# Analyze eigenvalues for our \"Go Dolphins!\" example\n",
    "print(\"EIGENVALUE ANALYSIS FOR 'GO DOLPHINS!' JACOBIANS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "x = features[0]  # \"Go Dolphins!\" example\n",
    "jacobians = compute_analytical_jacobians(net, x)\n",
    "\n",
    "eigenvalue_results = {}\n",
    "\n",
    "# Analyze each Jacobian\n",
    "for name, jacobian in jacobians.items():\n",
    "    result = analyze_jacobian_eigenvalues(jacobian, name)\n",
    "    eigenvalue_results[name] = result\n",
    "\n",
    "print(\"\\n✅ Eigenvalue analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare eigenvalue spectra across different network architectures\n",
    "def compare_network_architectures():\n",
    "    \"\"\"\n",
    "    Compare Jacobian eigenvalue properties across different network architectures.\n",
    "    \"\"\"\n",
    "    print(\"\\nCOMPARING JACOBIAN PROPERTIES ACROSS ARCHITECTURES\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Define different architectures to test\n",
    "    architectures = [\n",
    "        ([3, 1], \"Single Layer\"),\n",
    "        ([3, 4, 1], \"Two Layer\"),\n",
    "        ([3, 6, 3, 1], \"Three Layer\"),\n",
    "        ([3, 8, 4, 2, 1], \"Four Layer (Deep)\")\n",
    "    ]\n",
    "    \n",
    "    architecture_results = []\n",
    "    \n",
    "    x = features[0]  # Test on \"Go Dolphins!\"\n",
    "    \n",
    "    for arch, name in architectures:\n",
    "        print(f\"\\n{name} Network: {arch}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Create network\n",
    "        test_net = JacobianAnalysisNetwork(arch, seed=42)\n",
    "        \n",
    "        # Compute Jacobians\n",
    "        jacobians = compute_analytical_jacobians(test_net, x)\n",
    "        \n",
    "        # Analyze output-to-input Jacobian\n",
    "        J_output_input = jacobians['output_to_input']\n",
    "        \n",
    "        # Get singular values for conditioning analysis\n",
    "        U, s, Vt = svd(J_output_input)\n",
    "        condition_number = s[0] / s[-1] if len(s) > 1 and s[-1] > 1e-12 else np.inf\n",
    "        \n",
    "        # Calculate sensitivity metrics\n",
    "        frobenius_norm = np.linalg.norm(J_output_input, 'fro')\n",
    "        max_sensitivity = np.max(np.abs(J_output_input))\n",
    "        mean_sensitivity = np.mean(np.abs(J_output_input))\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'name': name,\n",
    "            'architecture': arch,\n",
    "            'num_parameters': sum(w.size + b.size for w, b in zip(test_net.weights, test_net.biases)),\n",
    "            'condition_number': condition_number,\n",
    "            'frobenius_norm': frobenius_norm,\n",
    "            'max_sensitivity': max_sensitivity,\n",
    "            'mean_sensitivity': mean_sensitivity,\n",
    "            'singular_values': s\n",
    "        }\n",
    "        architecture_results.append(result)\n",
    "        \n",
    "        print(f\"Parameters: {result['num_parameters']}\")\n",
    "        print(f\"Condition number: {condition_number:.2e}\")\n",
    "        print(f\"Frobenius norm: {frobenius_norm:.6f}\")\n",
    "        print(f\"Max sensitivity: {max_sensitivity:.6f}\")\n",
    "        print(f\"Singular values: {s}\")\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    names = [r['name'] for r in architecture_results]\n",
    "    \n",
    "    # Plot 1: Condition numbers\n",
    "    condition_numbers = [r['condition_number'] for r in architecture_results]\n",
    "    bars1 = axes[0, 0].bar(names, condition_numbers)\n",
    "    axes[0, 0].set_ylabel('Condition Number')\n",
    "    axes[0, 0].set_title('Jacobian Conditioning by Architecture')\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Color bars by conditioning quality\n",
    "    for bar, cond_num in zip(bars1, condition_numbers):\n",
    "        if cond_num > 1000:\n",
    "            bar.set_color('red')\n",
    "        elif cond_num > 100:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('green')\n",
    "    \n",
    "    # Plot 2: Overall sensitivity (Frobenius norm)\n",
    "    frobenius_norms = [r['frobenius_norm'] for r in architecture_results]\n",
    "    axes[0, 1].bar(names, frobenius_norms, color='skyblue')\n",
    "    axes[0, 1].set_ylabel('Frobenius Norm')\n",
    "    axes[0, 1].set_title('Overall Sensitivity by Architecture')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 3: Maximum sensitivity\n",
    "    max_sensitivities = [r['max_sensitivity'] for r in architecture_results]\n",
    "    axes[1, 0].bar(names, max_sensitivities, color='lightcoral')\n",
    "    axes[1, 0].set_ylabel('Max Sensitivity')\n",
    "    axes[1, 0].set_title('Peak Sensitivity by Architecture')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 4: Number of parameters vs sensitivity\n",
    "    num_params = [r['num_parameters'] for r in architecture_results]\n",
    "    axes[1, 1].scatter(num_params, frobenius_norms, s=100, alpha=0.7)\n",
    "    for i, name in enumerate(names):\n",
    "        axes[1, 1].annotate(name, (num_params[i], frobenius_norms[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points')\n",
    "    axes[1, 1].set_xlabel('Number of Parameters')\n",
    "    axes[1, 1].set_ylabel('Frobenius Norm')\n",
    "    axes[1, 1].set_title('Parameters vs Sensitivity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return architecture_results\n",
    "\n",
    "arch_comparison = compare_network_architectures()\n",
    "\n",
    "print(\"\\n✅ Architecture comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Robustness Assessment\n",
    "\n",
    "Finally, let's use Jacobian analysis to assess how robust our \"Go Dolphins!\" classifier is to input perturbations and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Assess model robustness using Jacobian analysis\n",
    "def assess_input_robustness(network, test_inputs, test_labels, test_texts):\n",
    "    \"\"\"\n",
    "    Assess model robustness to input perturbations using Jacobian analysis.\n",
    "    \"\"\"\n",
    "    print(\"ROBUSTNESS ASSESSMENT USING JACOBIAN ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    robustness_results = []\n",
    "    \n",
    "    for i, (x, y, text) in enumerate(zip(test_inputs, test_labels, test_texts)):\n",
    "        # Compute Jacobian for this input\n",
    "        jacobians = compute_analytical_jacobians(network, x)\n",
    "        J = jacobians['output_to_input']\n",
    "        \n",
    "        # Original prediction\n",
    "        original_output = network.forward_detailed(x)['output']\n",
    "        \n",
    "        # Robustness metrics\n",
    "        frobenius_norm = np.linalg.norm(J, 'fro')\n",
    "        max_sensitivity = np.max(np.abs(J))\n",
    "        \n",
    "        # Estimate robustness to small perturbations\n",
    "        # For perturbation ε, output change ≈ ||J|| * ||ε||\n",
    "        epsilon_magnitude = 0.1  # Small perturbation magnitude\n",
    "        estimated_output_change = frobenius_norm * epsilon_magnitude\n",
    "        \n",
    "        # Test actual robustness with random perturbations\n",
    "        num_tests = 50\n",
    "        actual_changes = []\n",
    "        \n",
    "        for _ in range(num_tests):\n",
    "            # Random perturbation\n",
    "            perturbation = np.random.normal(0, epsilon_magnitude, size=x.shape)\n",
    "            perturbed_input = x + perturbation\n",
    "            \n",
    "            # Perturbed output\n",
    "            perturbed_output = network.forward_detailed(perturbed_input)['output']\n",
    "            actual_change = abs(perturbed_output - original_output)\n",
    "            actual_changes.append(actual_change)\n",
    "        \n",
    "        mean_actual_change = np.mean(actual_changes)\n",
    "        std_actual_change = np.std(actual_changes)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'index': i,\n",
    "            'text': text,\n",
    "            'label': y,\n",
    "            'input': x,\n",
    "            'original_output': original_output,\n",
    "            'frobenius_norm': frobenius_norm,\n",
    "            'max_sensitivity': max_sensitivity,\n",
    "            'estimated_change': estimated_output_change,\n",
    "            'mean_actual_change': mean_actual_change,\n",
    "            'std_actual_change': std_actual_change,\n",
    "            'jacobian': J\n",
    "        }\n",
    "        robustness_results.append(result)\n",
    "    \n",
    "    return robustness_results\n",
    "\n",
    "def analyze_robustness_results(robustness_results):\n",
    "    \"\"\"\n",
    "    Analyze and visualize robustness assessment results.\n",
    "    \"\"\"\n",
    "    print(\"\\nROBUSTNESS ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Summary statistics\n",
    "    frobenius_norms = [r['frobenius_norm'] for r in robustness_results]\n",
    "    max_sensitivities = [r['max_sensitivity'] for r in robustness_results]\n",
    "    estimated_changes = [r['estimated_change'] for r in robustness_results]\n",
    "    actual_changes = [r['mean_actual_change'] for r in robustness_results]\n",
    "    \n",
    "    print(f\"Average Frobenius norm: {np.mean(frobenius_norms):.6f} ± {np.std(frobenius_norms):.6f}\")\n",
    "    print(f\"Average max sensitivity: {np.mean(max_sensitivities):.6f} ± {np.std(max_sensitivities):.6f}\")\n",
    "    print(f\"Average estimated change: {np.mean(estimated_changes):.6f} ± {np.std(estimated_changes):.6f}\")\n",
    "    print(f\"Average actual change: {np.mean(actual_changes):.6f} ± {np.std(actual_changes):.6f}\")\n",
    "    \n",
    "    # Correlation between Jacobian-based prediction and actual robustness\n",
    "    correlation = np.corrcoef(estimated_changes, actual_changes)[0, 1]\n",
    "    print(f\"\\nCorrelation (estimated vs actual): {correlation:.4f}\")\n",
    "    \n",
    "    if correlation > 0.7:\n",
    "        print(\"✅ Strong correlation: Jacobian analysis is a good robustness predictor\")\n",
    "    elif correlation > 0.3:\n",
    "        print(\"⚠️  Moderate correlation: Jacobian gives useful but imperfect robustness estimates\")\n",
    "    else:\n",
    "        print(\"❌ Weak correlation: Jacobian-based estimates don't match actual robustness\")\n",
    "    \n",
    "    # Detailed results for each example\n",
    "    print(\"\\nDETAILED ROBUSTNESS BY EXAMPLE:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'Ex':<3} | {'Text':<20} | {'Label':<5} | {'Output':<8} | {'Frobenius':<10} | {'Est.Change':<10} | {'Act.Change':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for r in robustness_results:\n",
    "        text_short = r['text'][:18] + '..' if len(r['text']) > 20 else r['text']\n",
    "        print(f\"{r['index']:<3} | {text_short:<20} | {r['label']:<5} | {r['original_output']:<8.4f} | \"\n",
    "              f\"{r['frobenius_norm']:<10.6f} | {r['estimated_change']:<10.6f} | {r['mean_actual_change']:<10.6f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Estimated vs Actual robustness\n",
    "    axes[0, 0].scatter(estimated_changes, actual_changes, alpha=0.7, s=80)\n",
    "    axes[0, 0].plot([0, max(max(estimated_changes), max(actual_changes))], \n",
    "                   [0, max(max(estimated_changes), max(actual_changes))], 'r--', alpha=0.5)\n",
    "    axes[0, 0].set_xlabel('Estimated Change (Jacobian-based)')\n",
    "    axes[0, 0].set_ylabel('Actual Change (Empirical)')\n",
    "    axes[0, 0].set_title(f'Robustness Prediction\\n(Correlation: {correlation:.3f})')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Sensitivity by example\n",
    "    indices = [r['index'] for r in robustness_results]\n",
    "    axes[0, 1].bar(indices, frobenius_norms, alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Example Index')\n",
    "    axes[0, 1].set_ylabel('Frobenius Norm')\n",
    "    axes[0, 1].set_title('Sensitivity by Example')\n",
    "    \n",
    "    # Plot 3: Robustness vs Original output\n",
    "    outputs = [r['original_output'] for r in robustness_results]\n",
    "    axes[1, 0].scatter(outputs, actual_changes, alpha=0.7, s=80)\n",
    "    axes[1, 0].set_xlabel('Original Output')\n",
    "    axes[1, 0].set_ylabel('Actual Change')\n",
    "    axes[1, 0].set_title('Robustness vs Prediction Confidence')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Distribution of sensitivities\n",
    "    axes[1, 1].hist(frobenius_norms, bins=10, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].axvline(np.mean(frobenius_norms), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(frobenius_norms):.4f}')\n",
    "    axes[1, 1].set_xlabel('Frobenius Norm')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Distribution of Sensitivities')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Perform robustness assessment\n",
    "robustness_data = assess_input_robustness(net, features, labels, texts)\n",
    "analyze_robustness_results(robustness_data)\n",
    "\n",
    "print(\"\\n✅ Robustness assessment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Summarize insights from Jacobian analysis\n",
    "def summarize_jacobian_insights():\n",
    "    \"\"\"\n",
    "    Summarize key insights from our comprehensive Jacobian analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE JACOBIAN ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n🔍 WHAT WE DISCOVERED:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    print(\"\\n1. SENSITIVITY STRUCTURE:\")\n",
    "    print(\"   • Output sensitivity varies significantly across input features\")\n",
    "    print(\"   • Different tweets show different sensitivity patterns\")\n",
    "    print(\"   • Hidden layers reveal intermediate feature combinations\")\n",
    "    \n",
    "    print(\"\\n2. MATHEMATICAL PROPERTIES:\")\n",
    "    print(\"   • Jacobian eigenvalues reveal stability characteristics\")\n",
    "    print(\"   • Condition numbers indicate optimization difficulty\")\n",
    "    print(\"   • Singular values show principal sensitivity directions\")\n",
    "    \n",
    "    print(\"\\n3. ROBUSTNESS INSIGHTS:\")\n",
    "    print(\"   • Frobenius norm predicts sensitivity to input noise\")\n",
    "    print(\"   • Jacobian-based estimates correlate with empirical robustness\")\n",
    "    print(\"   • Some examples are inherently more robust than others\")\n",
    "    \n",
    "    print(\"\\n4. ARCHITECTURAL EFFECTS:\")\n",
    "    print(\"   • Deeper networks can have different conditioning properties\")\n",
    "    print(\"   • Parameter count doesn't directly correlate with sensitivity\")\n",
    "    print(\"   • Architecture choice affects Jacobian spectral properties\")\n",
    "    \n",
    "    print(\"\\n🧮 MATHEMATICAL SIGNIFICANCE:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\n• JACOBIAN MATRICES provide complete sensitivity maps\")\n",
    "    print(\"• EIGENVALUE ANALYSIS reveals stability and conditioning\")\n",
    "    print(\"• SINGULAR VALUE DECOMPOSITION shows principal directions\")\n",
    "    print(\"• LINEAR APPROXIMATION enables robustness prediction\")\n",
    "    \n",
    "    print(\"\\n🎯 PRACTICAL APPLICATIONS:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    print(\"\\n1. MODEL INTERPRETABILITY:\")\n",
    "    print(\"   → Which input features most affect predictions?\")\n",
    "    print(\"   → How do internal representations depend on inputs?\")\n",
    "    \n",
    "    print(\"\\n2. ROBUSTNESS ASSESSMENT:\")\n",
    "    print(\"   → How sensitive is the model to input noise?\")\n",
    "    print(\"   → Which examples are most vulnerable to perturbations?\")\n",
    "    \n",
    "    print(\"\\n3. OPTIMIZATION ANALYSIS:\")\n",
    "    print(\"   → Is the loss landscape well-conditioned?\")\n",
    "    print(\"   → What are the principal optimization directions?\")\n",
    "    \n",
    "    print(\"\\n4. ARCHITECTURE DESIGN:\")\n",
    "    print(\"   → How does network depth affect sensitivity?\")\n",
    "    print(\"   → What architectural choices improve conditioning?\")\n",
    "    \n",
    "    print(\"\\n🚀 CONNECTION TO MODERN AI:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    print(\"\\n• ADVERSARIAL ROBUSTNESS: Jacobians reveal vulnerability to attacks\")\n",
    "    print(\"• FEATURE ATTRIBUTION: Gradients show input importance\")\n",
    "    print(\"• NETWORK PRUNING: Sensitivity analysis guides compression\")\n",
    "    print(\"• TRANSFER LEARNING: Jacobian analysis informs fine-tuning\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"The Jacobian is the mathematical lens through which we understand\")\n",
    "    print(\"how neural networks transform information and respond to changes.\")\n",
    "    print(\"Every modern AI advancement relies on this fundamental analysis!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "summarize_jacobian_insights()\n",
    "print(\"\\n✅ Jacobian analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "You've now mastered Jacobian analysis - a powerful mathematical tool for understanding neural network sensitivity and robustness! Here's what we discovered:\n",
    "\n",
    "**🔑 Key Mathematical Insights:**\n",
    "1. **Complete Sensitivity Maps** - Jacobians reveal how every output depends on every input\n",
    "2. **Eigenvalue Analysis** - Spectral properties indicate stability and conditioning\n",
    "3. **Robustness Prediction** - Linear approximations estimate response to perturbations\n",
    "4. **Principal Directions** - SVD reveals most important input-output relationships\n",
    "\n",
    "**🧮 Mathematical Tools Mastered:**\n",
    "- **Matrix calculus** for multi-dimensional sensitivity analysis\n",
    "- **Eigenvalue decomposition** for stability assessment\n",
    "- **Singular value decomposition** for principal component analysis\n",
    "- **Linear approximation theory** for robustness estimation\n",
    "\n",
    "**🎯 Why This Matters:**\n",
    "Jacobian analysis is fundamental to modern AI:\n",
    "- **Interpretability**: Understanding which inputs matter most\n",
    "- **Robustness**: Predicting model vulnerability to noise/attacks\n",
    "- **Optimization**: Revealing conditioning and convergence properties\n",
    "- **Architecture Design**: Guiding network structure choices\n",
    "\n",
    "**🚀 Coming in Problem 4: Vector Fields**\n",
    "- How do we visualize optimization dynamics across the entire landscape?\n",
    "- What are vector fields and how do they reveal learning patterns?\n",
    "- How do different optimizers create different flow patterns?\n",
    "- What can vector field analysis tell us about convergence?\n",
    "\n",
    "You're approaching a complete mathematical understanding of AI systems! 🐬➡️📊➡️🎯➡️⚡➡️🚀➡️🧮➡️🔗➡️📐➡️🌊"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}