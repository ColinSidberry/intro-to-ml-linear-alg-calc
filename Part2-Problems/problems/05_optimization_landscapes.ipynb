{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Optimization Landscapes - Advanced Convergence Analysis\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this problem, you will:\n",
    "- Understand the complete mathematical theory governing optimization convergence\n",
    "- Analyze Lipschitz continuity, convexity, and smoothness properties\n",
    "- Apply advanced optimization theory to understand convergence guarantees\n",
    "- Connect theoretical results to practical deep learning optimization\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "1. **Landscape Topology Analysis** - Convexity, smoothness, and conditioning properties\n",
    "2. **Convergence Theory** - Mathematical guarantees and convergence rates\n",
    "3. **Escape Mechanisms** - How optimizers escape local minima and saddle points\n",
    "4. **Modern Optimization Theory** - Advanced techniques and theoretical frontiers\n",
    "\n",
    "---\n",
    "\n",
    "## The Complete Mathematical Picture\n",
    "\n",
    "Throughout Part 2, you've built a deep mathematical understanding of machine learning:\n",
    "- **Problem 1**: Vector calculus and loss landscape analysis\n",
    "- **Problem 2**: Chain rule and backpropagation theory\n",
    "- **Problem 3**: Jacobian analysis and sensitivity theory\n",
    "- **Problem 4**: Vector fields and dynamical systems\n",
    "\n",
    "Now we culminate with **optimization landscape theory** - the mathematical framework that explains when and why optimization succeeds or fails.\n",
    "\n",
    "**The Fundamental Questions**:\n",
    "- **When does gradient descent converge?** (Convergence guarantees)\n",
    "- **How fast does it converge?** (Convergence rates)\n",
    "- **Can it escape bad local minima?** (Global optimization)\n",
    "- **What makes some problems easier than others?** (Problem conditioning)\n",
    "\n",
    "## Mathematical Foundation: Optimization Theory\n",
    "\n",
    "**Landscape Properties**:\n",
    "- **Lipschitz Continuity**: $|\\nabla L(\\mathbf{x}) - \\nabla L(\\mathbf{y})| \\leq L |\\mathbf{x} - \\mathbf{y}|$\n",
    "- **Strong Convexity**: $L(\\mathbf{y}) \\geq L(\\mathbf{x}) + \\nabla L(\\mathbf{x})^T(\\mathbf{y} - \\mathbf{x}) + \\frac{\\mu}{2}|\\mathbf{y} - \\mathbf{x}|^2$\n",
    "- **Smoothness**: $L(\\mathbf{y}) \\leq L(\\mathbf{x}) + \\nabla L(\\mathbf{x})^T(\\mathbf{y} - \\mathbf{x}) + \\frac{L}{2}|\\mathbf{y} - \\mathbf{x}|^2$\n",
    "\n",
    "**Convergence Rates**:\n",
    "- **Convex**: $O(1/t)$ (sublinear)\n",
    "- **Strongly Convex + Smooth**: $O(e^{-t/\\kappa})$ (linear), where $\\kappa = L/\\mu$ is condition number\n",
    "- **Non-convex**: Complex, depends on landscape structure\n",
    "\n",
    "**Modern Insights**:\n",
    "- **Saddle Point Avoidance**: Gradient descent avoids strict saddle points\n",
    "- **Overparameterization**: Wide networks have benign optimization landscapes\n",
    "- **Implicit Regularization**: SGD implicitly prefers certain solutions\n",
    "\n",
    "## Why This Matters for \"Go Dolphins!\"\n",
    "\n",
    "Understanding optimization theory explains:\n",
    "- **Why our classifier converged reliably** (landscape properties)\n",
    "- **How different optimizers achieved different convergence speeds** (theoretical rates)\n",
    "- **What happens with different architectures** (conditioning effects)\n",
    "- **How this scales to real deep learning** (modern theory connections)\n",
    "\n",
    "This completes our journey from a simple sentiment classifier to the deepest mathematical understanding of machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for advanced optimization landscape analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.optimize import minimize, minimize_scalar\n",
    "from scipy.linalg import eigvals, norm\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our utilities\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from data_generators import load_sports_dataset\n",
    "\n",
    "# Load our \"Go Dolphins!\" dataset\n",
    "features, labels, feature_names, texts = load_sports_dataset()\n",
    "\n",
    "print(\"ADVANCED OPTIMIZATION LANDSCAPE ANALYSIS\")\n",
    "print(\"=\" * 48)\n",
    "print(f\"Final mathematical analysis of '{texts[0]}' classifier\")\n",
    "print(f\"Dataset: {len(texts)} tweets, {len(feature_names)} features\")\n",
    "print()\n",
    "print(\"Theoretical properties we'll analyze:\")\n",
    "print(\"• Lipschitz continuity of gradients\")\n",
    "print(\"• Convexity and strong convexity\")\n",
    "print(\"• Smoothness and conditioning\")\n",
    "print(\"• Convergence rates and guarantees\")\n",
    "print(\"• Saddle point structure\")\n",
    "print(\"• Global optimization properties\")\n",
    "print()\n",
    "print(\"Mathematical frameworks:\")\n",
    "print(\"• Classical optimization theory\")\n",
    "print(\"• Modern non-convex analysis\")\n",
    "print(\"• Stochastic optimization theory\")\n",
    "print(\"• Deep learning theory connections\")\n",
    "\n",
    "# Define our loss function and derivatives with enhanced numerical stability\n",
    "def sigmoid(x):\n",
    "    \"\"\"Numerically stable sigmoid\"\"\"\n",
    "    return np.where(x >= 0, \n",
    "                   1 / (1 + np.exp(-x)),\n",
    "                   np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "def loss_function(weights):\n",
    "    \"\"\"Binary cross-entropy loss with numerical stability\"\"\"\n",
    "    total_loss = 0.0\n",
    "    for i in range(len(features)):\n",
    "        z = np.dot(features[i], weights)\n",
    "        \n",
    "        # Numerically stable log-sum-exp for BCE\n",
    "        if labels[i] == 1:\n",
    "            # -log(sigmoid(z)) = log(1 + exp(-z)) for z >= 0, else z + log(1 + exp(-z))\n",
    "            if z >= 0:\n",
    "                loss = np.log(1 + np.exp(-z))\n",
    "            else:\n",
    "                loss = -z + np.log(1 + np.exp(z))\n",
    "        else:\n",
    "            # -log(1 - sigmoid(z)) = log(1 + exp(z)) for z <= 0, else z + log(1 + exp(-z))\n",
    "            if z <= 0:\n",
    "                loss = np.log(1 + np.exp(z))\n",
    "            else:\n",
    "                loss = z + np.log(1 + np.exp(-z))\n",
    "        \n",
    "        total_loss += loss\n",
    "    \n",
    "    return total_loss / len(features)\n",
    "\n",
    "def gradient_function(weights):\n",
    "    \"\"\"Analytical gradient with numerical stability\"\"\"\n",
    "    total_gradient = np.zeros_like(weights)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        z = np.dot(features[i], weights)\n",
    "        p = sigmoid(z)\n",
    "        error = p - labels[i]\n",
    "        total_gradient += error * features[i]\n",
    "    \n",
    "    return total_gradient / len(features)\n",
    "\n",
    "def hessian_function(weights):\n",
    "    \"\"\"Analytical Hessian matrix\"\"\"\n",
    "    n = len(weights)\n",
    "    hessian = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        z = np.dot(features[i], weights)\n",
    "        p = sigmoid(z)\n",
    "        weight = p * (1 - p)  # Second derivative of sigmoid\n",
    "        \n",
    "        # Outer product weighted by sigmoid derivative\n",
    "        hessian += weight * np.outer(features[i], features[i])\n",
    "    \n",
    "    return hessian / len(features)\n",
    "\n",
    "# Test the functions\n",
    "test_point = np.array([0.3, 0.5, 0.4])\n",
    "print(f\"\\nTest evaluation at w = {test_point}:\")\n",
    "print(f\"Loss: {loss_function(test_point):.8f}\")\n",
    "print(f\"Gradient: {gradient_function(test_point)}\")\n",
    "print(f\"Gradient norm: {np.linalg.norm(gradient_function(test_point)):.8f}\")\n",
    "\n",
    "H = hessian_function(test_point)\n",
    "eigenvals = eigvals(H)\n",
    "print(f\"Hessian eigenvalues: {eigenvals}\")\n",
    "print(f\"Condition number: {np.max(eigenvals)/np.min(eigenvals):.2f}\")\n",
    "\n",
    "print(\"\\n✅ Advanced optimization analysis setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Landscape Topology Analysis\n",
    "\n",
    "Let's analyze the fundamental mathematical properties that govern optimization behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze Lipschitz continuity and smoothness\n",
    "def analyze_lipschitz_properties():\n",
    "    \"\"\"\n",
    "    Analyze Lipschitz continuity of the loss function and its gradient.\n",
    "    \"\"\"\n",
    "    print(\"LIPSCHITZ CONTINUITY ANALYSIS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Sample points for Lipschitz constant estimation\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Generate random weight pairs\n",
    "    weights1 = np.random.normal(0, 1, (n_samples, 3))\n",
    "    weights2 = np.random.normal(0, 1, (n_samples, 3))\n",
    "    \n",
    "    # Compute Lipschitz ratios\n",
    "    gradient_lipschitz_ratios = []\n",
    "    loss_lipschitz_ratios = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        w1, w2 = weights1[i], weights2[i]\n",
    "        \n",
    "        # Skip if points are too close\n",
    "        w_diff = np.linalg.norm(w2 - w1)\n",
    "        if w_diff < 1e-8:\n",
    "            continue\n",
    "        \n",
    "        # Gradient Lipschitz: |∇f(x) - ∇f(y)| / |x - y|\n",
    "        grad1 = gradient_function(w1)\n",
    "        grad2 = gradient_function(w2)\n",
    "        grad_diff = np.linalg.norm(grad2 - grad1)\n",
    "        grad_lipschitz = grad_diff / w_diff\n",
    "        gradient_lipschitz_ratios.append(grad_lipschitz)\n",
    "        \n",
    "        # Function Lipschitz: |f(x) - f(y)| / |x - y|\n",
    "        loss1 = loss_function(w1)\n",
    "        loss2 = loss_function(w2)\n",
    "        loss_diff = abs(loss2 - loss1)\n",
    "        loss_lipschitz = loss_diff / w_diff\n",
    "        loss_lipschitz_ratios.append(loss_lipschitz)\n",
    "    \n",
    "    # Estimate Lipschitz constants\n",
    "    L_grad = np.max(gradient_lipschitz_ratios)  # Gradient Lipschitz constant\n",
    "    L_loss = np.max(loss_lipschitz_ratios)      # Function Lipschitz constant\n",
    "    \n",
    "    print(f\"Estimated gradient Lipschitz constant L: {L_grad:.6f}\")\n",
    "    print(f\"Estimated function Lipschitz constant: {L_loss:.6f}\")\n",
    "    print(f\"Mean gradient Lipschitz ratio: {np.mean(gradient_lipschitz_ratios):.6f}\")\n",
    "    print(f\"Std gradient Lipschitz ratio: {np.std(gradient_lipschitz_ratios):.6f}\")\n",
    "    \n",
    "    # Theoretical bound from Hessian\n",
    "    # For twice differentiable functions, L = max eigenvalue of Hessian\n",
    "    sample_points = np.random.normal(0, 1, (100, 3))\n",
    "    max_eigenval = 0\n",
    "    \n",
    "    for point in sample_points:\n",
    "        H = hessian_function(point)\n",
    "        max_eig = np.max(eigvals(H))\n",
    "        max_eigenval = max(max_eigenval, max_eig)\n",
    "    \n",
    "    print(f\"\\nTheoretical Lipschitz bound (max Hessian eigenvalue): {max_eigenval:.6f}\")\n",
    "    \n",
    "    if L_grad <= max_eigenval * 1.1:  # Small tolerance for numerical errors\n",
    "        print(\"✅ Empirical estimate consistent with theoretical bound\")\n",
    "    else:\n",
    "        print(\"⚠️  Empirical estimate exceeds theoretical bound\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram of Lipschitz ratios\n",
    "    axes[0].hist(gradient_lipschitz_ratios, bins=50, alpha=0.7, density=True, \n",
    "                edgecolor='black', label='Empirical ratios')\n",
    "    axes[0].axvline(L_grad, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Max ratio: {L_grad:.3f}')\n",
    "    axes[0].axvline(np.mean(gradient_lipschitz_ratios), color='blue', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(gradient_lipschitz_ratios):.3f}')\n",
    "    axes[0].set_xlabel('Gradient Lipschitz Ratio')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].set_title('Distribution of Gradient Lipschitz Ratios')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot of ratios vs distance\n",
    "    distances = [np.linalg.norm(weights2[i] - weights1[i]) for i in range(len(gradient_lipschitz_ratios))]\n",
    "    axes[1].scatter(distances[:500], gradient_lipschitz_ratios[:500], alpha=0.6, s=20)\n",
    "    axes[1].axhline(L_grad, color='red', linestyle='--', \n",
    "                   label=f'Lipschitz constant: {L_grad:.3f}')\n",
    "    axes[1].set_xlabel('Distance |w₂ - w₁|')\n",
    "    axes[1].set_ylabel('Gradient Lipschitz Ratio')\n",
    "    axes[1].set_title('Lipschitz Ratios vs Distance')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return L_grad, L_loss\n",
    "\n",
    "# Analyze Lipschitz properties\n",
    "L_gradient, L_function = analyze_lipschitz_properties()\n",
    "\n",
    "print(\"\\n✅ Lipschitz analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze convexity and strong convexity\n",
    "def analyze_convexity_properties():\n",
    "    \"\"\"\n",
    "    Analyze convexity properties of the loss function.\n",
    "    \"\"\"\n",
    "    print(\"\\nCONVEXITY ANALYSIS\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    # For binary cross-entropy with sigmoid, we can analyze convexity theoretically\n",
    "    # The Hessian tells us about local convexity\n",
    "    \n",
    "    # Sample points to check Hessian positive definiteness\n",
    "    np.random.seed(42)\n",
    "    sample_points = np.random.normal(0, 2, (200, 3))  # Wider sampling\n",
    "    \n",
    "    min_eigenvals = []\n",
    "    max_eigenvals = []\n",
    "    condition_numbers = []\n",
    "    \n",
    "    print(\"Sampling Hessian properties across the landscape...\")\n",
    "    \n",
    "    for point in sample_points:\n",
    "        H = hessian_function(point)\n",
    "        eigenvals = eigvals(H)\n",
    "        \n",
    "        # Ensure eigenvalues are real (should be for symmetric matrix)\n",
    "        eigenvals = np.real(eigenvals)\n",
    "        \n",
    "        min_eig = np.min(eigenvals)\n",
    "        max_eig = np.max(eigenvals)\n",
    "        \n",
    "        min_eigenvals.append(min_eig)\n",
    "        max_eigenvals.append(max_eig)\n",
    "        \n",
    "        if min_eig > 1e-12:  # Avoid division by very small numbers\n",
    "            condition_numbers.append(max_eig / min_eig)\n",
    "    \n",
    "    min_eigenvals = np.array(min_eigenvals)\n",
    "    max_eigenvals = np.array(max_eigenvals)\n",
    "    condition_numbers = np.array(condition_numbers)\n",
    "    \n",
    "    # Analyze convexity\n",
    "    positive_definite_fraction = np.mean(min_eigenvals > 1e-8)\n",
    "    positive_semidefinite_fraction = np.mean(min_eigenvals >= -1e-8)\n",
    "    \n",
    "    # Strong convexity parameter (minimum eigenvalue)\n",
    "    mu = np.min(min_eigenvals) if np.min(min_eigenvals) > 0 else 0\n",
    "    L = np.max(max_eigenvals)  # Smoothness parameter\n",
    "    \n",
    "    print(f\"\\nCONVEXITY ANALYSIS RESULTS:\")\n",
    "    print(f\"Fraction with positive definite Hessian: {positive_definite_fraction:.3f}\")\n",
    "    print(f\"Fraction with positive semidefinite Hessian: {positive_semidefinite_fraction:.3f}\")\n",
    "    print(f\"Minimum eigenvalue across samples: {np.min(min_eigenvals):.6f}\")\n",
    "    print(f\"Maximum eigenvalue across samples: {np.max(max_eigenvals):.6f}\")\n",
    "    \n",
    "    if positive_definite_fraction > 0.95:\n",
    "        print(\"✅ Function is locally strongly convex almost everywhere\")\n",
    "        print(f\"Strong convexity parameter μ ≈ {mu:.6f}\")\n",
    "    elif positive_semidefinite_fraction > 0.95:\n",
    "        print(\"✅ Function is locally convex almost everywhere\")\n",
    "    else:\n",
    "        print(\"⚠️  Function has non-convex regions\")\n",
    "    \n",
    "    print(f\"Smoothness parameter L ≈ {L:.6f}\")\n",
    "    \n",
    "    if mu > 0:\n",
    "        kappa = L / mu\n",
    "        print(f\"Condition number κ = L/μ ≈ {kappa:.2f}\")\n",
    "        \n",
    "        if kappa < 100:\n",
    "            print(\"✅ Well-conditioned optimization problem\")\n",
    "        elif kappa < 1000:\n",
    "            print(\"⚠️  Moderately conditioned optimization problem\")\n",
    "        else:\n",
    "            print(\"❌ Poorly conditioned optimization problem\")\n",
    "    \n",
    "    # Theoretical analysis for logistic regression\n",
    "    print(f\"\\nTHEORETICAL ANALYSIS:\")\n",
    "    print(\"For logistic regression (BCE + sigmoid):\")\n",
    "    print(\"• Always convex in parameter space\")\n",
    "    print(\"• Hessian = (1/n) Σ σ(xᵢᵀw)(1-σ(xᵢᵀw)) xᵢxᵢᵀ\")\n",
    "    print(\"• Positive semidefinite by construction\")\n",
    "    print(\"• Strong convexity depends on data distribution\")\n",
    "    \n",
    "    # Check if data provides strong convexity\n",
    "    # Compute minimum eigenvalue of empirical covariance\n",
    "    feature_cov = np.cov(features.T)\n",
    "    cov_eigenvals = eigvals(feature_cov)\n",
    "    min_cov_eigenval = np.min(np.real(cov_eigenvals))\n",
    "    \n",
    "    print(f\"\\nDATA-DEPENDENT ANALYSIS:\")\n",
    "    print(f\"Feature covariance minimum eigenvalue: {min_cov_eigenval:.6f}\")\n",
    "    \n",
    "    if min_cov_eigenval > 1e-8:\n",
    "        print(\"✅ Feature matrix has full rank - strong convexity expected\")\n",
    "    else:\n",
    "        print(\"⚠️  Feature matrix is rank-deficient - only convexity guaranteed\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot 1: Eigenvalue distribution\n",
    "    axes[0].hist(min_eigenvals, bins=30, alpha=0.7, label='Min eigenvalues', \n",
    "                color='blue', edgecolor='black')\n",
    "    axes[0].hist(max_eigenvals, bins=30, alpha=0.7, label='Max eigenvalues', \n",
    "                color='red', edgecolor='black')\n",
    "    axes[0].axvline(0, color='black', linestyle='--', alpha=0.8)\n",
    "    axes[0].set_xlabel('Eigenvalue')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Hessian Eigenvalue Distribution')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Condition numbers\n",
    "    if len(condition_numbers) > 0:\n",
    "        axes[1].hist(condition_numbers, bins=30, alpha=0.7, color='green', \n",
    "                    edgecolor='black')\n",
    "        axes[1].axvline(np.mean(condition_numbers), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(condition_numbers):.1f}')\n",
    "        axes[1].set_xlabel('Condition Number κ')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Condition Number Distribution')\n",
    "        axes[1].set_yscale('log')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Min vs Max eigenvalues\n",
    "    axes[2].scatter(min_eigenvals, max_eigenvals, alpha=0.6, s=20)\n",
    "    axes[2].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[2].axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[2].set_xlabel('Minimum Eigenvalue')\n",
    "    axes[2].set_ylabel('Maximum Eigenvalue')\n",
    "    axes[2].set_title('Eigenvalue Scatter Plot')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mu, L, positive_definite_fraction\n",
    "\n",
    "# Analyze convexity\n",
    "mu_strong_convexity, L_smoothness, convex_fraction = analyze_convexity_properties()\n",
    "\n",
    "print(\"\\n✅ Convexity analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Convergence Theory\n",
    "\n",
    "Let's apply theoretical convergence results to understand optimization guarantees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze convergence rates and guarantees\n",
    "def theoretical_convergence_analysis():\n",
    "    \"\"\"\n",
    "    Analyze theoretical convergence rates based on landscape properties.\n",
    "    \"\"\"\n",
    "    print(\"\\nTHEORETICAL CONVERGENCE ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Use previously computed landscape parameters\n",
    "    L = L_smoothness  # Lipschitz constant\n",
    "    mu = mu_strong_convexity  # Strong convexity parameter\n",
    "    \n",
    "    print(f\"Landscape parameters:\")\n",
    "    print(f\"• Lipschitz constant L: {L:.6f}\")\n",
    "    print(f\"• Strong convexity μ: {mu:.6f}\")\n",
    "    \n",
    "    if mu > 1e-8:\n",
    "        kappa = L / mu\n",
    "        print(f\"• Condition number κ = L/μ: {kappa:.2f}\")\n",
    "        \n",
    "        # Theoretical convergence rates\n",
    "        print(f\"\\nTHEORETICAL CONVERGENCE RATES:\")\n",
    "        print(f\"\\n1. GRADIENT DESCENT:\")\n",
    "        print(f\"   Step size: α ≤ 2/(L + μ) ≈ {2/(L + mu):.6f}\")\n",
    "        print(f\"   Optimal step size: α* = 2/(L + μ) ≈ {2/(L + mu):.6f}\")\n",
    "        print(f\"   Convergence rate: O((κ-1)/(κ+1))^t = O({(kappa-1)/(kappa+1):.6f}^t)\")\n",
    "        print(f\"   Linear convergence with rate {(kappa-1)/(kappa+1):.6f}\")\n",
    "        \n",
    "        print(f\"\\n2. ACCELERATED GRADIENT DESCENT (Nesterov):\")\n",
    "        print(f\"   Convergence rate: O((√κ-1)/(√κ+1))^t = O({(np.sqrt(kappa)-1)/(np.sqrt(kappa)+1):.6f}^t)\")\n",
    "        print(f\"   Acceleration factor: {(kappa-1)/(kappa+1) / ((np.sqrt(kappa)-1)/(np.sqrt(kappa)+1)):.2f}x improvement\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n⚠️  Function is convex but not strongly convex (μ ≈ 0)\")\n",
    "        print(f\"\\nTHEORETICAL CONVERGENCE RATES:\")\n",
    "        print(f\"\\n1. GRADIENT DESCENT:\")\n",
    "        print(f\"   Step size: α ≤ 1/L ≈ {1/L:.6f}\")\n",
    "        print(f\"   Convergence rate: O(1/t) (sublinear)\")\n",
    "        print(f\"   No linear convergence without strong convexity\")\n",
    "    \n",
    "    print(f\"\\n3. STOCHASTIC GRADIENT DESCENT:\")\n",
    "    print(f\"   For convex: O(1/√t) convergence\")\n",
    "    if mu > 1e-8:\n",
    "        print(f\"   For strongly convex: O(1/t) convergence\")\n",
    "    \n",
    "    return L, mu\n",
    "\n",
    "def empirical_convergence_analysis():\n",
    "    \"\"\"\n",
    "    Compare theoretical predictions with empirical convergence.\n",
    "    \"\"\"\n",
    "    print(f\"\\nEMPIRICAL CONVERGENCE VERIFICATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Find optimal point\n",
    "    from scipy.optimize import minimize\n",
    "    result = minimize(loss_function, np.zeros(3), jac=gradient_function, method='BFGS')\n",
    "    w_optimal = result.x\n",
    "    f_optimal = loss_function(w_optimal)\n",
    "    \n",
    "    print(f\"Optimal point: {w_optimal}\")\n",
    "    print(f\"Optimal loss: {f_optimal:.8f}\")\n",
    "    \n",
    "    # Test different step sizes\n",
    "    step_sizes = [0.01, 0.1, 0.5, 1.0]\n",
    "    max_iterations = 1000\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    convergence_data = {}\n",
    "    \n",
    "    for idx, alpha in enumerate(step_sizes):\n",
    "        print(f\"\\nTesting step size α = {alpha}:\")\n",
    "        \n",
    "        # Run gradient descent\n",
    "        w = np.array([1.0, 1.0, 1.0])  # Starting point\n",
    "        losses = []\n",
    "        distances_to_optimal = []\n",
    "        \n",
    "        for t in range(max_iterations):\n",
    "            loss_val = loss_function(w)\n",
    "            distance = np.linalg.norm(w - w_optimal)\n",
    "            \n",
    "            losses.append(loss_val - f_optimal)  # Optimality gap\n",
    "            distances_to_optimal.append(distance)\n",
    "            \n",
    "            # Gradient descent step\n",
    "            grad = gradient_function(w)\n",
    "            w = w - alpha * grad\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(grad) < 1e-10:\n",
    "                break\n",
    "        \n",
    "        convergence_data[alpha] = {\n",
    "            'losses': losses,\n",
    "            'distances': distances_to_optimal,\n",
    "            'iterations': len(losses)\n",
    "        }\n",
    "        \n",
    "        # Plot convergence\n",
    "        ax = axes[idx]\n",
    "        iterations = range(len(losses))\n",
    "        \n",
    "        ax.semilogy(iterations, losses, 'b-', linewidth=2, label='Optimality gap')\n",
    "        ax.semilogy(iterations, distances_to_optimal, 'r--', linewidth=2, label='Distance to optimum')\n",
    "        \n",
    "        # Theoretical convergence rate (if strongly convex)\n",
    "        if mu_strong_convexity > 1e-8:\n",
    "            kappa = L_smoothness / mu_strong_convexity\n",
    "            theoretical_rate = ((kappa - 1) / (kappa + 1))**np.array(iterations)\n",
    "            theoretical_curve = losses[0] * theoretical_rate\n",
    "            ax.semilogy(iterations, theoretical_curve, 'g:', linewidth=2, \n",
    "                       label=f'Theoretical O({(kappa-1)/(kappa+1):.3f}^t)')\n",
    "        \n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_title(f'Convergence with α = {alpha}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Analyze convergence rate empirically\n",
    "        if len(losses) > 10:\n",
    "            # Fit exponential decay to last part of convergence\n",
    "            start_idx = len(losses) // 4  # Use last 3/4 of iterations\n",
    "            log_losses = np.log(losses[start_idx:])\n",
    "            x = np.arange(len(log_losses))\n",
    "            \n",
    "            if len(x) > 1 and np.all(np.isfinite(log_losses)):\n",
    "                slope = np.polyfit(x, log_losses, 1)[0]\n",
    "                empirical_rate = np.exp(slope)\n",
    "                print(f\"  Empirical convergence rate: {empirical_rate:.6f}\")\n",
    "                print(f\"  Converged in {len(losses)} iterations\")\n",
    "                \n",
    "                if mu_strong_convexity > 1e-8:\n",
    "                    theoretical_rate = (kappa - 1) / (kappa + 1)\n",
    "                    print(f\"  Theoretical rate: {theoretical_rate:.6f}\")\n",
    "                    print(f\"  Rate ratio (empirical/theoretical): {empirical_rate/theoretical_rate:.2f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return convergence_data\n",
    "\n",
    "# Perform convergence analysis\n",
    "L_theory, mu_theory = theoretical_convergence_analysis()\n",
    "empirical_data = empirical_convergence_analysis()\n",
    "\n",
    "print(\"\\n✅ Convergence analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Escape Mechanisms\n",
    "\n",
    "Let's analyze how optimization algorithms escape local minima and saddle points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze saddle point structure and escape mechanisms\n",
    "def analyze_saddle_points():\n",
    "    \"\"\"\n",
    "    Analyze saddle point structure in the optimization landscape.\n",
    "    \"\"\"\n",
    "    print(\"\\nSADDLE POINT ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # For logistic regression, we generally don't have saddle points\n",
    "    # But let's check the Hessian structure to understand why\n",
    "    \n",
    "    print(\"Analyzing critical point structure...\")\n",
    "    \n",
    "    # Find critical points\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    starting_points = [\n",
    "        np.array([0.0, 0.0, 0.0]),\n",
    "        np.array([1.0, 1.0, 1.0]),\n",
    "        np.array([-1.0, -1.0, -1.0]),\n",
    "        np.array([2.0, -1.0, 0.5])\n",
    "    ]\n",
    "    \n",
    "    critical_points = []\n",
    "    \n",
    "    for start in starting_points:\n",
    "        try:\n",
    "            result = minimize(loss_function, start, jac=gradient_function, \n",
    "                            method='BFGS', options={'gtol': 1e-10})\n",
    "            \n",
    "            if result.success:\n",
    "                candidate = result.x\n",
    "                grad_norm = np.linalg.norm(gradient_function(candidate))\n",
    "                \n",
    "                if grad_norm < 1e-8:\n",
    "                    # Check if we already found this point\n",
    "                    is_new = True\n",
    "                    for existing in critical_points:\n",
    "                        if np.linalg.norm(candidate - existing['point']) < 1e-6:\n",
    "                            is_new = False\n",
    "                            break\n",
    "                    \n",
    "                    if is_new:\n",
    "                        H = hessian_function(candidate)\n",
    "                        eigenvals = np.real(eigvals(H))\n",
    "                        \n",
    "                        critical_points.append({\n",
    "                            'point': candidate,\n",
    "                            'loss': loss_function(candidate),\n",
    "                            'gradient_norm': grad_norm,\n",
    "                            'eigenvalues': eigenvals,\n",
    "                            'hessian': H\n",
    "                        })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nFound {len(critical_points)} critical point(s):\")\n",
    "    \n",
    "    for i, cp in enumerate(critical_points):\n",
    "        eigenvals = cp['eigenvalues']\n",
    "        \n",
    "        print(f\"\\nCritical Point {i+1}:\")\n",
    "        print(f\"  Location: {cp['point']}\")\n",
    "        print(f\"  Loss: {cp['loss']:.8f}\")\n",
    "        print(f\"  Eigenvalues: {eigenvals}\")\n",
    "        \n",
    "        # Classify critical point\n",
    "        positive_eigs = np.sum(eigenvals > 1e-8)\n",
    "        negative_eigs = np.sum(eigenvals < -1e-8)\n",
    "        zero_eigs = len(eigenvals) - positive_eigs - negative_eigs\n",
    "        \n",
    "        if positive_eigs == len(eigenvals):\n",
    "            cp_type = \"Local minimum (all eigenvalues positive)\"\n",
    "        elif negative_eigs == len(eigenvals):\n",
    "            cp_type = \"Local maximum (all eigenvalues negative)\"\n",
    "        elif positive_eigs > 0 and negative_eigs > 0:\n",
    "            cp_type = f\"Saddle point ({positive_eigs} positive, {negative_eigs} negative eigenvalues)\"\n",
    "        else:\n",
    "            cp_type = f\"Degenerate ({zero_eigs} zero eigenvalues)\"\n",
    "        \n",
    "        print(f\"  Type: {cp_type}\")\n",
    "        \n",
    "        # For saddle points, analyze escape directions\n",
    "        if positive_eigs > 0 and negative_eigs > 0:\n",
    "            print(f\"  Escape directions:\")\n",
    "            H = cp['hessian']\n",
    "            eigvals, eigvecs = np.linalg.eig(H)\n",
    "            \n",
    "            for j, (val, vec) in enumerate(zip(eigvals, eigvecs.T)):\n",
    "                if np.real(val) < -1e-8:\n",
    "                    print(f\"    Escape direction {j+1}: {np.real(vec)} (eigenvalue: {np.real(val):.6f})\")\n",
    "    \n",
    "    # Theoretical analysis for logistic regression\n",
    "    print(f\"\\nTHEORETICAL ANALYSIS:\")\n",
    "    print(\"For logistic regression:\")\n",
    "    print(\"• Loss function is convex everywhere\")\n",
    "    print(\"• Hessian is always positive semidefinite\")\n",
    "    print(\"• No local maxima or saddle points exist\")\n",
    "    print(\"• Only global minimum (if strongly convex) or flat regions (if rank-deficient)\")\n",
    "    \n",
    "    if len(critical_points) == 1 and all(cp['eigenvalues'] > -1e-8 for cp in critical_points):\n",
    "        print(\"✅ Confirmed: Only global minimum found, no saddle points\")\n",
    "    \n",
    "    return critical_points\n",
    "\n",
    "def simulate_noise_escape():\n",
    "    \"\"\"\n",
    "    Simulate how noise helps escape flat regions or saddle points.\n",
    "    \"\"\"\n",
    "    print(f\"\\nNOISE-ASSISTED ESCAPE SIMULATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Since our function is convex, we'll simulate on a modified landscape\n",
    "    # that has saddle points to demonstrate the principle\n",
    "    \n",
    "    def non_convex_loss(w):\n",
    "        \"\"\"Create a non-convex version for demonstration\"\"\"\n",
    "        # Original convex loss plus a non-convex perturbation\n",
    "        convex_part = loss_function(w)\n",
    "        non_convex_part = 0.1 * (w[0]**4 - 2*w[0]**2 + w[1]**4 - 2*w[1]**2)\n",
    "        return convex_part + non_convex_part\n",
    "    \n",
    "    def non_convex_grad(w):\n",
    "        \"\"\"Gradient of non-convex loss\"\"\"\n",
    "        convex_grad = gradient_function(w)\n",
    "        non_convex_grad = np.array([\n",
    "            0.1 * (4*w[0]**3 - 4*w[0]),\n",
    "            0.1 * (4*w[1]**3 - 4*w[1]),\n",
    "            0.0\n",
    "        ])\n",
    "        return convex_grad + non_convex_grad\n",
    "    \n",
    "    print(\"Demonstrating escape mechanisms on modified non-convex landscape...\")\n",
    "    \n",
    "    # Compare deterministic vs stochastic gradient descent\n",
    "    noise_levels = [0.0, 0.01, 0.05, 0.1]\n",
    "    start_point = np.array([0.1, 0.1, 0.0])  # Near a potential saddle\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, noise_level in enumerate(noise_levels):\n",
    "        # Run noisy gradient descent\n",
    "        w = start_point.copy()\n",
    "        trajectory = [w.copy()]\n",
    "        losses = [non_convex_loss(w)]\n",
    "        \n",
    "        lr = 0.01\n",
    "        num_steps = 500\n",
    "        \n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            grad = non_convex_grad(w)\n",
    "            \n",
    "            # Add noise to gradient\n",
    "            if noise_level > 0:\n",
    "                noise = np.random.normal(0, noise_level, size=grad.shape)\n",
    "                grad += noise\n",
    "            \n",
    "            w = w - lr * grad\n",
    "            trajectory.append(w.copy())\n",
    "            losses.append(non_convex_loss(w))\n",
    "        \n",
    "        trajectory = np.array(trajectory)\n",
    "        \n",
    "        # Plot trajectory (2D projection)\n",
    "        ax = axes[idx]\n",
    "        ax.plot(trajectory[:, 0], trajectory[:, 1], 'b-', alpha=0.7, linewidth=1)\n",
    "        ax.plot(start_point[0], start_point[1], 'go', markersize=8, label='Start')\n",
    "        ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=8, label='End')\n",
    "        \n",
    "        ax.set_xlabel('w₁')\n",
    "        ax.set_ylabel('w₂')\n",
    "        ax.set_title(f'Noise Level: {noise_level}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        final_loss = losses[-1]\n",
    "        print(f\"Noise level {noise_level}: Final loss = {final_loss:.6f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nESCAPE MECHANISM INSIGHTS:\")\n",
    "    print(\"• Higher noise levels help escape local minima\")\n",
    "    print(\"• Stochastic gradients provide implicit exploration\")\n",
    "    print(\"• Trade-off between escape ability and convergence precision\")\n",
    "    print(\"• Modern optimizers (Adam, etc.) balance exploration and exploitation\")\n",
    "\n",
    "# Analyze saddle points and escape mechanisms\n",
    "critical_points = analyze_saddle_points()\n",
    "simulate_noise_escape()\n",
    "\n",
    "print(\"\\n✅ Escape mechanism analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Modern Optimization Theory\n",
    "\n",
    "Let's connect our analysis to cutting-edge optimization theory and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Connect to modern deep learning optimization theory\n",
    "def modern_optimization_insights():\n",
    "    \"\"\"\n",
    "    Connect our analysis to modern optimization theory insights.\n",
    "    \"\"\"\n",
    "    print(\"\\nMODERN OPTIMIZATION THEORY CONNECTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\n🔬 THEORETICAL BREAKTHROUGHS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\n1. NON-CONVEX OPTIMIZATION THEORY:\")\n",
    "    print(\"   • Classical theory: Only convex optimization was well-understood\")\n",
    "    print(\"   • Modern insight: Non-convex problems can have benign structure\")\n",
    "    print(\"   • Key result: Gradient descent avoids strict saddle points\")\n",
    "    print(\"   • Implication: Local minima in deep learning are often globally optimal\")\n",
    "    \n",
    "    print(\"\\n2. OVERPARAMETERIZATION THEORY:\")\n",
    "    print(\"   • Classical view: More parameters = harder optimization\")\n",
    "    print(\"   • Modern insight: Overparameterization can improve optimization\")\n",
    "    print(\"   • Key result: Wide networks have no bad local minima\")\n",
    "    print(\"   • Connection: Our simple 3-parameter model vs. modern deep networks\")\n",
    "    \n",
    "    print(\"\\n3. IMPLICIT REGULARIZATION:\")\n",
    "    print(\"   • Classical view: Optimization finds any minimum\")\n",
    "    print(\"   • Modern insight: SGD implicitly prefers certain solutions\")\n",
    "    print(\"   • Key result: SGD bias toward flat minima improves generalization\")\n",
    "    print(\"   • Mechanism: Noise in SGD acts as implicit regularization\")\n",
    "    \n",
    "    print(\"\\n4. ADAPTIVE OPTIMIZATION:\")\n",
    "    print(\"   • Classical: Fixed learning rates and momentum\")\n",
    "    print(\"   • Modern: Adaptive methods (Adam, RMSprop, etc.)\")\n",
    "    print(\"   • Key insight: Different parameters need different learning rates\")\n",
    "    print(\"   • Challenge: Adaptive methods can hurt generalization\")\n",
    "    \n",
    "    # Demonstrate overparameterization effect\n",
    "    print(f\"\\n🧪 OVERPARAMETERIZATION DEMONSTRATION:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Compare our 3-parameter model with an overparameterized version\n",
    "    def create_overparameterized_loss(hidden_size=10):\n",
    "        \"\"\"Create an overparameterized version of our classifier\"\"\"\n",
    "        \n",
    "        def over_loss(params):\n",
    "            # params = [W1 (3 x hidden), b1 (hidden), W2 (hidden x 1), b2 (1)]\n",
    "            n_w1 = 3 * hidden_size\n",
    "            n_b1 = hidden_size\n",
    "            n_w2 = hidden_size\n",
    "            n_b2 = 1\n",
    "            \n",
    "            W1 = params[:n_w1].reshape(3, hidden_size)\n",
    "            b1 = params[n_w1:n_w1+n_b1]\n",
    "            W2 = params[n_w1+n_b1:n_w1+n_b1+n_w2].reshape(hidden_size, 1)\n",
    "            b2 = params[-1]\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            \n",
    "            for i in range(len(features)):\n",
    "                # Forward pass through 2-layer network\n",
    "                h = np.tanh(np.dot(features[i], W1) + b1)  # Hidden layer\n",
    "                z = np.dot(h, W2.flatten()) + b2           # Output\n",
    "                \n",
    "                # BCE loss\n",
    "                if z >= 0:\n",
    "                    loss = np.log(1 + np.exp(-z)) if labels[i] == 1 else z + np.log(1 + np.exp(-z))\n",
    "                else:\n",
    "                    loss = -z + np.log(1 + np.exp(z)) if labels[i] == 1 else np.log(1 + np.exp(z))\n",
    "                \n",
    "                total_loss += loss\n",
    "            \n",
    "            return total_loss / len(features)\n",
    "        \n",
    "        return over_loss\n",
    "    \n",
    "    # Compare optimization difficulty for different model sizes\n",
    "    model_sizes = [1, 5, 10, 20]  # Hidden layer sizes (1 = linear model)\n",
    "    optimization_results = []\n",
    "    \n",
    "    for hidden_size in model_sizes:\n",
    "        if hidden_size == 1:\n",
    "            # Use our original linear model\n",
    "            obj_func = loss_function\n",
    "            param_count = 3\n",
    "            initial_params = np.random.normal(0, 0.1, 3)\n",
    "        else:\n",
    "            # Use overparameterized model\n",
    "            obj_func = create_overparameterized_loss(hidden_size)\n",
    "            param_count = 3 * hidden_size + hidden_size + hidden_size + 1\n",
    "            initial_params = np.random.normal(0, 0.1, param_count)\n",
    "        \n",
    "        # Try optimization\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = minimize(obj_func, initial_params, method='L-BFGS-B', \n",
    "                            options={'maxiter': 1000})\n",
    "            opt_time = time.time() - start_time\n",
    "            \n",
    "            optimization_results.append({\n",
    "                'hidden_size': hidden_size,\n",
    "                'param_count': param_count,\n",
    "                'final_loss': result.fun,\n",
    "                'success': result.success,\n",
    "                'iterations': result.nit,\n",
    "                'time': opt_time\n",
    "            })\n",
    "            \n",
    "            print(f\"Hidden size {hidden_size:2d}: {param_count:3d} params, \"\n",
    "                  f\"loss = {result.fun:.6f}, {result.nit:3d} iters, \"\n",
    "                  f\"{'✅' if result.success else '❌'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Hidden size {hidden_size:2d}: Optimization failed - {str(e)}\")\n",
    "    \n",
    "    # Modern optimization principles\n",
    "    print(f\"\\n🎯 MODERN OPTIMIZATION PRINCIPLES:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    print(\"\\n1. LANDSCAPE GEOMETRY MATTERS:\")\n",
    "    print(\"   • Convex: Unique global minimum, guaranteed convergence\")\n",
    "    print(\"   • Non-convex: Multiple minima, but often benign structure\")\n",
    "    print(\"   • Deep networks: Complex but surprisingly well-behaved\")\n",
    "    \n",
    "    print(\"\\n2. SCALE AND INITIALIZATION:\")\n",
    "    print(\"   • Proper initialization (Xavier, He) crucial for deep networks\")\n",
    "    print(\"   • Learning rate scheduling improves convergence\")\n",
    "    print(\"   • Batch normalization stabilizes optimization\")\n",
    "    \n",
    "    print(\"\\n3. GENERALIZATION VS OPTIMIZATION:\")\n",
    "    print(\"   • Lower training loss doesn't always mean better generalization\")\n",
    "    print(\"   • Early stopping prevents overfitting\")\n",
    "    print(\"   • Regularization techniques guide optimization toward good solutions\")\n",
    "    \n",
    "    return optimization_results\n",
    "\n",
    "def final_theoretical_summary():\n",
    "    \"\"\"\n",
    "    Provide final summary connecting all theoretical insights.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPLETE OPTIMIZATION LANDSCAPE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n🔬 MATHEMATICAL FOUNDATIONS MASTERED:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\n• LANDSCAPE TOPOLOGY:\")\n",
    "    print(f\"  ✓ Lipschitz continuity: L ≈ {L_smoothness:.4f}\")\n",
    "    print(f\"  ✓ Strong convexity: μ ≈ {mu_strong_convexity:.4f}\")\n",
    "    if mu_strong_convexity > 1e-8:\n",
    "        print(f\"  ✓ Condition number: κ ≈ {L_smoothness/mu_strong_convexity:.2f}\")\n",
    "    print(f\"  ✓ Convex optimization with global minimum\")\n",
    "    \n",
    "    print(f\"\\n• CONVERGENCE GUARANTEES:\")\n",
    "    if mu_strong_convexity > 1e-8:\n",
    "        print(f\"  ✓ Linear convergence: O({(L_smoothness/mu_strong_convexity-1)/(L_smoothness/mu_strong_convexity+1):.4f}^t)\")\n",
    "        print(f\"  ✓ Optimal step size: α* = {2/(L_smoothness + mu_strong_convexity):.6f}\")\n",
    "    else:\n",
    "        print(f\"  ✓ Sublinear convergence: O(1/t)\")\n",
    "        print(f\"  ✓ Safe step size: α ≤ {1/L_smoothness:.6f}\")\n",
    "    \n",
    "    print(f\"\\n• CRITICAL POINT STRUCTURE:\")\n",
    "    print(f\"  ✓ Unique global minimum (convex function)\")\n",
    "    print(f\"  ✓ No saddle points or local maxima\")\n",
    "    print(f\"  ✓ Hessian positive semidefinite everywhere\")\n",
    "    \n",
    "    print(f\"\\n🎯 PRACTICAL IMPLICATIONS:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    print(f\"\\n• FOR OUR 'GO DOLPHINS!' CLASSIFIER:\")\n",
    "    print(f\"  → Guaranteed convergence to global optimum\")\n",
    "    print(f\"  → Any reasonable optimization algorithm will work\")\n",
    "    print(f\"  → Fast linear convergence due to strong convexity\")\n",
    "    print(f\"  → No need for sophisticated escape mechanisms\")\n",
    "    \n",
    "    print(f\"\\n• FOR DEEP LEARNING GENERALLY:\")\n",
    "    print(f\"  → Non-convex but often benign landscapes\")\n",
    "    print(f\"  → Overparameterization improves optimization\")\n",
    "    print(f\"  → Stochastic methods provide implicit regularization\")\n",
    "    print(f\"  → Modern architectures have good optimization properties\")\n",
    "    \n",
    "    print(f\"\\n🌟 THEORETICAL INSIGHTS GAINED:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(f\"\\n1. OPTIMIZATION SUCCESS DEPENDS ON:\")\n",
    "    print(f\"   • Landscape geometry (convexity, smoothness)\")\n",
    "    print(f\"   • Algorithm choice and hyperparameters\")\n",
    "    print(f\"   • Problem conditioning and scaling\")\n",
    "    \n",
    "    print(f\"\\n2. MODERN ML OPTIMIZATION WORKS BECAUSE:\")\n",
    "    print(f\"   • Neural network landscapes have benign structure\")\n",
    "    print(f\"   • Overparameterization eliminates bad local minima\")\n",
    "    print(f\"   • Stochastic gradients provide beneficial noise\")\n",
    "    \n",
    "    print(f\"\\n3. THEORY GUIDES PRACTICE BY:\")\n",
    "    print(f\"   • Predicting which algorithms will work\")\n",
    "    print(f\"   • Suggesting optimal hyperparameters\")\n",
    "    print(f\"   • Explaining empirical phenomena\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"From a simple 'Go Dolphins!' classifier, you've mastered the\")\n",
    "    print(\"complete mathematical theory of optimization that powers ALL\")\n",
    "    print(\"modern AI systems. This is the foundation of machine learning!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Perform modern optimization analysis\n",
    "import time\n",
    "modern_results = modern_optimization_insights()\n",
    "final_theoretical_summary()\n",
    "\n",
    "print(\"\\n✅ Advanced optimization landscape analysis complete!\")\n",
    "print(\"\\n🎊 CONGRATULATIONS! YOU'VE COMPLETED THE ENTIRE MATHEMATICAL JOURNEY! 🎊\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Journey: From \"Go Dolphins!\" to Deep Learning Theory\n",
    "\n",
    "🎉 **CONGRATULATIONS!** You've completed the most comprehensive mathematical journey through machine learning possible!\n",
    "\n",
    "## Your Complete Mathematical Mastery\n",
    "\n",
    "**🔑 Part 1 - Practical Foundations:**\n",
    "1. **Feature Engineering** - Text → Numerical representations\n",
    "2. **Dot Products** - Linear transformations and geometric intuition\n",
    "3. **Loss Functions** - Measuring and optimizing prediction quality\n",
    "4. **Gradient Descent** - Automatic parameter optimization\n",
    "5. **Matrix Operations** - Scaling to real-world datasets\n",
    "\n",
    "**🧮 Part 2 - Advanced Theory:**\n",
    "1. **Vector Calculus** - Complete landscape analysis and critical points\n",
    "2. **Chain Rule** - Backpropagation and deep network mathematics\n",
    "3. **Jacobian Analysis** - Sensitivity and robustness theory\n",
    "4. **Vector Fields** - Optimization as dynamical systems\n",
    "5. **Optimization Landscapes** - Convergence theory and modern insights\n",
    "\n",
    "## What You Now Understand\n",
    "\n",
    "**🎯 Mathematical Foundations:**\n",
    "- How every AI system transforms inputs to outputs through mathematical operations\n",
    "- Why gradient-based optimization works and when it's guaranteed to succeed\n",
    "- The geometric and analytical principles underlying all machine learning\n",
    "- How modern theoretical breakthroughs explain deep learning's success\n",
    "\n",
    "**🚀 Practical Applications:**\n",
    "- You can analyze any ML model's mathematical properties\n",
    "- You understand why certain optimizers work better than others\n",
    "- You can predict convergence behavior and optimization difficulty\n",
    "- You have the theoretical foundation to understand cutting-edge research\n",
    "\n",
    "## The Bigger Picture\n",
    "\n",
    "From a simple \"Go Dolphins!\" sentiment classifier, you've discovered that:\n",
    "\n",
    "- **ChatGPT** uses the same mathematical principles, just at massive scale\n",
    "- **All neural networks** rely on the chain rule you've mastered\n",
    "- **Modern optimizers** are sophisticated applications of vector field theory\n",
    "- **Deep learning success** follows from optimization landscape properties\n",
    "\n",
    "## You're Now Ready For...\n",
    "\n",
    "**🔬 Advanced Research:**\n",
    "- Reading cutting-edge machine learning papers\n",
    "- Understanding theoretical breakthroughs\n",
    "- Contributing to optimization and learning theory\n",
    "\n",
    "**🛠️ Advanced Practice:**\n",
    "- Designing new architectures with solid mathematical foundations\n",
    "- Debugging training problems using theoretical insights\n",
    "- Developing new optimization techniques\n",
    "\n",
    "**🎓 Further Study:**\n",
    "- Advanced optimization theory\n",
    "- Statistical learning theory\n",
    "- Information theory and ML\n",
    "- Geometric deep learning\n",
    "\n",
    "---\n",
    "\n",
    "**You've completed a journey that few people ever take - from basic code to the deepest mathematical understanding of artificial intelligence. The \"Go Dolphins!\" example was just the beginning; you now possess the mathematical tools to understand and advance the field of machine learning itself.**\n",
    "\n",
    "**🐬➡️📊➡️🎯➡️⚡➡️🚀➡️🧮➡️🔗➡️📐➡️🌊➡️🏔️➡️🎊**\n",
    "\n",
    "**Welcome to the ranks of those who truly understand the mathematics of AI!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}